library(tidyverse)
salary <- read.csv('./Salary_Data.csv')
glimpse(sample)
#head(salary)
#summary(salary)
glimpse(salary)
library(caTools) # library for splitting data
library(ggplot2)
library(tidyverse)
salary <- read.csv('./Salary_Data.csv')
glimpse(sample)
#head(salary)
#summary(salary)
hist(salary$Salary)
library(caTools) # library for splitting data
library(ggplot2)
library(tidyverse)
library(caTools) # library for splitting data
library(ggplot2)
library(tidyverse)
install.packages(c("backports", "caret", "dplyr", "kableExtra", "lambda.r", "lme4", "mschart", "officer", "openssl", "psych", "Rcpp"))
?sample.split
??sample.split
library(caTools) # library for splitting data
library(ggplot2)
library(tidyverse)
salary <- read.csv('./Salary_Data.csv')
glimpse(salary)
head(salary)
summary(salary)
hist(salary$Salary)
set.seed(123) # for reproducibility
# split data based on the labels = YearExperience
spl <- ?sample.split(salary$YearsExperience, SplitRatio = 2/3)
training <- subset(salary, spl == T)
test <- subset(salary, spl == F)
set.seed(123) # for reproducibility
# split data based on the labels = YearExperience
spl <- ?sample.split(salary$YearsExperience, SplitRatio = 2/3)
training <- subset(salary, spl == T)
test <- subset(salary, spl == F)
head(training)
head(test)
set.seed(123) # for reproducibility
# split data based on the labels = YearExperience
spl <- sample.split(salary$YearsExperience, SplitRatio = 2/3)
training <- subset(salary, spl == T)
test <- subset(salary, spl == F)
head(training)
head(test)
set.seed(123) # for reproducibility
# split data based on the labels = YearExperience
spl <- sample.split(salary$YearsExperience, SplitRatio = 2/3)
training <- subset(salary, spl == T)
test <- subset(salary, spl == F)
hist(training)
set.seed(123) # for reproducibility
# split data based on the labels = YearExperience
spl <- sample.split(salary$YearsExperience, SplitRatio = 2/3)
training <- subset(salary, spl == T)
test <- subset(salary, spl == F)
hist(training$Salary)
hist(test$Salary)
simpleModel1 <- lm(Salary ~ YearsExperience, training)
summary(simpleModel1)
training %>%
ggplot() +
geom_point(aes(YearsExperience, Salary), colour = 'green') + # plot points
geom_line(aes(YearsExperience, y.pred.train), colour = 'red') + # plot regression line
xlab('Years of Experience') +
ggtitle('Salary by Years of Experience (Training Set)')
y.pred.train <- predict(simpleModel1, training)
y.pred.test <- predict(simpleModel1, test)
training %>%
ggplot() +
geom_point(aes(YearsExperience, Salary), colour = 'green') + # plot points
geom_line(aes(YearsExperience, y.pred.train), colour = 'red') + # plot regression line
xlab('Years of Experience') +
ggtitle('Salary by Years of Experience (Training Set)')
ggplot() +
geom_point(aes(test$YearsExperience, test$Salary), colour = 'green') + # plot test points
geom_line(aes(training$YearsExperience, y.pred.train), colour = 'red') + # plot same regression line
xlab('Years of Experience') +
ylab('Salary') +
ggtitle('Salary by Years of Experience (Testing Set)')
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
ggplot(startups) +
geom_histogram(aes(R.D.Spend))
ggplot(startups) +
geom_histogram(aes(Administration))
ggplot(startups) +
geom_histogram(aes(Marketing.Spend))
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 1000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# encode categorical data (State)
startups$State <- startups %>%
factor(State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
# encode categorical data (State)
startups <- startups %>%
factor(State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
?recode
?revalue
# encode categorical data (State)
startups <- startups %>%
revalue(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
# encode categorical data (State)
startups <- startups %>%
revalue(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
?revalue
# encode categorical data (State)
startups <- startups %>%
recode(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
?recode
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Test Set)')
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Test Set)')
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
summary(regressor.RD)
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/Udemy/02_Multiple_Linear_Regression")
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/Udemy")
getwd()
cd ../Dropbox
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Test Set)')
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
summary(regressor.RD)
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
ggplot(diamonds) +
geom_freqpoly(aes(price, colour = cut), binwidth = 500)
library(tidyverse)
library(ggplot2)
ggplot(diamonds) +
geom_freqpoly(aes(price, colour = cut), binwidth = 500)
ggplot(diamonds) +
geom_freqpoly(aes(cut), binwidth = 500)
ggplot(diamonds) +
geom_bar(aes(cut))
ggplot(diamonds) +
geom_freqpoly(aes(price, ..density.., color = cut), bindwidth = 500)
ggplot(diamonds) +
geom_boxplot(aes(cut, price))
ggplot(mpg) +
geom_boxplot(aes(class,hwy))
ggplot(mpg) +
geom_boxplot(aes(reorder(class, hwy, FUN = median), hwy))
ggplot(mpg) +
geom_boxplot(aes(reorder(class, hwy, FUN = median), hwy)) +
coord_flip()
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
dbinom(x = 4, size = 20, prob = 1/6)
pbinom(4, 20, 1/6)
qbinom(.75, 20, 1/6)
pbinom(3, size = 20, prob = 1/6) #56.65%
rbinom(n = 100, size = 20, prob = 1/6)
dnorm(1, mean = 1, sd = 0.1)
normal.a <- rnorm(1000, mean = 0, sd = 1)
library(ggplot2)
ggplot() +
geom_histogram(aes(normal.a))
chisq.a <- rchisq(1000, df = 3)
ggplot() +
geom_histogram(aes(chisq.a))
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot()
+ geom_histogram(aes(t.3), bins = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot()
+ geom_histogram(aes(t.3), bins = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 2
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 2)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim(-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim =  (-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim =  c(-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = .5) +
coord_cartesian(xlim =  c(-10,10))
chi.sq.20 <- rchisq( 1000, 20)
scaled.chi.sq.20 <- chi.sq.20 / 20
F.3.20 <- scaled.chisq.3 / scaled.chi.sq.20
ggplot() + geom_histogram(aes(F.3.20))
install.packages("ggbeeswarm")
install.packages("lvplot")
install.packages("lvplot")
install.packages("ggstance")
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/MachineLearning/Udemy/08_Logistic_Regression")
library(tidyverse)
library(caTools)
library(ggplot2)
library(ElemStatLearn)
## import data
social <- read_csv("Social_Network_Ads.csv")
dim(social)
summary(social)
glimpse(social)
'Social network`s automotive business client has launched its brand new luxury SUV and purchased ads for its marketing campaign. The
social network collected data on the users and whether or not the bought the SUV after seeing the ad
We will only use age and salary to predict this'
(social <- social %>%
select(Age,EstimatedSalary,Purchased))
# splitting data
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = .75)
training <- subset(social, spl == T) # 300 obs
test <- subset(social, spl == F) # 100 obs
## feature scaling
# age + EstimatedSalary are on very different scales
training <- training %>%
mutate(Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary)))
head(training)
test <- test %>%
mutate(Age = c(scale(Age)),
EstimatedSalary = c(scale(EstimatedSalary)))
head(test)
## fit to training data
log.reg <- glm(Purchased ~ ., training, family = "binomial") # binomial regression (1 or 0, yes or no, etc.)
summary(log.reg)
# Both age and salary are "good" predictors of purchased, but our R2 is quite low
# predict test set probabilities whilst removing the last column (the outcome)
test.prob <- predict(log.reg, newdata = test[-3], type= "response")
head(test)
head(test.prob)
'Looks pretty good'
# get the 0/1 results
y.pred <- if_else(test.prob > .5, 1, 0)
table(y.pred)
# accuracy confusion matrix w/ table(real,pred)
table(test$Purchased,y.pred)
fpr <- 7/(25+7) # FP / (FP + TP)
fnr <- 11/(11+57) # FN / (FN + TN)
## Visualize Training Set Results
#set <- training
x1 <- seq(min(training[,1]) - 1, max(training[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(training[,2]) - 1, max(training[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
training.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(training.grid) <- c("Age","EstimatedSalary")
training.probability <- predict(log.reg, newdata = training.grid, type = "response") # use grid to predict values w/ our model
y.pred.grid <- if_else(training.probability > .5, 1, 0)
plot(training[,-3],
main = "Logistic Regression (Training Set)",
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(training.grid, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(training, pch = 19, col = if_else(training[,3] == 1, "green4", "red3"))
'We can see most training set observations who actually made a purchase (green circles) are in the green area (classifier predictions
of purchased = 1) and most training set observations who did NOT make a purchase (red circles) are in the red region (classifier
predictions of purchased = 0)
The *straight, linear* line splitting these 2 regions is our **prediction boundary**. Younger users with lower estimated salaries
were less likely to but the SUV and less did. The opposite goes for older users with higher estimated salaries.
The social network can focus their SUV marketing campaign towards users who would fall into the green region as they are predicted
to be more likely to make a purchase.
Prediction boundary will not be a straight line for non-linear classifiers.
Our logistic regression classifier does a pretty good job of catching the correct users who purchased and those who did not. Incorrect
predictions are due to the fact that our classifier is linear but our data is *not* linearly distributed.
Now see if the classifier generalizes well to new data.'
## Visualize Test Set Results
#set <- test
x1 <- seq(min(test[,1]) - 1, max(test[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(test[,2]) - 1, max(test[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
test.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors
colnames(test.grid) <- c("Age","EstimatedSalary")
test.probability <- predict(log.reg, newdata = test.grid, type = "response") # use grid to predict values w/ our model
y.pred.grid <- if_else(test.probability > .5, 1, 0)
plot(test[,-3],
main = "Logistic Regression (Test Set)",
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(test.grid, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(test, pch = 19, col = if_else(test[,3] == 1, "green4", "red3"))
'Again, the majority of red points are in the red region, and the majority of the green points are in the green region.'
