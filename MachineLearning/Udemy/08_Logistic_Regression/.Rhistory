ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 1000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 10000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# encode categorical data (State)
startups$State <- startups %>%
factor(State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
# encode categorical data (State)
startups <- startups %>%
factor(State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
?recode
?revalue
# encode categorical data (State)
startups <- startups %>%
revalue(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
# encode categorical data (State)
startups <- startups %>%
revalue(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
?revalue
# encode categorical data (State)
startups <- startups %>%
recode(State, 'New York' = 1, 'California' = 2, 'Florida' = 3)
?recode
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Test Set)')
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Test Set)')
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
summary(regressor.RD)
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/Udemy/02_Multiple_Linear_Regression")
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/Udemy")
getwd()
cd ../Dropbox
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
ggplot(startups) +
geom_histogram(aes(R.D.Spend), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Administration), binwidth = 20000)
ggplot(startups) +
geom_histogram(aes(Marketing.Spend), binwidth = 100000)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Test Set)')
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
summary(regressor.RD)
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
ggplot(diamonds) +
geom_freqpoly(aes(price, colour = cut), binwidth = 500)
library(tidyverse)
library(ggplot2)
ggplot(diamonds) +
geom_freqpoly(aes(price, colour = cut), binwidth = 500)
ggplot(diamonds) +
geom_freqpoly(aes(cut), binwidth = 500)
ggplot(diamonds) +
geom_bar(aes(cut))
ggplot(diamonds) +
geom_freqpoly(aes(price, ..density.., color = cut), bindwidth = 500)
ggplot(diamonds) +
geom_boxplot(aes(cut, price))
ggplot(mpg) +
geom_boxplot(aes(class,hwy))
ggplot(mpg) +
geom_boxplot(aes(reorder(class, hwy, FUN = median), hwy))
ggplot(mpg) +
geom_boxplot(aes(reorder(class, hwy, FUN = median), hwy)) +
coord_flip()
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
dbinom(x = 4, size = 20, prob = 1/6)
pbinom(4, 20, 1/6)
qbinom(.75, 20, 1/6)
pbinom(3, size = 20, prob = 1/6) #56.65%
rbinom(n = 100, size = 20, prob = 1/6)
dnorm(1, mean = 1, sd = 0.1)
normal.a <- rnorm(1000, mean = 0, sd = 1)
library(ggplot2)
ggplot() +
geom_histogram(aes(normal.a))
chisq.a <- rchisq(1000, df = 3)
ggplot() +
geom_histogram(aes(chisq.a))
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
# square values of 3 distributions and sum
normal.a <- rnorm(1000)
normal.b <- rnorm(1000)
normal.c <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
ggplot() +
geom_histogram(aes(chi.sq.3))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot()
+ geom_histogram(aes(t.3), bins = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot()
+ geom_histogram(aes(t.3), bins = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 10)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 2
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 2)
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim(-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim =  (-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = 1) +
coord_cartesian(xlim =  c(-10,10))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() +
geom_histogram(aes(t.3), binwidth = .5) +
coord_cartesian(xlim =  c(-10,10))
chi.sq.20 <- rchisq( 1000, 20)
scaled.chi.sq.20 <- chi.sq.20 / 20
F.3.20 <- scaled.chisq.3 / scaled.chi.sq.20
ggplot() + geom_histogram(aes(F.3.20))
install.packages("ggbeeswarm")
install.packages("lvplot")
install.packages("lvplot")
install.packages("ggstance")
install.packages("shiny")
install.packages("RPresto")
devtools::install_github("ricardo-bion/ggtech", dependencies=TRUE)
install.packages("extrafont")
install.packages("utils")
install.packages("utils")
library(extrafont)
library(utils)
download.file("http://social-fonts.com/assets/fonts/facebook-letter-faces/facebook-letter-faces.ttf", "C:/Users/Nimz/Dropbox/facebook-letter-faces.ttf", method="curl")
font_import(pattern = "C:/Users/Nimz/Dropbox/facebook-letter-faces.ttf", prompt=FALSE)
ibrary(devtools)
install.packages("RTools")
install_github("susanathey/causalTree")
install.packages("NLopt")
install.packages("dygraphs")
install.packages("leaflet")
install.packages("DiagrammeR")
install.packages("plotly")
install.packages("broom")
install.packages("pwr")
library(tidyverse)
library(caTools)
library(ggplot2)
library(ElemStatLearn)
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/MachineLearning/Udemy/08_Logistic_Regression")
social <- read_csv("Social_Network_Ads.csv")
dim(social)
summary(social)
glimpse(social)
'Social network`s automotive business client has launched its brand new luxury SUV and purchased ads for its marketing campaign. The
social network collected data on the users and whether or not the bought the SUV after seeing the ad
We will only use age and salary to predict this'
(social <- social %>%
select(Age,EstimatedSalary,Purchased))
# splitting data
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = .75)
training <- subset(social, spl == T) # 300 obs
test <- subset(social, spl == F) # 100 obs
## feature scaling
# age + EstimatedSalary are on very different scales
training <- training %>%
mutate(Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary)))
head(training)
test <- test %>%
mutate(Age = c(scale(Age)),
EstimatedSalary = c(scale(EstimatedSalary)))
head(test)
## fit to training data
log.reg <- glm(Purchased ~ ., training, family = "binomial") # binomial regression (1 or 0, yes or no, etc.)
summary(log.reg)
library(broom)
tidy(log.reg)
library(tidyverse)
library(caTools)
library(ggplot2)
library(ElemStatLearn)
library(broom)
## import data
social <- read_csv("Social_Network_Ads.csv")
dim(social)
summary(social)
glimpse(social)
'Social network`s automotive business client has launched its brand new luxury SUV and purchased ads for its marketing campaign. The
social network collected data on the users and whether or not the bought the SUV after seeing the ad
We will only use age and salary to predict this'
(social <- social %>%
select(Age,EstimatedSalary,Purchased))
# splitting data
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = .75)
training <- subset(social, spl == T) # 300 obs
test <- subset(social, spl == F) # 100 obs
## feature scaling
# age + EstimatedSalary are on very different scales
training <- training %>%
mutate(Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary)))
head(training)
test <- test %>%
mutate(Age = c(scale(Age)),
EstimatedSalary = c(scale(EstimatedSalary)))
head(test)
## fit to training data
log.reg <- glm(Purchased ~ ., training, family = "binomial") # binomial regression (1 or 0, yes or no, etc.)
summary(log.reg)
tidy(log.reg)
# Both age and salary are "good" predictors of purchased, but our R2 is quite low
# predict test set probabilities whilst removing the last column (the outcome)
test.prob <- predict(log.reg, newdata = test[-3], type= "response")
head(test)
head(test.prob)
'Looks pretty good'
# get the 0/1 results
y.pred <- if_else(test.prob > .5, 1, 0)
table(y.pred)
# accuracy confusion matrix w/ table(real,pred)
table(test$Purchased,y.pred)
fpr <- 7/(25+7) # FP / (FP + TP)
fnr <- 11/(11+57) # FN / (FN + TN)
## Create function to visualize different set results
plot.logistic.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
set.probability <- predict(log.reg, newdata = set.grid, type = "response") # use grid to predict values w/ our model
y.pred.grid <- if_else(set.probability > .5, 1, 0)
plot(set[,-3],
main = "Logistic Regression (Training Set)",
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.logistic.model(training)
x1 <- seq(min(training[,1]) - 1, max(training[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(training[,2]) - 1, max(training[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
training.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(training.grid) <- c("Age","EstimatedSalary")
training.probability <- predict(log.reg, newdata = training.grid, type = "response") # use grid to predict values w/ our model
y.pred.grid <- if_else(training.probability > .5, 1, 0)
plot(training[,-3],
main = "Logistic Regression (Training Set)",
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(training.grid, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(training, pch = 19, col = if_else(training[,3] == 1, "green4", "red3"))
plot.logistic.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
set.probability <- predict(log.reg, newdata = set.grid, type = "response") # use grid to predict values w/ our model
y.pred.grid <- if_else(set.probability > .5, 1, 0)
plot(set[,-3],
main = "Logistic Regression (set Set)",
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.logistic.model(training)
plot.logistic.model(test)
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/MachineLearning/Udemy/08_Logistic_Regression")
ggplot(faithful) +
geom_point(aes(eruptions,waiting))
ggplot(faithful) +
geom_histogram(aes(eruptions), binwidth = .25)
install.packages("modelR")
library(tidyverse)
library(modelr)
?add_residuals
diamonds %>%
add_residuals(mod) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
model <- lm(log(price) ~ log(carat), diamonds) # scale price + carat b/c they're on very different scales
diamonds2 <- diamonds %>%
add_residuals(mod) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
diamonds2$resid
# residuals = a view of the price of the diamond once the effect of carat has been removed.
model <- lm(log(price) ~ log(carat), diamonds) # scale price + carat b/c they're on very different scales
diamonds2 <- diamonds %>%
add_residuals(mod) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
diamonds2$resid
model <- lm(log(price) ~ log(carat), diamonds) # scale price + carat b/c they're on very different scales
diamonds2 <- diamonds %>%
add_residuals(mod) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
diamonds2$resid
model <- lm(log(price) ~ log(carat), diamonds) # scale price + carat b/c they're on very different scales
diamonds2 <- diamonds %>%
add_residuals(mod) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
diamonds2 <- diamonds %>%
add_residuals(model) %>%  # add the residuals of a model to a data frame (modelr)
mutate(resid = exp(resid))
diamonds2$resid
summary(diamonds2$resid)
ggplot(diamonds2) +
geom_point(aes(carat, resid))
cor(diamonds$carat,diamonds$price)
cor(diamonds$carat,diamonds$cut)
ggplot(diamonds2) +
geom_boxplot(aes(x = cut, y = resid))
diamonds %>%
count(cut, clarity) %>%
ggplot(aes(clarity, cut, fill = n)) +
geom_tile()
