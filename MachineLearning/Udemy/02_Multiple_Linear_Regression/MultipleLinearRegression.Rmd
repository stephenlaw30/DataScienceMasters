---
title: "Multiple Linear Regression - Udemy MLAZ"
author: "Steve Newns"
date: "October 3, 2017"
output: html_document
---

## Business Problem - Venture Capatalism

Our VC firm has data on 50 companies, including extracts from their P&L for the given financial year. So we know how much they spent on R&D, on Admin costs (salaries), and on Marketing. We also know their profits and which US state they operate in.

We want to be able to predict profits based on the different expenses of a company as well as the state they operate in via a multiple linear regression model.

With this model, the VC firm will know which new companies they'd want to invest in. This will also depend whether our VC firm is more concerned with each the spends and which would yield more profits.

```{r message=F, warning=F}
# library for splitting data
library(caTools)
library(ggplot2)
library(tidyverse)

# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
glimpse(startups)
```

We have no missing values, and it appears that the median and mean scores are all relatively close for each of the spends, so they might be normal distributions.

```{r}
ggplot(startups) + 
  geom_histogram(aes(R.D.Spend), binwidth = 20000)

ggplot(startups) + 
  geom_histogram(aes(Administration), binwidth = 20000)

ggplot(startups) + 
  geom_histogram(aes(Marketing.Spend), binwidth = 100000)
```

Or maybe not. Whatever. This is what I was given. 

**State** is a factor variable, and when dealing with factors in a linear regression, we need to create **dummy variables** for each unique level of the factor and use *only ONE of these dummy variables in the regression*.

```{r}
# encode categorical data (State)
startups$State <- factor(startups$State, 
                         levels = c('New York', 'California', 'Florida'),
                         labels = c(1,2,3))

head(startups)
```

Now our factor is using integers, so it can actually be affected by the regression coefficient assigned to it. Now let's split and train the data with 80% of our 50 observations in the training set and the remaining 20% in the test set.

```{r}
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)

dim(training)
dim(test)

# create regressor (model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
```

We see that only R&D spend is significant in this model, but State2 has highest p-value, and increases in Administration costs seem to cause a *decrease* in profit.

Let's check a model with only the significant R.D.Spend variable.

```{r}
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
```

Now we need to use the model to train it on the traning and test data.

```{r}
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)

# compare test w/ actuals
y.pred.test
test$Profit[0:10]
```

We see that our predictions are kind of close to the actual values in the test set for profit.

Now that we have some predictions of profits based on our linear model, we can check our accuracy of these predictions, and therefore of our model.

```{r visualize training set accuracy}
ggplot(training) + 
  geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot traning points
  geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
  xlab('R&D Spend ($)') +
  ylab('Profits ($)') +
  ggtitle('Profits by Amount of R&D Spend (Training Set)')

ggplot() + 
  geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot test points
  geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
  xlab('R&D Spend ($)') +
  ylab('Profits ($)') +
ggtitle('Profits by Amount of R&D Spend (Test Set)')
```
We wanted to use the same regression line for both graphs, since our regression model was created using the training set, and it looks like most of our predictions are pretty close, with estimates for R&D spend around $100k and $110k being a bit more off.


## BACKWARDS ELIMINATION

Let's look at backwards elimination of the dummy variables.

```{r}
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
```

State 2 and 3 have the highest p-values (> .95), so they have minimal impact on profits. 

Remove both by removing "State" from the model to remove all dummy variables

```{r}
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
```

Now Admin costs have the highest p-value, so remove that variable.

```{r}
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
```

We see that Marketing spend now is *barely significant*. Should we remove it?

This is an arbitrary choice, depending on the significance level, but let's remove it for now, which we already did in an earlier model.

```{r}
summary(regressor.RD)
```

So we only have 1 variable left in our model and it is indeed significant, but *our Adj R2 has decreased from the previous model (.9468 -> .9434)*. That doesn't matter. We'll take a simpler model with all significant predictors if our Adjusted R2 is not decreased too much.


Now let's use the final model on the full dataset.

```{r}
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
```