summary(clin.sci.model)
summary(clin.sci.model2)
lmBeta(clin.sci.model2)
round(lmBeta(clin.sci.model2),3)
round(pCorr(clin.sci.model2),3)
round(100*pCorr(clin.sci.model2),3)
round(100*pCorr(clin.sci.model2),2)
summary(clin.sci.model2
)
library(SDSFoundations)
res <- TempskiResilience
res <- TempskiResilience
table(res$Group)
bas.sci <- res[res$Group == 'Basic Sciences']
bas.sci <- res[res$Group == 'Basic Sciences',]
names(bas.sci)
whoModel <- lm(QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
library(car)
vif(whoModel)
1/vif(whoModel)
plot(whoModel, which = 1)
cutoff <- 4/(whoModel$df)
plot(whoModel, which = 4, cook.levels = cutoff, id.n = 5)
lmBeta(whoModel)
round(pCorr(whoModel), 4)
round(100*pCorr(whoModel),2) # 8.23
corr.test(bas.sci[,vars])$r
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
corr.test(bas.sci[,vars])$r
cor(bas.sci[,vars])
cor(bas.sci[,vars])$r
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(bas.sci[,vars])
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(res[,vars])
whoModel <- lm(MS.QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(res[,vars])
corr.test(res[,vars])$r
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
corr.test(res[,vars])$r
corr.test(res[,vars],use = "pairwise.complete.obs)$r
corr.test(res[,vars], use = "pairwise.complete.obs')$r
corr.test(res[,vars], use = "pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
vif(whoModel)
1/vif(whoModel)
whoModel
whoModel <- lm(MS.QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
vif(whoModel)
plot(whoModel, which = 1)
cutoff <- 4/(whoModel$df)
plot(whoModel, which = 4, cook.levels = cutoff, id.n = 5)
lmBeta(whoModel)
function (model)
round(100*pCorr(whoModel),2) # 8.23
round(100*pCorr(whoModel),2) # 8.23
whoModel$t
whoModel
res <- TempskiResilience
clin.sci <- res[res$Group == 'Clinical Sciences',]
clin.sci
summary(lm(BDI~Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Gender+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Gender+Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Female+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci))
model1 <- summary(lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(model1)
summary(model1)
model1 <- lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci)
summary(model1)
lmBeta(model1)
round(lmBeta(model1),3)
round(100*pCorr(model1),2)
SS = 1848.76
df <- 20-2
SS = 1848.76
SS = 1848.76
df <- 20-2
25592/2646
-23.4325 / 12.74
22.245*421
8.32*0.1528
480.78 / 1848.76
480.78 / (1848.76+480.78)
480.78 / (1848.76 - 480.78)
1848.76 / (1848.76 - 480.78)
1848.76 / (1848.76 + 480.78)
install.packages('rattle')
install.packages('RGTK2')
install.packages('RGtk2')
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/MachineLearning/Udemy/06_RandomForestRegression/Random_Forest_Regression')
dataset = read.csv('Position_Salaries.csv')
salaries = read.csv('Position_Salaries.csv')
install.packages('randomForest')
library(randomForest)
set.seed(1234)
?randomForest
salaries <- read.csv('Position_Salaries.csv')
new_salaries <- salaries[2:3]
rf_model <- randomForest(Level ~ Salary, new_salaries, ntree = 500)
predictions1 <- predict(rf_model, data.frame(Level = 6.5))
new_salaries
rf_model
predictions1 <- predict(rf_model, data.frame(Level = 6.5))
rf_model2 <- randomForest(new_salaries$Level, new_salaries$Salary, ntree = 500)
rf_model2 <- randomForest(x = new_salaries$Level,
y = new_salaries$Salary, ntree = 500)
new_salaries$Level
new_salaries[1]
rf_model2 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 500)
rf_model2
rf_model
predictions2 <- predict(rf_model2, data.frame(Level = 6.5))
# documentation says put x as a data frame and y as vector
rf_model_small <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 10)
# predictions
predictions2 <- predict(rf_model_small, data.frame(Level = 6.5))
set.seed(1234)
rf_model_small <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 10)
# predictions
predictions2 <- predict(rf_model_small, data.frame(Level = 6.5))
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'red')
library(ggplot2)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'red')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = 6.5))),
color = 'red')
x_grid <- seq(min(new_salaries$Level), max(min(new_salaries$Level)))
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = 6.5))),
color = 'red')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
x_grid = seq(min(dataset$Level), max(dataset$Level), 0.01)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Random Forest Regression)') +
xlab('Level') +
ylab('Salary')
x_grid = seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(x = new_salaries$Level, y = new_salaries$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Random Forest Regression)') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
library(ggplot2)
x_grid <- seq(min(new_salaries$Level), max(min(new_salaries$Level)))
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x = x_grid, y = predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
rf_model_100
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x = x_grid, y = predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x = x_grid, y = predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
ggplot() +
geom_point(aes(x = new_salaries$Level, y = new_salaries$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Random Forest Regression)') +
xlab('Level') +
ylab('Salary')
x_grid <- seq(min(new_salaries$Level), max(min(new_salaries$Level)), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
x_grid <- seq(min(new_salaries$Level), max(min(new_salaries$Level)), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
x_grid = seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(x = new_salaries$Level, y = new_salaries$Salary),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(rf_model_small, newdata = data.frame(Level = x_grid))),
colour = 'blue') +
ggtitle('Truth or Bluff (Random Forest Regression)') +
xlab('Level') +
ylab('Salary')
library(ggplot2)
x_grid <- seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_small, data.frame(Level = x_grid))),
color = 'red')
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
set.seed(1234)
rf_model_500 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 500)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_500, data.frame(Level = x_grid))),
color = 'red')
predictions <- predict(rf_model_500, data.frame(Level = 6.5))
predictions
data.frame(Level = 6.5)
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
set.seed(1234)
rf_model_10 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 10)
# visualize results with line graph
library(ggplot2)
x_grid <- seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_10, data.frame(Level = x_grid))),
color = 'red')
#predict
predict_10 <- predict(rf_model_10, data.frame(Level = 6.5))
# increase resolution
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
predictions_500 <- predict(rf_model_500, data.frame(Level = 6.5))
set.seed(1234)
rf_model_500 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 500)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_500, data.frame(Level = x_grid))),
color = 'red')
'More trees != more steps b/c the more trees we add, the more the average of the
different DT predictions is converging to the same ultimate average + therefore
converge to some certain shape of stairs'
# predictions
predictions_500 <- predict(rf_model_500, data.frame(Level = 6.5))
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
## Random Forest Regression
'Business Problem:
- Want to predict salaries based on position title and the level
- We are in HR and are about to give an offer to a potential new employee
- We must negotiate a salary for an interviewee who has 25 yrs experience asking for
a minimum of $160k/yr.
- You are in HR + call the previous employer to check that info about their previous salary
- All the info you get from HR is the simple table of salaries for 10 positions + levels'
## Importing the dataset
salaries <- read.csv('Position_Salaries.csv')
new_salaries <- salaries[2:3]
'Remember Decision Tree regression is a NON-CONTINOUS model, so Random Forests of Decision Trees will be
as well'
#install.packages('randomForest')
library(randomForest)
set.seed(1234)
## Breiman's random forest w/ specified independent + dependent variables
rf_model <- randomForest(Level ~ Salary, new_salaries, ntree = 500)
# predictions
predictions1 <- predict(rf_model, data.frame(Level = 6.5))
'******RESULTS IN ERROR*************'
# documentation says put x as a data frame and y as vector
set.seed(1234)
rf_model_10 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 10)
# visualize results with line graph
library(ggplot2)
x_grid <- seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_10, data.frame(Level = x_grid))),
color = 'red')
#predict
predict_10 <- predict(rf_model_10, data.frame(Level = 6.5))
'only predicted $130k, which is way below what the employee sais their salary was, +
this is bad since we think he is bluffing when he is not, as we will see below'
# increase resolution
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
'Predicted just over $163k, which is good to show the employee was NOT bluffing and
was honest'
# increase resolution
set.seed(1234)
rf_model_500 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 500)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_500, data.frame(Level = x_grid))),
color = 'red')
'More trees != more steps b/c the more trees we add, the more the average of the
different DT predictions is converging to the same ultimate average + therefore
converge to some certain shape of stairs'
# predictions
predictions_500 <- predict(rf_model_500, data.frame(Level = 6.5))
'Our prediction has barely changed at all, so this the best RF we can get and therefore
the best prediction we can make'
## Random Forest Regression
'Business Problem:
- Want to predict salaries based on position title and the level
- We are in HR and are about to give an offer to a potential new employee
- We must negotiate a salary for an interviewee who has 25 yrs experience asking for
a minimum of $160k/yr.
- You are in HR + call the previous employer to check that info about their previous salary
- All the info you get from HR is the simple table of salaries for 10 positions + levels'
## Importing the dataset
salaries <- read.csv('Position_Salaries.csv')
new_salaries <- salaries[2:3]
'Remember Decision Tree regression is a NON-CONTINOUS model, so Random Forests of Decision Trees will be
as well'
#install.packages('randomForest')
library(randomForest)
set.seed(1234)
## Breiman's random forest w/ specified independent + dependent variables
rf_model <- randomForest(Level ~ Salary, new_salaries, ntree = 500)
# predictions
predictions1 <- predict(rf_model, data.frame(Level = 6.5))
'******RESULTS IN ERROR*************'
# documentation says put x as a data frame and y as vector
set.seed(1234)
rf_model_10 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 10)
# visualize results with line graph
library(ggplot2)
x_grid <- seq(min(new_salaries$Level), max(new_salaries$Level), 0.01)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_10, data.frame(Level = x_grid))),
color = 'red')
#predict
predict_10 <- predict(rf_model_10, data.frame(Level = 6.5))
'only predicted $130k, which is way below what the employee sais their salary was, +
this is bad since we think he is bluffing when he is not, as we will see below'
# increase resolution
set.seed(1234)
rf_model_100 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 100)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_100, data.frame(Level = x_grid))),
color = 'red')
'Now we have a lot more steps in this RF than we had w/ just 1 or 10 DTs, so we have
more splits of the levels and therefore more intervals where y is the average of y
values over these interval x values
More steps is intuitive because we had 100 trees voting on which step of salary the
level 6.5 should be, and then the RF takes the average of those 100 predictions
Each of those predictions becomes a vote of expected salary for that position
We get more steps b/c the whole range of levels is split into more intervals b/c the RF
is calculating many different averages of its DTs predictions in each interval'
#predict
predict_100 <- predict(rf_model_100, data.frame(Level = 6.5))
'Predicted just over $163k, which is good to show the employee was NOT bluffing and
was honest'
# increase resolution
set.seed(1234)
rf_model_500 <- randomForest(x = new_salaries[1],
y = new_salaries$Salary, ntree = 500)
ggplot() +
geom_point(aes(new_salaries$Level,new_salaries$Salary), color = 'blue') +
geom_line(aes(x_grid, predict(rf_model_500, data.frame(Level = x_grid))),
color = 'red')
'More trees != more steps b/c the more trees we add, the more the average of the
different DT predictions is converging to the same ultimate average + therefore
converge to some certain shape of stairs'
# predictions
predictions_500 <- predict(rf_model_500, data.frame(Level = 6.5))
'Our prediction has barely changed at all, so this the best RF we can get and therefore
the best prediction we can make'
'Business Problem:
