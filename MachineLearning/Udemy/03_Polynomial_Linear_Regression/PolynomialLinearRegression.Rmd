---
title: "Polynomial Linear Regression - Udemy MLAZ"
author: "Steve Newns"
date: "October 5, 2017"
output: html_document
---

Now we are building **non-linear regression models**, where the relationship between the output and inputs is NOT linear.

Business Problem:
  - We want to predict salaries based on position title and the level
  - We are in HR and are about to give an offer to a potential nw employee
  - We must negotiate a salary for the interviewee, who has 25 yrs experience, was a regional manager, and is asking for a minimum of $160k/yr.
  - We (HR) call their previous employer to verify the info about their previous salary
  - But, all the info we get from their HR deptartment is a simple table of salaries for 10 positions + levels'
```{r warning=F, message=F}
library(tidyverse)
library(ggplot2)
library(caTools)

# import data
salary <- read.csv('Position_Salaries.csv')
summary(salary)
glimpse(salary)
head(salary)
```

Let's just check the distribution just for kicks.

```{r}
ggplot(salary) + 
  geom_histogram(aes(Salary), binwidth = 100000)
```

So it's pretty right skewed, showing most salaries are below the $250k/year.


Let's look at the relationship between Level and Salary.

```{r}
ggplot(salary) + 
  geom_point(aes(Level, Salary), colour = 'red') + # plot points
  ggtitle('Salary by Position Level')
```

After plotting, we can see a non-linear relationship between salary and position level

However we know the prospective employee was a regional manager, and it usually takes ~4 years to get from regional manager to partner (next the level up)

So, this employer was between Levels 6 and 7, so we will say he was at level 6.5. Therefore, we can use regression to see if he was bluffing about his previous salary.

We also know that position is directly equivalent to Level (i.e. it's basically already encoded), so only need to use 1 of these variables in our regression.

```{r}
# remove positions col
new_salaries <- salary[,c('Level','Salary')]
head(new_salaries,10)
```

We don't need to split our data nor do any feature scaling, since it is such a small dataset, so we can jump straight to running a simple regression model.

```{r}
set.seed(123)

lin_reg <- lm(Salary ~ ., new_salaries)
summary(lin_reg)
```
So, with Level's p-value being < .005, we can say that this model indicates that Level is a "good" predictor of salary, but it could be better (it only has 2 significance stars)

Let's see if we can improve the model by adding **polynomial features**.

These will be additional indepenent variables that will, in some way, compose a new matrix of features which we will apply a multiple linear regression model to in order to make the whole model a polynomial regression model

So, in short, this polynomial regression model is a multiple regression model composed of 1 independent variable + additional polynomial terms of that independent variable.

This makes the model more "flexible" to try to accurately predict the outcome.

```{r}
# add new 2-nd order polynomial Level
new_salaries$level_sq <- new_salaries$Level^2
head(new_salaries)
```

Now to make the model.

```{r}
poly_reg <- lm(Salary ~., new_salaries)
summary(poly_reg)
```
So, level is even less significant here, while the 2nd-order Level is a bit more significant than the Level variable in our original model. 

But, R2 and Adj R2 have improved

Let's check this model against our training (AKA our only) data set.

```{r}
# training predictions
y.pred <- predict(poly_reg, new_salaries)

(squared_plot <- ggplot(new_salaries) + 
  geom_point(aes(Level, Salary), colour = 'blue') + # plot points
  geom_line(aes(Level, y.pred), colour = 'red') + # plot regression line
  xlab('Levels') +
  ggtitle('Truth or Bluff (Squared Polynomial Regression)'))
```


Let's see if a 3rd-order polynomial is even better.

```{r}
new_salaries$level_cub <- new_salaries$Level^3

poly_reg_3 <- lm(Salary ~., new_salaries)
summary(poly_reg_3)
```

Again, our previous, lower-order Level variables have decreased in significant, but our new 3rd-order variable is significant. Also, again our R2 and Adj R2 have improved.

Now for the predictions and plot.

```{r}
y.pred_3 <- predict(poly_reg_3, new_salaries)

(cubed_plot <- ggplot(new_salaries) + 
  geom_point(aes(Level, Salary), colour = 'blue') + # plot points
  geom_line(aes(Level, y.pred_3), colour = 'red') + # plot regression line
  xlab('Levels') +
  ggtitle('Truth or Bluff (Cubed Polynomial Regression'))
```

Well, this looks like it might be overfit. Let's compare it to the simple linear regression.

```{r}
y.pred_lr <- predict(lin_reg, new_salaries)

ggplot(new_salaries) + 
  geom_point(aes(Level, Salary), colour = 'blue') + # plot points
  geom_line(aes(Level, y.pred_lr), colour = 'red') + # plot regression line
  xlab('Levels') + 
  ggtitle('Truth or Bluff (Linear Regression)')
```

We can see that a level 10 salary prediction is *WAY* off of the actual salary (prediction is almost 50% less)

According to this model, if we're hiring a new CEO, they would be furious when we'd suggest a salary of $600k, when in actuality, it should be ~$1M.

In our situation, a level 6.5 position employee is asking for $160k, and we'd predicted his salary was over $250k, which would be good for us if it were true, as we're saving money by paying him much less than we predicted he *was* earning.

Looking at the 2nd-order polynomial regression,
```{r}
squared_plot
```

We've fit the data a bit better, and now we'd offer a lower salary than the level 6.5 is asking for.

Checking the 3rd-order polynomial linear regression:
```{r}
cubed_plot
```
Now we're pretty close to the actual values with our predictions, and we'd still offer < $160k for level 6.5 position. 

As we add more and more degrees, we have higher **order of contact** between the real value **f(x)** for salaries and our expansion/prediction predicted values **g(x)**

For kicks, look at the 4th-order polynomial.
```{r}
new_salaries$level_quart <- new_salaries$Level^4
poly_reg_4 <- lm(Salary ~., new_salaries)
summary(poly_reg_4)

y.pred_4 <- predict(poly_reg_4, new_salaries)

(quartered_plot <- ggplot(new_salaries) + 
  geom_point(aes(Level, Salary), colour = 'blue') + # plot points
  geom_line(aes(Level, y.pred_4), colour = 'red') + # plot regression line
  xlab('Levels') +
  ggtitle('Truth or Bluff (Quartered(?) Polynomial Regression'))
```

 This will be our final model for this lesson.

## TEST PREDICTIONS WITH DIFFERENT MODELS

Now, we will predict a *single, new* result with Simple Linear Regression, of a level = 6.5

```{r}
single_prediction_lin_reg <- data.frame(Level = 6.5)
(final_y_predict_lr <- predict(lin_reg, single_prediction_lin_reg))
```
This prediction says the interviewee's salaray was $330,378.8, so right now he's asking for less, which is good for us, but not accurate.

Check with the polynomial linear regressions.

```{r}
## predict a single new result with Polynomial Linear Regression
single_prediction_poly_reg <- data.frame(Level = 6.5, level_sq = 6.5^2, level_cub = 6.5^3, level_quart = 6.5^4)
(final_y_predict_poly <- predict(poly_reg_4, single_prediction_poly_reg))
```

So our "final" prediction for his previous salary was $158,862.5, so right now he's asking for a bit more, which is good for him, and it's not that much more, so it's not bad for us.

Now, we can go forward with our negotiations on a good note, as this employee was honest.

## NOTES

Whether or not to use a polynomial regression or linear regression depends on a case-by-case basis.