---
title: "NLP"
author: "Steve Newns"
output: html_document
---

```{r, warning=F,message=F}
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
```

We want to analyze restaraunt reviews to predict if a review is a "good" review or a bad "review"

```{r}
## import data 
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "", stringsAsFactors = F)
                          #sep = "\t", 
glimpse(restaraunts)
head(restaraunts)
```

So we have 2 columns, one with the text of the review, and another with the binary 1/0 Label `Liked` of whether a customer liked the restaraunt or did not like the restaraunt.

Now we want to create some variables (predictors) that are equal to all the words appearing in the reviews. We will transform our data set into corpus and then into a sparse matrix where each row is a review, and then we will have a column for each unique word from all of the reviews. The cells will be the count of time each word has appeared in a specific review. We also need to remove the most common words in the English language and words that would not be a predictor of the meaning of a phrase. These are **stop words**, and we need will remove these from our corpus.

```{r}
corpus <- VCorpus(VectorSource(restaraunts$Review)) 
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])

# clean data + reduce # of cols
corpus %<>%
  tm_map(content_transformer(tolower)) %>% # need content_transformer() to keep corpus as a VCorpus, not a list
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords()) %>% # remove English stopwords
  tm_map(stemDocument) %>% # get the ROOTs of each word via stemming
  tm_map(stripWhitespace) # get rid of extra spaces leftover

# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
```

```{r}
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
```

We can see we have 100% sparsity, which means we have so many 0's in the matrix that it displays "100% 0's" in the matrix. We also have 1577 *terms in our 1000 documents/reviews* (words in our sparse matrix). Next, we'll filter this down to only keep the most common words in the documents (reviews) by keeping only those terms that appear more than once. This will also lower the sparsity.
```{r}
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
```
691
We only kept 691 words and sparsity decreased by 1%. The most common classification models used on a document term matrix for NLP are Naive-Bayes, Decision Trees, and Random Forests (also including CART and maximum entropy models). We'll use a random forest classifier.
```{r}
library(randomForest)
library(caTools) # split data

# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))

str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
```

## Fitting Random Forest to the Training set

```{r}
# set.seed(123)
# split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
# training_set = subset(dtm_df, split == TRUE)
# test_set = subset(dtm_df, split == FALSE)
# 
# classifier = randomForest(x = training[-692],
#                           y = training$Liked,
#                           ntree = 10)
# 
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test[-692])
# 
# # Making the Confusion Matrix
# (cm = table(test[, 692], y_pred))
```

```{r}
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)

training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)

cols <- ncol(training)

restaraunts_rf <- randomForest(x = training[-cols], y = training[,cols], ntree = 10)

# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-cols])
head(y_pred_rf)
```

Now we will check our prediction results vs. the actual results with a confusion matrix.

```{r}
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

So we end up with 79 correctly-predicted negative reviews and 70 correctly-predicted positive reviews, giving an accuracy of 74.5%

Now let's do the same with other classification models.

## Logistic Regression

```{r}
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)

# Predict using model on test set labels
temp_y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
y_pred_logreg <- if_else(temp_y_pred_logreg > .5, 1, 0)

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_logreg))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

So logistic regression does much worse, indicating that the data set is not linearly seperable.

## K-NN
```{r}
library(class)
## fit to training data
y_pred_knn <- knn(training[,-cols], test[,-cols], cl = training[,cols], k = 5)

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_knn))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

So kNN does better than logistic regression, but still worse than a random forest. We can play with the parameters though.

## SVM
### Linear
```{r}
library(e1071) # most popular SVM libary, other is `kernlab`

# Fitting Kernel SVM to the Training set
# `type` = *C-classification* to make sure we're doing a classification machine 
# `kernel` = *linear*, as we are starting w/ the most basic kernel, the **linear kernel**.

restaraunts.svm <- svm(Liked ~ ., training, type = 'C-classification', kernel = 'linear')

# Predict on test set results
y_pred_svm <- predict(restaraunts.svm, test[-cols])

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_svm))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```
Linear SVM does better than kNN and logistic regression, *and* random forest.

### Radial
```{r}
library(e1071) # most popular SVM libary, other is `kernlab`

# Fitting Kernel SVM to the Training set
# `type` = *C-classification* to make sure we're doing a classification machine 
# `kernel` = *linear*, as we are starting w/ the most basic kernel, the **linear kernel**.

restaraunts.svm <- svm(Liked ~ ., training, type = 'C-classification', kernel = 'radial')

# Predict on test set results
y_pred_svm <- predict(restaraunts.svm, test[-cols])

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_svm))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

Radial SVM does the worst so far.

## Naive-Bayes
```{r}
restaraunts_nb <- naiveBayes(Liked ~ ., training)

# Predict on test set results
y_pred_nb <- predict(restaraunts_nb, test[-cols])

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_nb))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

Nevermind, Naive-Bayes is the worst.

## Decision Trees
```{r}
library(rpart)
restaraunts_dtc <- rpart(Liked ~ ., training)

# Predict on test set results
y_pred_dtc <- predict(restaraunts_dtc, test[-cols], type = "class")

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_dtc))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```

# Regularized logistic regression
```{r}
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogReg <- glmnet(as.matrix(training[-cols]),
                                        as.matrix(training[cols]), 
                                        family = "binomial") 
```

Compare various settings for `lambda` to see which gives best performance. 

To push through this example quickly, we’ll cheat a bit and test the hyperparameter settings on the test set rather than do repeated splits of the training data. If rigorously testing models, don't make this sort of simplification. For now, just try all values `glmnet` proposed + see which performs best on the test set:
```{r}
lambdas <- restaraunts_regularizedlogReg$lambda

performance <- data.frame()
for (lambda in lambdas) {
 # test model on new test data with each lambda
 temp_predictions <- predict(restaraunts_regularizedlogReg, 
                             as.matrix(test[-cols]),
                             s = lambda)
 predictions <- as.numeric(temp_predictions > 0)
 # calculate MSE
 mse <- mean(predictions != as.matrix(training[cols]))
 # add row back to DF
 performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}

ggplot(performance, aes(Lambda, MSE)) + 
 geom_point() + 
 scale_x_log10()

(final_lambda <- with(performance, max(Lambda[which(MSE == min(MSE))])))
(logreg_mse <- with(subset(performance, Lambda == final_lambda), MSE))
```

```{r}
library(glmnet)
# Predict on test set results
restaraunts_regularizedlogReg <- glmnet(as.matrix(training[-cols]),
                                        as.matrix(training[cols]), 
                                        family = "binomial", 
                                        lambda = final_lambda) 

y_pred_rlr <- predict(restaraunts_regularizedlogReg, test[-cols], type = "class")

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rlr))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```
```{r}
fit <- cv.glmnet(x = as.matrix(training[-cols]),
                 y = as.matrix(training[cols]), 
                 family = 'binomial', 
                 # lasso penalty
                 alpha = 1,
                 # interested area unded ROC curve
                 type.measure = "auc",
                 # 5-fold cross-validation
                 nfolds = 5,
                 # high value, less accurate, but faster training
                 thresh = 1e-3,
                 # again lower number iterations for faster training
                 # in this vignette
                 maxit = 1e3)

y_pred_rlr2 <- predict(fit, test[-cols], type = "class")

## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rlr2))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
```
```