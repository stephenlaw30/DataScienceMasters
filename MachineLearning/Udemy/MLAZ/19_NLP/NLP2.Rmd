---
title: "NLP"
author: "Steve Newns"
output: html_document
---

```{r, warning=F,message=F}
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
```

We want to analyze restaraunt reviews to predict if a review is a "good" review or a bad "review"

```{r}
## import data 
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "",
                          sep = "\t", stringsAsFactors = F)
glimpse(restaraunts)
head(restaraunts)
```

So we have 2 columns, one with the text of the review, and another with the binary 1/0 Label `Liked` of whether a customer liked the restaraunt or did not like the restaraunt.

Now we want to create some variables (predictors) that are equal to all the words appearing in the reviews. We will transform our data set into corpus and then into a sparse matrix where each row is a review, and then we will have a column for each unique word from all of the reviews. The cells will be the count of time each word has appeared in a specific review. We also need to remove the most common words in the English language and words that would not be a predictor of the meaning of a phrase. These are **stop words**, and we need will remove these from our corpus.

```{r}
corpus <- VCorpus(VectorSource(restaraunts$Review)) 
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])

# clean data + reduce # of cols

corpus <- tm_map(content_transformer(tolower))  # need content_transformer() to keep corpus as a VCorpus, not a list
corpus <-   tm_map(removeNumbers) 
  corpus <- tm_map(removePunctuation) 
  corpus <- tm_map(removeWords, stopwords())  # remove English stopwords
  corpus <- tm_map(stemDocument)  # get the ROOTs of each word via stemming
  corpus <- tm_map(stripWhitespace) # get rid of extra spaces leftover

# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
```

```{r}
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
```

We can see we have 100% sparsity, which means we have so many 0's in the matrix that it displays "100% 0's" in the matrix. We also have 1577 *terms in our 1000 documents/reviews* (words in our sparse matrix). Next, we'll filter this down to only keep the most common words in the documents (reviews) by keeping only those terms that appear more than once. This will also lower the sparsity.
```{r}
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
```
691
We only kept 691 words and sparsity decreased by 1%. The most common classification models used on a document term matrix for NLP are Naive-Bayes, Decision Trees, and Random Forests (also including CART and maximum entropy models). We'll use a random forest classifier.
```{r}
library(randomForest)
library(caTools) # split data

# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- restaraunts$Liked
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
```

## Fitting Random Forest to the Training set

```{r}
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)

training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)

restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, nTree = 10)

# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
```


```{r}
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-692])
head(y_pred_rf)
```
Now we will check our prediction results vs. the actual results with a confusion matrix.

```{r}
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
```
