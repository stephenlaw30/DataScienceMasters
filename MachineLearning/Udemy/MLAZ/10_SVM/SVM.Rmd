---
title: "SVM"
author: "Steve Newns"
output: html_document
---

# SVM

```{r, warning=F,message=F}
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
```

Social network`s automotive business client has launched its brand new luxury SUV and purchased ads for its marketing campaign. The social network collected data on the users and whether or not the bought the SUV after seeing the ad.

We will only use age and salary to predict this
```{r}
## import data 
social <- read_csv("Social_Network_Ads.csv")

## Cut down dataset
social <- social %>%
  select(Age,EstimatedSalary,Purchased) %>% # cut down cols
  mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
         # scale data (age is in 10s, salary in 10k's)
         Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
         EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
```

Now to get to the modeling.

```{r}
# split data into Training + Test sets
set.seed(123)

spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here

(training <- social %>%
  subset(spl == TRUE))
(test <- social %>%
  subset(spl == FALSE))
```

## Fitting Kernel SVM to the Training set

We will set `type` to be **C-classification** to make sure we're doing a classification machine and `kernel` to be **linear**, as we are starting w/ the most basic kernel, the **linear kernel**.

Then, we will make predictions on the test set.

```{r}
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')

# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
```

Now we will check our prediction results vs. the actual results with a confusion matrix.
```{r}
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
```

Now we need to visualize the results.
```{r}
## Create function to visualize different set results
plot.svm.model <- function(set) {
  x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
  x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
  
  set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
  colnames(set.grid) <- c("Age","EstimatedSalary")
  
  y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
  
  plot(set[,-3],
       main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
       xlab = "Age",
       ylab = "Estimated Salary",
       xlim = range(x1),
       ylim = range(x2))
  
  contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
  
  points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
  points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
  
}

plot.svm.model(training)
plot.svm.model(test)
```

See that our classifier is indeed linear (a straight line in a 2D space). It also capture most DP's correctly.

*But*, a good number of DP's with lower salaries that were older in fact purchased the SUV when our SVM predicted the wouldn't. This is evident in both the training and test sets.

B/c our classifier is a straight line (linear), we need to introduce a new classifier w/ a new kernel to get this decision boundary to curve/bend to capture more DP's correctly.
