---
title: "Naive Bayes"
author: "Steve Newns"
output: html_document
---

# Kernel SVM

```{r, warning=F,message=F}
library(tidyverse)
library(ggplot2)
library(caTools) # for splitting data
library(e1071) # most popular SVM + Naive Bayes libary, other is `kernlab` (for SVM)
library(ElemStatLearn) 
```

Social network`s automotive business client has launched its brand new luxury SUV and purchased ads for its marketing campaign. The social network collected data on the users and whether or not the bought the SUV after seeing the ad.

We will only use age and salary to predict this
```{r}
## import data 
social <- read.csv("../data/Social_Network_Ads.csv")

## Cut down dataset
social <- social %>%
  select(Age,EstimatedSalary,Purchased) %>% # cut down cols
  mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
         # scale data (age is in 10s, salary in 10k's)
         Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
         EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
```

Now to get to the modeling.

```{r}
# split data into Training + Test sets
set.seed(123)

spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here

(training <- social %>%
  subset(spl == TRUE))
(test <- social %>%
  subset(spl == FALSE))
```

## Fitting Naive Baues to the Training set

We will set `type` to be **C-classification** to make sure we're doing a classification machine and `kernel` to be **linear**, as we are starting w/ the most basic kernel, the **linear kernel**.

Then, we will make predictions on the test set.

```{r}
social.nb <- naiveBayes(Purchased ~ ., training)

# Predict on test set results
y_pred.nb <- predict(social.nb, test[-3])
```

Now we will check our prediction results vs. the actual results with a confusion matrix.
```{r}
## Making the Confusion Matrix
(cm <- table(test$Purchased, y_pred.nb))

sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n

cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or Recall): ",specificity)
cat("\nPrecision (Positive Predictive Value): ",precision)
cat("\nAccuracy: ",accuracy)
```

Now we need to visualize the results.

```{r}
## Create function to visualize different set results
plot.nb.model <- function(set) {
  x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
  x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
  
  set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
  colnames(set.grid) <- c("Age","EstimatedSalary")
  
  y_pred.svm.grid <- predict(social.nb, set.grid) # use grid to predict values w/ our model
  
  plot(set[,-3],
       main = c("Naive Bayes (", deparse(substitute(set), nlines = 1),
                " set)"), # get set name from argument given
       xlab = "Age",
       ylab = "Estimated Salary",
       xlim = range(x1),
       ylim = range(x2))
  
  contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
  
  points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
  points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
  
}

plot.nb.model(training)
plot.nb.model(test)
```

See that our classifier is indeed making better predictions than linear classifiers because it can curve. It's capturing more points in the correct regions than the linear classifiers.