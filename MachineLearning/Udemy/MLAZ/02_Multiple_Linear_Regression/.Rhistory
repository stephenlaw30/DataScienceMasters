library(SDSFoundations)
post <- PostSurvey
dim(post
dim(post)
dim(post)
nrow(post)
count(unique(post$ID))
unique(post$ID)
length(unique(post$ID))
head(post)
head(post[post$gender == 'Male'])
head(post[post$gender == 'Male',])
head(post[post$gender == 'Male','Classification'])
head(post[post$gender == 'Male',Classification])
head(post[post$gender == 'Male','Classification'])
head(post[post$gender == 'Male'])
head(post[post$gender == 'Male',])
head(post,10)
post[1:10,]
first10 <- post[1:10,]
length(first10[first10$live_campus == 'yes',])
first10 <- post[1:10,]
first10
first10[first10$live_campus == 'yes',]
nrow(first10[first10$live_campus == 'yes',])
nrow(first10[first10$live_campus == 'yes',])/nrow(first10)
names(pos)
names(post)
head(post[post$gender == 'Male',])
'Pre-Lab 3: Post Student-Survey Data
Students at UT at Austin answered a set of questions at the beginning + end of the semester.
We will use this data to compare different groups + to explore what has (or has not)
changed over time for these students.
******Primary Research Questions*****
1.  Who is happier at the beginning of the semester:  under-classmen or upper-classmen?
2.  Does student happiness change from the beginning of the semester to the end?
Begin by examining the data in R.'
library(SDSFoundations)
post <- PostSurvey
'1a. How many students are in the dataset?'
dim(post)
length(unique(post$ID)) #214
'1b. What is the classification of the 1st male student?'
head(post[post$gender == 'Male',]) #Freshman
'1c. Of the 1st 10 students in the dataset, what % live on campus?'
first10 <- post[1:10,]
nrow(first10[first10$live_campus == 'yes',])/nrow(first10) # 50%
'Check the Variables of Interest to find the variables we need to answer the question.
2a. Which variable tells us whether a student is a lower-classman (freshman or sophomore)?'
names(post) # classification --> categorical
'2b. Which variable tells us how happy students were at the beginning of the semester?'
#happy --> quantitative
'2c. Which variable tells us how happy students were at the end of the semester?'
# post_happy --> quantitative
'***Which method should we be using for this analysis and why?***
3a. We will use an independent t-test to help us compare the *happiness of the under +
upper-classmen*. Why?'
# We want to compare the happiness of two different populations of students.
'3b. We will use a dependent t-test to help us determine *whether happiness levels
changed over the semester*. Why?'
table(post$classification)
post[post$classification == 'Freshman' | post$classification == 'Sophomore',]
happy_under <- post[post$classification == 'Freshman' | post$classification == 'Sophomore',]
happy_upper <- post[post$classification == 'Junior' | post$classification == 'Senior',]
'Pre-Lab 3: Post Student-Survey Data
Students at UT at Austin answered a set of questions at the beginning + end of the semester.
We will use this data to compare different groups + to explore what has (or has not)
changed over time for these students.
******Primary Research Questions*****
1.  Who is happier at the beginning of the semester:  under-classmen or upper-classmen?
2.  Does student happiness change from the beginning of the semester to the end?
Begin by examining the data in R.'
library(SDSFoundations)
post <- PostSurvey
'1a. How many students are in the dataset?'
dim(post)
length(unique(post$ID)) #214
'1b. What is the classification of the 1st male student?'
head(post[post$gender == 'Male',]) #Freshman
'1c. Of the 1st 10 students in the dataset, what % live on campus?'
first10 <- post[1:10,]
nrow(first10[first10$live_campus == 'yes',])/nrow(first10)
'Pre-Lab 3: Post Student-Survey Data
Students at UT at Austin answered a set of questions at the beginning + end of the semester.
We will use this data to compare different groups + to explore what has (or has not)
changed over time for these students.
******Primary Research Questions*****
1.  Who is happier at the beginning of the semester:  under-classmen or upper-classmen?
2.  Does student happiness change from the beginning of the semester to the end?
Begin by examining the data in R.'
library(SDSFoundations)
post <- PostSurvey
'1a. How many students are in the dataset?'
dim(post)
length(unique(post$ID)) #214
'1b. What is the classification of the 1st male student?'
head(post[post$gender == 'Male',]) #Freshman
'1c. Of the 1st 10 students in the dataset, what % live on campus?'
first10 <- post[1:10,]
nrow(first10[first10$live_campus == 'yes',])/nrow(first10)
happy_under <- post[post$classification == 'Freshman' | post$classification == 'Sophomore',]
happy_upper <- post[post$classification == 'Junior' | post$classification == 'Senior',]
ggplot(happy_under) + geom_histogram(aes(x = happiness))
library(ggplot2)
ggplot(happy_under) + geom_histogram(aes(x = happiness))
happy_under
ggplot(happy_under) + geom_histogram(aes(x = happy))
ggplot(happy_under) + geom_histogram(aes(x = happy), bins = 5)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 5)
mu.un <- mean(happy_under$happy)
mu.up <- mean(happy_upper$happy)
s.un <- sd(happy_under$happy)
s.up <- sd(happy_upper$happy)
n.un <- nrow(happy_under)
n.up <- nrow(happy_upper)
se <- sqrt(((s.un^2)/n.un) + ((s.up^2)/2))
t <- (mu.un - mu.up)/se
happy_upper
nrow(happy_upper)
nrow(happy_under)
happy_under
(s.un^2)/n.un)
((s.un^2)/n.un)
s.un^2
s.un
s.up
((s.un^2)/n.un)
((s.up^2)/2)
s.up^2
se <- sqrt(((s.un^2)/n.un) + ((s.up^2)/n.up))
t <- (mu.un - mu.up)/se
((s.up^2)/n.up)
((s.un^2)/n.un)
n.un
n.up
((s.un^2)/n.un)
((s.up^2)/n.up)
se <- sqrt(((s.un^2)/n.un) + ((s.up^2)/n.up))
t <- (mu.un - mu.up)/se
t.test(underclass_happy, upperclass_happy)
t.test(happy_under, happy_upper)
t.test(happy_under$happy, happy_upper$happy)
post$happy_diff <-post$happy - post$post_happ
ggplot(post) + geom_histogram(aes(x = happy_diff), bins = 5)
ggplot(post) + geom_histogram(aes(x = happy_diff), bins = 15)
t.test(post$happy_diff)
mu.d <- mean(post$happy_diff)
s.d <- sd(post$happy_diff)
n.d <- nrow(post)
se.d <- s.d/sqrt(n.d)
t <- (mu.d)/se.d
t
post
t
n.d
t
mu.un <- mean(happy_under$happy)
mu.up <- mean(happy_upper$happy)
s.un <- sd(happy_under$happy)
s.up <- sd(happy_upper$happy)
n.un <- nrow(happy_under)
n.up <- nrow(happy_upper)
se <- sqrt(((s.un^2)/n.un) + ((s.up^2)/n.up))
t <- (mu.un - mu.up)/se
t
mu.un
round(mu.un ,1)
round(mean(happy_under$happy),1)
round(mu.up,1)
round((mu.un - mu.up)/se,3)
nrow(post)
mean(post$happy_diff)
round((mu.d)/se.d,3)
ggplot(happy_under) + geom_histogram(aes(x = happy), bins = 5)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 5)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 10)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 12)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 8)
ggplot(happy_under) + geom_histogram(aes(x = happy), bins = 11)
ggplot(happy_upper) + geom_histogram(aes(x = happy), bins = 8)
ggplot(post) + geom_histogram(aes(x = happy_diff), bins = 15)
library(SDSFoundations)
post <- PostSurvey
mean((post$hw_hours_college - post$hw_hours_HS)
mean(post$hw_hours_college - post$hw_hours_HS)
mean((post$hw_hours_college - post$hw_hours_HS))
mean(post$hw_hours_college) - mean(post$hw_hours_HS)
round(mean((post$hw_hours_college - post$hw_hours_HS)),2)
round(mean(post$hw_hours_college) - mean(post$hw_hours_HS),2)
# both = 10.94626
library(SDSFoundations)
post <- PostSurvey
library(SDSFoundations)
post <- PostSurvey
library(SDSFoundations)
post <- PostSurvey
round(mean((post$hw_hours_college - post$hw_hours_HS)),2)
round(mean(post$hw_hours_college) - mean(post$hw_hours_HS),2)
# both = 10.94626 = 10.95
avg_hw_diff <- round(mean((post$hw_hours_college - post$hw_hours_HS)),2)
t.test(avg_hw_diff)
avg_hw_diff
hw_diff <- (post$hw_hours_college - post$hw_hours_HS)
hw_diff
t.test(
t.test(avg_hw_diff)
t.test(avg_hw_diff)
t.test(hw_diff)
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'less')
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'greater')
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'greater')
t.test(post$hw_hours_college, post$hw_hours_HS, alternative = 'less')
t.test(hw_diff)
post$greek
greek <- post[post$greek == 'yes']
non.greek <- post[post$greek == 'no']
greek <- post[post$greek == 'yes',]
non.greek <- post[post$greek == 'no',]
round(mean((greek$sleep_Sat - non.greek$sleep_Sat)),1)
round(mean(greek$sleep_Sat) - mean(non.greek$sleep_Sat),1)
t.test(greek$sleep_Sat, non.greek$sleep_Sat)
t.test(greek$sleep_Sat, non.greek$sleep_Sat, alternative = 'less')
hist(hw_diff)
hist(greek$sleep_Sat)
hist(non.greek$sleep_Sat)
hist(hw_diff)
library(SDSFoundations)
post <- PostSurvey
post$hw_diff <- post$hw_hours_college - post$hw_hours_HS
table(post$major)
hw_diff_nursing <- post$hw_diff[post$major == 'Nursing']
hw_diff_bio <- post$hw_diff[post$major == 'Biology']
library(ggplot2)
ggplot() + geom_histogram(aes(hw_diff_bio))
ggplot() + geom_histogram(aes(hw_diff_bio, bins = 5))
ggplot() + geom_histogram(aes(hw_diff_bio, binwidth = 2))
ggplot() + geom_histogram(aes(hw_diff_bio), binwidth = 2)
ggplot() + geom_histogram(aes(hw_diff_bio), binwidth = 1)
ggplot() + geom_histogram(aes(hw_diff_bio), binwidth = 4)
ggplot() + geom_histogram(aes(hw_diff_bio), binwidth = 5)
ggplot() + geom_histogram(aes(hw_diff_bio), binwidth = 4)
ggplot() + geom_histogram(aes(hw_diff_nursing), binwidth = 4)
t.test(hw_diff_bio,hw_diff_nursing)
n.s <- 26
n.ns <- 32
dF <- n.s - 1
mu.s <- 80
mu.ns <- 74
s.s <- 5
s.ns <- 6
s.ns^2
s.s^2
n.s
n.ns
s.ns^2
(s.s^2/n.s)
(s.ns^2/n.ns)
se <- sqrt((s.s^2/n.s) + (s.ns^2/n.ns))
se
se <- round(sqrt((s.s^2/n.s) + (s.ns^2/n.ns)),2)
se
t <- (mu.s - mu.ns)/se
t.crit <- 1.708
t > t.crit
mu.s
mu.ns
se
t
t <- round((mu.s - mu.ns)/se,2)
t
'Business Problem - Venture Capatalism
Our VC has data on 50 companies, including extracts from their P&L for the given financial
year. So we know how much they spent on R&D, on Admin costs (salaries), and on Marketing.
Then we also know their profits and which state they operate in.
We want to be able to predict profits based on the expenses of a company, as well
as the state the operate in via a multiple linear regression model.
Therefore, the VC will now which new companies they would want to invest in.
This will also depend on if the VC is more concerned with each of the spends,
and which would yield more profits.'
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/MachineLearning/Udemy/02_Multiple_Linear_Regression')
# library for splitting data
library(caTools)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
dim(startups)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (the model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
'Residuals:
Min     1Q Median     3Q    Max
-33128  -4865      5   6098  18065
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)      4.965e+04  7.637e+03   6.501 1.94e-07 ***
R.D.Spend        7.986e-01  5.604e-02  14.251 6.70e-16 ***
Administration  -2.942e-02  5.828e-02  -0.505    0.617
Marketing.Spend  3.268e-02  2.127e-02   1.537    0.134
State2           1.213e+02  3.751e+03   0.032    0.974
State3           2.376e+02  4.127e+03   0.058    0.954
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 9908 on 34 degrees of freedom
Multiple R-squared:  0.9499,	Adjusted R-squared:  0.9425
F-statistic:   129 on 5 and 34 DF,  p-value: < 2.2e-163
F-statistic: 494.8 on 1 and 18 DF,  p-value: 1.524e-14
See that only R&D spend is significant, but State2 has highest p-value, and
Administration costs seem to decrease profit as it increases
********MODEL 2**************'
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
'Residuals:
Min     1Q Median     3Q    Max
-34334  -4894   -340   6752  17147
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 4.902e+04  2.748e+03   17.84   <2e-16 ***
R.D.Spend   8.563e-01  3.357e-02   25.51   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 9836 on 38 degrees of freedom
Multiple R-squared:  0.9448,	Adjusted R-squared:  0.9434
F-statistic: 650.8 on 1 and 38 DF,  p-value: < 2.2e-16
********PREDICTIONS**************
Now we need to use the model to train it on the test data'
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
'Now that we have some predictions of profits based on our linear model, we can check
our accuracy of these predictions, and therefore of our model
*******VISUALIZE TRAINING SET RESULTS**********'
library(ggplot2)
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Test Set)')
'We wanted to use the same regression line for both graphs, since our
regression model was created using the training set.
Looking at the test set plot, most of our predictions look good
********BACKWARDS ELIMINATION**************'
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
# State 2 and 3 have the highest p-values, so they have minimal impact on profits,
# Remove both by removing "State" from the model to remove dummy variables
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
# now Admin is highest p-value
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
# See that Marketing spend now is *barely significant*. Should we remove it?
# this is an arbitrary choice, depending on the significance level
# remove it for now
summary(regressor.RD)
# so we only have 1 variable + its significant, but Adj R2 has decreased (.9468 -> .9434)
'********USE FINAL MODEL ON ACTUAL DATASET**************'
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
'Residuals:
Min     1Q Median     3Q    Max
-34351  -4626   -375   6249  17188
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 4.903e+04  2.538e+03   19.32   <2e-16 ***
R.D.Spend   8.543e-01  2.931e-02   29.15   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 9416 on 48 degrees of freedom
Multiple R-squared:  0.9465,	Adjusted R-squared:  0.9454
F-statistic: 849.8 on 1 and 48 DF,  p-value: < 2.2e-16'
'Business Problem - Venture Capatalism
Our VC has data on 50 companies, including extracts from their P&L for the given financial
year. So we know how much they spent on R&D, on Admin costs (salaries), and on Marketing.
Then we also know their profits and which state they operate in.
We want to be able to predict profits based on the expenses of a company, as well
as the state the operate in via a multiple linear regression model.
Therefore, the VC will now which new companies they would want to invest in.
This will also depend on if the VC is more concerned with each of the spends,
and which would yield more profits.'
#setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/MachineLearning/Udemy/02_Multiple_Linear_Regression')
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/MachineLearning/Udemy/02_Multiple_Linear_Regression')
# library for splitting data
library(caTools)
# inspect
startups <- read.csv('50_Startups.csv')
head(startups)
summary(startups)
dim(startups)
# encode categorical data (State)
startups$State <- factor(startups$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3))
head(startups)
# split data
set.seed(123)
spl <- sample.split(startups$Profit, SplitRatio = .8) # 40 in train, 10 in test
training <- subset(startups, spl == T)
test <- subset(startups, spl == F)
dim(training)
dim(test)
# create regressor (the model) and do backwards elimination
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
'Residuals:
Min     1Q Median     3Q    Max
-33128  -4865      5   6098  18065
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)      4.965e+04  7.637e+03   6.501 1.94e-07 ***
R.D.Spend        7.986e-01  5.604e-02  14.251 6.70e-16 ***
Administration  -2.942e-02  5.828e-02  -0.505    0.617
Marketing.Spend  3.268e-02  2.127e-02   1.537    0.134
State2           1.213e+02  3.751e+03   0.032    0.974
State3           2.376e+02  4.127e+03   0.058    0.954
---
Residual standard error: 9908 on 34 degrees of freedom
Multiple R-squared:  0.9499,	Adjusted R-squared:  0.9425
F-statistic:   129 on 5 and 34 DF,  p-value: < 2.2e-163
F-statistic: 494.8 on 1 and 18 DF,  p-value: 1.524e-14
See that only R&D spend is significant, but State2 has highest p-value, and
Administration costs seem to decrease profit as it increases
********MODEL 2**************'
regressor.RD <- lm(Profit ~ R.D.Spend, training)
summary(regressor.RD)
'Residuals:
Min     1Q Median     3Q    Max
-34334  -4894   -340   6752  17147
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 4.902e+04  2.748e+03   17.84   <2e-16 ***
R.D.Spend   8.563e-01  3.357e-02   25.51   <2e-16 ***
---
Residual standard error: 9836 on 38 degrees of freedom
Multiple R-squared:  0.9448,	Adjusted R-squared:  0.9434
F-statistic: 650.8 on 1 and 38 DF,  p-value: < 2.2e-16
********PREDICTIONS**************
Now we need to use the model to train it on the test data'
y.pred.train <- predict(regressor.RD, training)
y.pred.test <- predict(regressor.RD, test)
# compare test w/ actuals
y.pred.test
test$Profit[0:10]
'Now that we have some predictions of profits based on our linear model, we can check
our accuracy of these predictions, and therefore of our model
*******VISUALIZE TRAINING SET RESULTS**********'
library(ggplot2)
ggplot(training) +
geom_point(aes(R.D.Spend, Profit), colour = 'green') + # plot points
geom_line(aes(R.D.Spend, y.pred.train), colour = 'red') + # plot regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Training Set)')
ggplot() +
geom_point(aes(test$R.D.Spend, test$Profit), colour = 'green') + # plot points
geom_line(aes(training$R.D.Spend, y.pred.train), colour = 'red') + # plot same regression line
xlab('R&D Spend ($)') +
ylab('Profits ($)')
ggtitle('Profits by Amount of R&D Spend (Test Set)')
'We wanted to use the same regression line for both graphs, since our
regression model was created using the training set.
Looking at the test set plot, most of our predictions look good
********BACKWARDS ELIMINATION**************'
regressor.all <- lm(Profit ~ ., training)
summary(regressor.all)
# State 2 and 3 have the highest p-values, so they have minimal impact on profits,
# Remove both by removing "State" from the model to remove dummy variables
regressor.noState <- lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, training)
summary(regressor.noState)
# now Admin is highest p-value
regressor.noStateAdmin <- lm(Profit ~ R.D.Spend + Marketing.Spend, training)
summary(regressor.noStateAdmin)
# See that Marketing spend now is *barely significant*. Should we remove it?
# this is an arbitrary choice, depending on the significance level
# remove it for now
summary(regressor.RD)
# so we only have 1 variable + its significant, but Adj R2 has decreased (.9468 -> .9434)
'********USE FINAL MODEL ON ACTUAL DATASET**************'
finalModel.RD <- lm(Profit ~ R.D.Spend, startups)
summary(finalModel.RD)
'Residuals:
Min     1Q Median     3Q    Max
-34351  -4626   -375   6249  17188
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept) 4.903e+04  2.538e+03   19.32   <2e-16 ***
R.D.Spend   8.543e-01  2.931e-02   29.15   <2e-16 ***
---
Residual standard error: 9416 on 48 degrees of freedom
Multiple R-squared:  0.9465,	Adjusted R-squared:  0.9454
F-statistic: 849.8 on 1 and 48 DF,  p-value: < 2.2e-16'
