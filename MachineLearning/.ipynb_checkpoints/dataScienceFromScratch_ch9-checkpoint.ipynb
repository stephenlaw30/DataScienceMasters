{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9. Getting Data\n",
    "\n",
    "### stdin, stdout\n",
    " \n",
    "If running Python scripts at the command line, you can **pipe** (`|`) data through them using `sys.stdin` and `sys.stdout`. \n",
    "Ex: Scripts that reads in lines of text + spits back out the ones that match a RegEx and counts the lines it recieves + writes out that count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# egrep.py\n",
    "import sys, re\n",
    "\n",
    "# sys.argv = list of CLI args\n",
    "# sys.argv[0] = name of the program itself\n",
    "# sys.argv[1] = RegEx specified at the CL\n",
    "regex = sys.argv[1]\n",
    "\n",
    "# for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    # if matches RegEx, write to stdout\n",
    "    if re.search(regex,line):\n",
    "        sys.stdout.write(line)\n",
    "        \n",
    "# line_count.py\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "# print goes to stdout\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use these to count how many lines in a file contain a #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# windows\n",
    "#!type someFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n",
    "\n",
    "# linunx\n",
    "#!cat someFile.txt | python egrep.py \"[0-9]\" | python line_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: most_common_words.py num_words\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEWNSS\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# script that counts words in its input + writes out most common ones:\n",
    "# most_common_words.py\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# pass in number of words as 1st arg\n",
    "try:\n",
    "    num_words = int(sys.argv[1])\n",
    "except:\n",
    "    print(\"usage: most_common_words.py num_words\")\n",
    "    sys.exit(1) # non-zero exit code = indicates error\n",
    "\n",
    "counter = Counter(word.lower()\n",
    "                 for line in sys.stdin\n",
    "                 for word in line.strip().split() # split on space \n",
    "                 if word) # skip empty words\n",
    "\n",
    "for word,count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "### Ex of above script\n",
    "## type the_bible.txt | python most_common_words.py 10\n",
    "# 64193 the\n",
    "# 51380 and\n",
    "# 34753 of\n",
    "# 13643 to\n",
    "# 12799 that\n",
    "# 12560 in\n",
    "# 10263 he\n",
    "# 9840 shall\n",
    "# 8987 unto\n",
    "# 8836 for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files\n",
    "\n",
    "Can also explicitly read from + write to files directly in Python code.\n",
    "\n",
    "### Basics of Text Files\n",
    "\n",
    "1st step to working with a text file = **obtain a *file object* via `open`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reading_file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d0eb14981c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# 'r' = read only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_for_reading\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reading_file.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# 'w' = write = ***DESTROYS files if it already exits****\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_for_writing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"writing_file.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reading_file.txt'"
     ]
    }
   ],
   "source": [
    "# 'r' = read only\n",
    "file_for_reading = open(\"reading_file.txt\", \"r\")\n",
    "\n",
    "# 'w' = write = ***DESTROYS files if it already exits****\n",
    "file_for_writing = open(\"writing_file.txt\", \"w\")\n",
    "\n",
    "# 'a' = append (to end of file)\n",
    "file_for_appending = open(\"appending_file.txt\", \"a\")\n",
    "\n",
    "# close files when done\n",
    "file_for_writing.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to forget to close files, so always use them in a **`with` block**, at the end of which they will be closed automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-63ab50242c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction_to_get_data_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# at this point, file has been closed, don't try to use it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "with open(filename,'r') as f:\n",
    "    data=function_to_get_data_from(f)\n",
    "    \n",
    "# at this point, file has been closed, don't try to use it\n",
    "process(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read over whole text file, iterate over lines with `for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-79ca7b9f05c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstarts_with_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^#\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# check if each line starts with # and count if True\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.txt'"
     ]
    }
   ],
   "source": [
    "starts_with_hash = 0\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    for line in file:\n",
    "        if re.match(\"^#\",line): # check if each line starts with # and count if True\n",
    "            starts_with_hash += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line you get this way ends in a **newline** character, so you’ll often want to `strip()` it before doing anything with it.\n",
    "\n",
    "Ex: You have a file full of email addresses, 1 per line, that you need to generate a histogram of the domains. The rules for correctly extracting domains are somewhat subtle (e.g., the Public Suffix List), but a good 1st approximation = just take the parts of the email addresses that come after the @ (Which gives the wrong answer\n",
    "for email addresses like \"@mail.datasciencester.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'email_address.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a3ebd8b932cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0memail_address\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"@\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"email_address.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     domain_counts - Counter(get_domain(line.strip())\n\u001b[1;32m      7\u001b[0m                            \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'email_address.txt'"
     ]
    }
   ],
   "source": [
    "def get_domain(email_address):\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "with open(\"email_address.txt\", \"r\") as f:\n",
    "    domain_counts - Counter(get_domain(line.strip())\n",
    "                           for line in f\n",
    "                           if \"@\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimited Files\n",
    "\n",
    "More often we work w/ files w/ lots of data on each line that're very often either comma or tab-separated. Each line has several fields, w/ a comma/tab indicating where 1 field ends + the next starts.\n",
    "\n",
    "This gets complicated when you have fields w/ commas + tabs + newlines in them. For this reason, it’s pretty much always a mistake to try to parse them yourself. Instead, use Python’s `csv` module (or `pandas` library).\n",
    "\n",
    "For technical reasons, always work w/ CSV files in **binary mode** by including a `b` after the `r` or `w` (see Stack Overflow).\n",
    "\n",
    "If your file has no headers (which means you probably want each row as a list, + which places the burden on you to know what’s in each column), you can use `csv.reader` to iterate over rows, each of which will be an appropriately split list.\n",
    "\n",
    "Ex: TSV of stock prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "6/19/2014 AAPL 91.86\n",
      "6/19/2014 MSFT 41.51\n",
      "6/19/2014 FB 64.34\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process(date, symbol, price):\n",
    "    print(date, symbol, price)\n",
    "\n",
    "with open(\"tab_delimited_stock_prices.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    \n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file has headers, we can skip them (with an inital call to `reader.next()`) or get each row as a `dict` (with headers as keys) by using `csv.DictReader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process(date, symbol, price):\n",
    "    print(date, symbol, price)\n",
    "\n",
    "with open(\"colon_delimited_stock_prices.txt\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\":\")\n",
    "    \n",
    "    for row in reader:\n",
    "        date = row[\"date\"]\n",
    "        symbol = row[\"symbol\"]\n",
    "        closing_price = float(row[\"closing_price\"])\n",
    "        process(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the file doesn’t have headers we can still use `DictReader` by passing it the keys as a `fieldnames` parameter, + we can similarly write out delimited data using `csv.writer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "todays_prices = {\"AAPL\":90.91,\"MSFT\":41.68,\"FB\":64.5}\n",
    "\n",
    "with open(\"comma_delimited_stock_prices.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\",\")\n",
    "    \n",
    "    for stock,price in todays_prices.items():\n",
    "        writer.writerow([stock,price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`csv.writer` does the right thing if fields themselves have commas in them. Your own hand-rolled writer probably won’t. For example, if you attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0d6cd8a135ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad_csv.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# might have too many commas in it\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# row might already have newlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tow' is not defined"
     ]
    }
   ],
   "source": [
    "results = [[\"test1\", \"success\", \"Monday\"],\n",
    "           [\"test2\", \"success, kind of\", \"Tuesday\"],\n",
    "           [\"test3\", \"failure, kind of\", \"Wednesday\"],\n",
    "           [\"test4\", \"failure, utter\", \"Thursday\"]]\n",
    "\n",
    "# BAD - DON'T DO\n",
    "with open(\"bad_csv.txt\",\"wb\") as f:\n",
    "    for row in results:\n",
    "        f.write(\",\".join(map(str,tow))) # might have too many commas in it\n",
    "        f.write(\"\\n\") # row might already have newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will end up with a csv file no one will ever be able to make sense of that looks like:\n",
    "\n",
    "* test1,success,Monday\n",
    "* test2,success, kind of,Tuesday\n",
    "* test3,failure, kind of,Wednesday\n",
    "* test4,failure, utter,Thursday\n",
    "\n",
    "\n",
    "### Scraping the Web\n",
    "\n",
    "Fetching web pages = easym, getting meaningful structured info out of them = less so\n",
    "\n",
    "#### HTML and the Parsing Thereof\n",
    "\n",
    "Pages on the Web = written in HTML, in which text is (ideally) marked up into **elements** + **their attributes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A web page</title>\\n</head>\\n<body>\\n<p id=\"author\">Joel Grus</p>\\n<p id=\"subject\">Data Science</p>\\n</body>\\n</html>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''<html>\n",
    "<head>\n",
    "<title>A web page</title>\n",
    "</head>\n",
    "<body>\n",
    "<p id=\"author\">Joel Grus</p>\n",
    "<p id=\"subject\">Data Science</p>\n",
    "</body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world where all web pages are marked up semantically for our benefit, we'd be able to extract data using rules like “find the `<p>` element whose `id` = \"subject\"\n",
    "+ return the text it contains.” In the actual world, HTML is not generally well-formed,\n",
    "let alone annotated. This means we’ll need help making sense of it.\n",
    "\n",
    "To get data out of HTML, use **BeautifulSoup library** = builds a tree out of  the various elements on a web page + provides a simple interface for accessing them + use the **requests library** = a much nicer way of making HTTP requests than anything built into Python, + use **html5lib** as an HTML parser since Python’s built-in HTML parser = not that lenient = AKA doesn’t always cope well w/ HTML that’s not perfectly formed.\n",
    "\n",
    "To use Beautiful Soup = pass some HTML into `BeautifulSoup()` function, which will come from the result of a call to `requests.get()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html = requests.get(\"http://www.google.com\").text\n",
    "soup = BeautifulSoup(html,\"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we can get pretty far using a few simple methods. We’ll typically work with **Tag objects** = correspond to the tags representing the structure of an HTML page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p style=\"color:#767676;font-size:8pt\">© 2018 - <a href=\"/intl/en/policies/privacy/\">Privacy</a> - <a href=\"/intl/en/policies/terms/\">Terms</a></p>\n",
      "<p style=\"color:#767676;font-size:8pt\">© 2018 - <a href=\"/intl/en/policies/privacy/\">Privacy</a> - <a href=\"/intl/en/policies/terms/\">Terms</a></p>\n",
      "\n",
      " © 2018 - Privacy - Terms ['©', '2018', '-', 'Privacy', '-', 'Terms']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bf858a648bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;31m# extract tag's attributes via treating it like a `dict`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfirst_p_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m      \u001b[1;31m# returns KeyError if no 'id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\NEWNSS\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \"\"\"tag[key] returns the value of the 'key' attribute for the tag,\n\u001b[1;32m   1010\u001b[0m         and throws an exception if it's not there.\"\"\"\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "# find 1st paragraph + its contents\n",
    "first_paragraph = soup.find('p')\n",
    "\n",
    "# second way\n",
    "first_paragraph2 = soup.p\n",
    "\n",
    "print(first_paragraph)\n",
    "print(first_paragraph2)\n",
    "\n",
    "# get text contents of a Tag via its `text` property\n",
    "first_paragraph_txt = soup.p.text\n",
    "# split the text into seperate words\n",
    "first_paragraph_words = soup.p.text.split()\n",
    "print(\"\\n\",first_paragraph_txt,first_paragraph_words)\n",
    "\n",
    "# extract tag's attributes via treating it like a `dict`\n",
    "first_p_id = soup.p['id']      # returns KeyError if no 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " None\n",
      "\n",
      " [<p style=\"color:#767676;font-size:8pt\">© 2018 - <a href=\"/intl/en/policies/privacy/\">Privacy</a> - <a href=\"/intl/en/policies/terms/\">Terms</a></p>] \n",
      " []\n"
     ]
    }
   ],
   "source": [
    "first_p_id2 = soup.p.get('id') # returns None if no 'id'\n",
    "\n",
    "print(\"\\n\",first_p_id2)\n",
    "\n",
    "# get multiple tags at once\n",
    "all_paragraphs = soup.find_all('p') # or use soup('p')\n",
    "paragraphs_with_ids = [p for p in soup('p') if p.get('id')]\n",
    "\n",
    "print(\"\\n\",all_paragraphs,\"\\n\",paragraphs_with_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can find tags with a specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impt_paragraphs = soup('p', {'class': 'important'})\n",
    "impt_paragraphs2 = soup('p','important')\n",
    "impt_paragraphs3 = [p for p in soup('p')\n",
    "                    if 'important' in p.get('class', [])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can combine these to implement more elaborate logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## find every <span> element contained inside a <div> element\n",
    "#     - warning, will return the same span multiple times\n",
    "#        if it sits inside multiple divs\n",
    "#     - be more clever if that's the case\n",
    "spans_inside_div = [span\n",
    "                   for div in soup('div')\n",
    "                   for span in div('span')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just this handful of features will allow us to do quite a lot. If you end up needing to do\n",
    "more-complicated things, check the documentation.\n",
    "\n",
    "Will need to carefully inspect the source HTML, reason through selection logic, + worry about edge cases to make sure your data is correct. Let’s look at an example: **O’Reilly Books About Data**.\n",
    "\n",
    "A potential investor in our social network thinks data is just a fad. To prove him wrong, you decide to examine how many data books O’Reilly has published over time. After digging\n",
    "through its website, you find it has many pages of data books + videos, reachable through 30-items-at-a-time directory pages with URLs like:\n",
    "http://shop.oreilly.com/category/browse-subjects/data.do?sortby=publicationDate&page=1.\n",
    "\n",
    "Unless you want to be a jerk + you want your scraper to get banned, before scraping data from a website, 1st check to see if it has some sort of access policy. Looking at:\n",
    "http://oreilly.com/terms/, there seems to be nothing prohibiting this project. In order to be good citizens, we should also check for a `robots.txt` file, which tells webcrawlers how to behave. The important lines in http://shop.oreilly.com/robots.txt are:\n",
    "\n",
    "* `Crawl-delay: 30` == should wait 30 seconds between requests\n",
    "* `Request-rate: 1/30` == should request only one page every 30 seconds\n",
    "\n",
    "Basically 2 different ways of saying the same thing. (There're other lines that indicate directories not to scrape, but they don’t include our URL, so we’re OK.)\n",
    "\n",
    "* ***NOTE***: There’s always the possibility that O’Reilly will at some point revamp its website + break all logic in this section.\n",
    "\n",
    "To figure out how to extract the data, let’s download one of those pages and feed it to\n",
    "Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ssearch.oreilly.com/?q=data\"\n",
    "soup = BeautifulSoup(requests.get(url).text,'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we view the source of the page , we’ll see each book (or video) seems to be uniquely contained in a `<td>` table cell element whose class = `thumbtext`. Therefore, a good first step = find all `td` thumbtext tag elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "articles = soup('article','result product-result')\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’d like to filter out videos. (The would-be investor is only impressed by\n",
    "books.) If we inspect the HTML further, we see that each article contains 1+ `a` elements whose `class = \"book\"`, and whose text looks like `Ebook`: or `Video`: or\n",
    "`Print`:. It appears the videos contain only one `pricelabel`, whose text starts with\n",
    "`Video` (after removing leading spaces). This means we can test for videos with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def is_video(article):\n",
    "    \"\"\"Is video if only 1 pricelabel + if stripped text inside \n",
    "    pricelabel starts w/ 'Video'\"\"\"\n",
    "    pricelabel = article('a','book')\n",
    "    return (len(pricelabel)==1 and\n",
    "           pricelabel[0].text.strip().startswith(\"Video\"))\n",
    "\n",
    "print(len([article for article in articles if not is_video(article)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to start pulling data out of the td elements. It looks like the book title is the text inside an `<a>` tag withing an `<p>` class=\"title\"> tag inside the `<div class=\"book_text\">`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Interactive Data Visualization for the Web', 'Data Preparation in the Big Data Era', 'Data Driven: Creating a Data Culture', 'Selenium Framework Design in Data-Driven Testing', 'The Big Data Market', 'Oil, Gas, and Data', 'Going Pro in Data Science', 'Data and Social Good', 'Understanding the Chief Data Officer', 'Mapping Big Data', 'The Security Data Lake', 'Not All Data Is Created Equal', '2015 Data Science Salary Survey', 'Managing the Data Lake', 'Python for Data Developers']\n"
     ]
    }
   ],
   "source": [
    "titles = list(article.find(\"div\",\"book_text\").a.text.strip() for article in articles)\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author(s) are in the text of the \"note\" `<p>`, + are prefaced by a `By` + separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['By Scott Murray', 'By Federico Castanedo', 'By Hilary Mason, DJ Patil', 'By Carl Cocchiaro', 'By Aman Naimat', 'By Daniel Cowles', 'By Jerry Overton', 'By Mike Barlow', 'By Julie Steele', 'By Russell Jurney', 'By Raffael Marty', 'By Mike Barlow, Gregory Fell', 'By John King, Roger Magoulas', \"Publisher: O'Reilly Media\", \"By O'Reilly Media, Inc.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['By Scott Murray',\n",
       " 'By Federico Castanedo',\n",
       " 'By Hilary Mason, DJ Patil',\n",
       " 'By Carl Cocchiaro',\n",
       " 'By Aman Naimat',\n",
       " 'By Daniel Cowles',\n",
       " 'By Jerry Overton',\n",
       " 'By Mike Barlow',\n",
       " 'By Julie Steele',\n",
       " 'By Russell Jurney',\n",
       " 'By Raffael Marty',\n",
       " 'By Mike Barlow, Gregory Fell',\n",
       " 'By John King, Roger Magoulas',\n",
       " \"Publisher: O'Reilly Media\",\n",
       " \"By O'Reilly Media, Inc.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # RegEx\n",
    "print(list(article.find(\"div\",\"book_text\").find(\"p\",\"note\").text\n",
    "           for article in articles))\n",
    "\n",
    "author_names = [article.find(\"p\",\"note\").text\n",
    "                for article in articles]\n",
    "(author_names)\n",
    "# remove leading 'By' and split on commas\n",
    "#authors = [x.strip() for x in re.sub(\"^By \", \"\", author_names).split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scott Murray',\n",
       " 'Federico Castanedo',\n",
       " 'Hilary Mason, DJ Patil',\n",
       " 'Carl Cocchiaro',\n",
       " 'Aman Naimat',\n",
       " 'Daniel Cowles',\n",
       " 'Jerry Overton',\n",
       " 'Mike Barlow',\n",
       " 'Julie Steele',\n",
       " 'Russell Jurney',\n",
       " 'Raffael Marty',\n",
       " 'Mike Barlow, Gregory Fell',\n",
       " 'John King, Roger Magoulas',\n",
       " \"Publisher: O'Reilly Media\",\n",
       " \"O'Reilly Media, Inc.\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(x).strip() for x in [re.sub('By *', '', x) for x in author_names]]#.split(\", \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ISBN seems to be contained in the link that’s in the thumbheader `<div>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-eca20bd57db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0misbn_link\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"thumbheader\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m# re.match captures the part of the regex in parentheses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0misbn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/product/(.*)\\.do\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misbn_link\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "# re.match captures the part of the regex in parentheses\n",
    "isbn = re.match(\"/product/(.*)\\.do\", isbn_link).group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the date is just the contents of the `<span class=\"directorydate\">:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-58dce978340f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"directorydate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "date = td.find(\"span\", \"directorydate\").text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s put this all together into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def book_info(td):\n",
    "    \"\"\"given a BeautifulSoup <td> Tag representing a book,\n",
    "    extract the book's details and return a dict\"\"\"\n",
    "    \n",
    "    title = td.find(\"div\", \"thumbheader\").a.text\n",
    "    by_author = td.find('div', 'AuthorName').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", by_author).split(\",\")]\n",
    "    isbn_link = td.find(\"div\", \"thumbheader\").a.get(\"href\")\n",
    "    isbn = re.match(\"/product/(.*)\\.do\", isbn_link).groups()[0]\n",
    "    date = td.find(\"span\", \"directorydate\").text.strip()\n",
    "    \n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"isbn\" : isbn,\n",
    "        \"date\" : date\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "souping page 1 , 0\n",
      "souping page 2 , 0\n",
      "souping page 3 , 0\n",
      "souping page 4 , 0\n",
      "souping page 5 , 0\n",
      "souping page 6 , 0\n",
      "souping page 7 , 0\n",
      "souping page 8 , 0\n",
      "souping page 9 , 0\n",
      "souping page 10 , 0\n",
      "souping page 11 , 0\n",
      "souping page 12 , 0\n",
      "souping page 13 , 0\n",
      "souping page 14 , 0\n",
      "souping page 15 , 0\n",
      "souping page 16 , 0\n",
      "souping page 17 , 0\n",
      "souping page 18 , 0\n",
      "souping page 19 , 0\n",
      "souping page 20 , 0\n"
     ]
    }
   ],
   "source": [
    "### SCRAPE\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "base_url = \"http://shop.oreilly.com/category/browse-subjects/\" + \\\n",
    "    \"data.do?sortby=publicationDate&page=\"\n",
    "books = []\n",
    "NUM_PAGES = 31 # at the time of writing, probably more by now\n",
    "\n",
    "for page_num in range(1, NUM_PAGES + 1):\n",
    "    print(\"souping page\", page_num, \",\", len(books)), \" found so far\"\n",
    "    url = base_url + str(page_num)\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "\n",
    "    for td in soup('td', 'thumbtext'):\n",
    "        if not is_video(td):\n",
    "            books.append(book_info(td))\n",
    "    # now be a good citizen and respect the robots.txt!\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Extracting data from HTML = more data art than data science.\n",
    "\n",
    "Now that we’ve collected the data, we can plot the number of books published each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'November 2014' so we need to\n",
    "    split on the space and then take the second piece\"\"\"\n",
    "    return int(book[\"date\"].split()[1])\n",
    "\n",
    "# 2014 is the last complete year of data (when I ran this)\n",
    "year_counts = Counter(get_year(book) for book in books\n",
    "                      if get_year(book) <= 2014)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "years = sorted(year_counts)\n",
    "book_counts = [year_counts[year] for year in years]\n",
    "plt.plot(years, book_counts)\n",
    "plt.ylabel(\"# of data books\")\n",
    "plt.title(\"Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the would-be investor looks at the graph and decides that 2013 was “peak\n",
    "data.\n",
    "\n",
    "### Using APIs\n",
    "Many websites + web services provide application programming interfaces (APIs), which allow you to explicitly request data in a structured format == saves you the trouble of having to scrape them\n",
    "\n",
    "### JSON (and XML)\n",
    "\n",
    "B/c HTTP = a protocol for transferring text, the data you request through a web API needs to be **serialized** into a string format. Often this serialization uses **JavaScript Object Notation (JSON)**. JavaScript objects look quite similar to Python `dicts`, which makes their string representations easy to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{ \"title\" : \"Data Science Book\",\n",
    "\"author\" : \"Joel Grus\",\n",
    "\"publicationYear\" : 2014,\n",
    "\"topics\" : [ \"data\", \"science\", \"data science\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can parse JSON using `json` module, in particular, `loads()` = deserializes a string representing a JSON object into a Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "serialized = \"\"\"{ \"title\" : \"Data Science Book\",\n",
    "                \"author\" : \"Joel Grus\",\n",
    "                \"publicationYear\" : 2014,\n",
    "                \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n",
    "\n",
    "# parse JSON to create Python dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
