{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9. Getting Data\n",
    "\n",
    "### stdin, stdout\n",
    " \n",
    "If running Python scripts at the command line, you can **pipe** (`|`) data through them using `sys.stdin` and `sys.stdout`. \n",
    "Ex: Scripts that reads in lines of text + spits back out the ones that match a RegEx and counts the lines it recieves + writes out that count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# egrep.py\n",
    "import sys, re\n",
    "\n",
    "# sys.argv = list of CLI args\n",
    "# sys.argv[0] = name of the program itself\n",
    "# sys.argv[1] = RegEx specified at the CL\n",
    "regex = sys.argv[1]\n",
    "\n",
    "# for every line passed into the script\n",
    "for line in sys.stdin:\n",
    "    # if matches RegEx, write to stdout\n",
    "    if re.search(regex,line):\n",
    "        sys.stdout.write(line)\n",
    "        \n",
    "# line_count.py\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    count += 1\n",
    "# print goes to stdout\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use these to count how many lines in a file contain a #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# windows\n",
    "#!type someFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n",
    "\n",
    "# linunx\n",
    "#!cat someFile.txt | python egrep.py \"[0-9]\" | python line_count.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: most_common_words.py num_words\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEWNSS\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# script that counts words in its input + writes out most common ones:\n",
    "# most_common_words.py\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# pass in number of words as 1st arg\n",
    "try:\n",
    "    num_words = int(sys.argv[1])\n",
    "except:\n",
    "    print(\"usage: most_common_words.py num_words\")\n",
    "    sys.exit(1) # non-zero exit code = indicates error\n",
    "\n",
    "counter = Counter(word.lower()\n",
    "                 for line in sys.stdin\n",
    "                 for word in line.strip().split() # split on space \n",
    "                 if word) # skip empty words\n",
    "\n",
    "for word,count in counter.most_common(num_words):\n",
    "    sys.stdout.write(str(count))\n",
    "    sys.stdout.write(\"\\t\")\n",
    "    sys.stdout.write(word)\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "### Ex of above script\n",
    "## type the_bible.txt | python most_common_words.py 10\n",
    "# 64193 the\n",
    "# 51380 and\n",
    "# 34753 of\n",
    "# 13643 to\n",
    "# 12799 that\n",
    "# 12560 in\n",
    "# 10263 he\n",
    "# 9840 shall\n",
    "# 8987 unto\n",
    "# 8836 for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files\n",
    "\n",
    "Can also explicitly read from + write to files directly in Python code.\n",
    "\n",
    "### Basics of Text Files\n",
    "\n",
    "1st step to working with a text file = **obtain a *file object* via `open`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'r' = read only\n",
    "file_for_reading = open(\"reading_file.txt\", \"r\")\n",
    "\n",
    "# 'w' = write = ***DESTROYS files if it already exits****\n",
    "file_for_writing = open(\"writing_file.txt\", \"w\")\n",
    "\n",
    "# 'a' = append (to end of file)\n",
    "file_for_appending = open(\"appending_file.txt\", \"a\")\n",
    "\n",
    "# close files when done\n",
    "file_for_writing.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to forget to close files, so always use them in a **`with` block**, at the end of which they will be closed automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filename,'r') as f:\n",
    "    data=function_to_get_data_from(f)\n",
    "    \n",
    "# at this point, file has been closed, don't try to use it\n",
    "process(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read over whole text file, iterate over lines with `for`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts_with_hash = 0\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    for line in file:\n",
    "        if re.match(\"^#\",line): # check if each line starts with # and count if True\n",
    "            starts_with_hash += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line you get this way ends in a **newline** character, so you’ll often want to `strip()` it before doing anything with it.\n",
    "\n",
    "Ex: You have a file full of email addresses, 1 per line, that you need to generate a histogram of the domains. The rules for correctly extracting domains are somewhat subtle (e.g., the Public Suffix List), but a good 1st approximation = just take the parts of the email addresses that come after the @ (Which gives the wrong answer\n",
    "for email addresses like \"@mail.datasciencester.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'email_address.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a3ebd8b932cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0memail_address\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"@\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"email_address.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     domain_counts - Counter(get_domain(line.strip())\n\u001b[1;32m      7\u001b[0m                            \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'email_address.txt'"
     ]
    }
   ],
   "source": [
    "def get_domain(email_address):\n",
    "    \"\"\"Split on '@' and return the last piece\"\"\"\n",
    "    return email_address.lower().split(\"@\")[-1]\n",
    "\n",
    "with open(\"email_address.txt\", \"r\") as f:\n",
    "    domain_counts - Counter(get_domain(line.strip())\n",
    "                           for line in f\n",
    "                           if \"@\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delimited Files\n",
    "\n",
    "More often we work w/ files w/ lots of data on each line that're very often either comma or tab-separated. Each line has several fields, w/ a comma/tab indicating where 1 field ends + the next starts.\n",
    "\n",
    "This gets complicated when you have fields w/ commas + tabs + newlines in them. For this reason, it’s pretty much always a mistake to try to parse them yourself. Instead, use Python’s `csv` module (or `pandas` library).\n",
    "\n",
    "For technical reasons, always work w/ CSV files in **binary mode** by including a `b` after the `r` or `w` (see Stack Overflow).\n",
    "\n",
    "If your file has no headers (which means you probably want each row as a list, + which places the burden on you to know what’s in each column), you can use `csv.reader` to iterate over rows, each of which will be an appropriately split list.\n",
    "\n",
    "Ex: TSV of stock prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n",
      "6/19/2014 AAPL 91.86\n",
      "6/19/2014 MSFT 41.51\n",
      "6/19/2014 FB 64.34\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process(date, symbol, price):\n",
    "    print(date, symbol, price)\n",
    "\n",
    "with open(\"tab_delimited_stock_prices.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    \n",
    "    for row in reader:\n",
    "        date = row[0]\n",
    "        symbol = row[1]\n",
    "        closing_price = float(row[2])\n",
    "        process(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the file has headers, we can skip them (with an inital call to `reader.next()`) or get each row as a `dict` (with headers as keys) by using `csv.DictReader`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/20/2014 AAPL 90.91\n",
      "6/20/2014 MSFT 41.68\n",
      "6/20/2014 FB 64.5\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process(date, symbol, price):\n",
    "    print(date, symbol, price)\n",
    "\n",
    "with open(\"colon_delimited_stock_prices.txt\", \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\":\")\n",
    "    \n",
    "    for row in reader:\n",
    "        date = row[\"date\"]\n",
    "        symbol = row[\"symbol\"]\n",
    "        closing_price = float(row[\"closing_price\"])\n",
    "        process(date,symbol,closing_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the file doesn’t have headers we can still use `DictReader` by passing it the keys as a `fieldnames` parameter, + we can similarly write out delimited data using `csv.writer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "todays_prices = {\"AAPL\":90.91,\"MSFT\":41.68,\"FB\":64.5}\n",
    "\n",
    "with open(\"comma_delimited_stock_prices.txt\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\",\")\n",
    "    \n",
    "    for stock,price in todays_prices.items():\n",
    "        writer.writerow([stock,price])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`csv.writer` does the right thing if fields themselves have commas in them. Your own hand-rolled writer probably won’t. For example, if you attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [[\"test1\", \"success\", \"Monday\"],\n",
    "           [\"test2\", \"success, kind of\", \"Tuesday\"],\n",
    "           [\"test3\", \"failure, kind of\", \"Wednesday\"],\n",
    "           [\"test4\", \"failure, utter\", \"Thursday\"]]\n",
    "\n",
    "# BAD - DON'T DO\n",
    "with open(\"bad_csv.txt\",\"wb\") as f:\n",
    "    for row in results:\n",
    "        f.write(\",\".join(map(str,tow))) # might have too many commas in it\n",
    "        f.write(\"\\n\") # row might already have newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will end up with a csv file no one will ever be able to make sense of that looks like:\n",
    "\n",
    "* test1,success,Monday\n",
    "* test2,success, kind of,Tuesday\n",
    "* test3,failure, kind of,Wednesday\n",
    "* test4,failure, utter,Thursday\n",
    "\n",
    "\n",
    "### Scraping the Web\n",
    "\n",
    "Fetching web pages = easym, getting meaningful structured info out of them = less so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
