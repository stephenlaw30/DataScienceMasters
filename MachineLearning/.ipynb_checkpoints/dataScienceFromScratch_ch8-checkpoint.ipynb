{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. Gradient Descent\n",
    "\n",
    "Frequently when doing data science, we’ll be trying to the find the best model for a certain situation. And usually “best” means **“minimizes the error of the model”** or **“maximizes the likelihood of the data.”** In other words, it will represent the solution to some sort of optimization problem.\n",
    "\n",
    "This means we’ll need to solve a number of optimization problems. In particular,\n",
    "we’ll need to solve them from scratch, + our approach = **gradient descent**, which lends itself pretty well to a from-scratch treatment.\n",
    "\n",
    "### The Idea Behind Gradient Descent\n",
    "\n",
    "Suppose we have some function `f` that takes as input a vector of real numbers and outputs a single real number. One simple such function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(vector):\n",
    "    \"\"\"Compute sum of squared elements in given vector\"\"\"\n",
    "    return sum(element**2 for element in vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll frequently need to maximize (or minimize) such functions, i.e. find the input `vector` that produces the largest (or smallest) possible value. For functions like ours, the **gradient** (vector of partial derivatives) gives the input a direction in which the function most quickly increases.\n",
    "\n",
    "Accordingly, one approach to maximizing a function is to pick a random starting point,\n",
    "compute the gradient, take a small step in the direction of the gradient (direction that causes the function to increase the most), and repeat with the new starting point.\n",
    "\n",
    "Similarly, you can try to minimize a function by taking small steps in the opposite\n",
    "direction (negative gradient)\n",
    "\n",
    "* ***NOTE***: If a function has a unique **global minimum**, this procedure is likely to find it. If a function has **local (i.e. multiple) minima**, this procedure might “find” the wrong one of *them*, in which case you might re-run the procedure from a variety of starting points. **If a function has no minimum, then it’s possible the procedure might go on forever.**\n",
    "\n",
    "## Estimating the Gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
