---
title: "01b_USDA"
output: html_document
---
# UNIT 2 - LINEAR REGRESSION

## STATISTICAL SOMMELIER - INTRO TO LINEAR REGRESSION

### PREDICTING QUALITY OF WINE

Bordeaux is a region in France that is popular for producing wine, and while it has been produced in much the same way for hundreds of years, there are differences in price and in quality from year to year that are sometimes very significant.

Bordeaux wines are widely believed to taste better when older, so there's an incentive to store young wines until they are mature. The main problem here is that it's hard to determine quality of wine when it is so young just by tasting it, since tastes of the wine will change very significantly by the time it will actually be consumed. This is why wine tasters and experts are helpful, who taste the wines and then predict which ones will be the best wines later.

### Can analytics model this process better + make stronger predictions?

On March 4, 1990, the NYT announced that Princeton economics professor Orley Ashenfelter could predict the quality of Bordeaux wine without tasting a single drop via the results of a mathematical model. Ashenfelter used linear regression to predict an outcome using a set of what predictors. For the outcome (dependent variable), Ashenfelter chose to use a typical `price` in 1990-1991 for Bordeaux wine in an auction, as price approximates quality. For predictors (independent variables), he used wine `age` (older wines are more expensive) and some weather-related info, specifically the average growing season temperature, the harvest rain measurements, and the winter rain measurements. 

Ashenfelter believed that his predictions were more accurate than those of the world's most influential wine critic, Robert Parker, who had called Ashenfelter "an absolute total sham," and added that Ashenfelter was "rather like a movie critic who never goes to see the movie but tells you how good it is based on the actors and the director."

In Ashenfelter's model, more `harvest rain` was associated wtih a lower logarithmic (realization) `price`, + higher temperatures were associated with a higher logarithmic (realization) price.

# ONE VARIABLE LINEAR REGRESSION

The goal of linear regression is to create a best-fit predictive line through the data plotted: `y(i) = B0 + B1X(i) + epsilon(i)`. A baseline model would be just to predict the average value of the outcome variables in the training set for each data point's outcome, regardless of their predictor values. Our goal is to beat this baseline model.

So, we take the intercept coefficient/beta value, `B0`, and add to it the regression predictor coefficient, `B1` (or slope of line) multiplied by the `i`th observation of `X` (a predictor), as well as the **standard error** for the `i`th obs (the true y actual - predicted y value) to get the predicted outcome value for i. We add `epsilon(i)` because our slope, `B1`, must the same for all data points, so our prediction may be a bit off from the actual value of y(i), so we add it back in the **residual** (the error term), Our epsilon value for an observation would be 0 if the point was lying exactly on the regression line (which *rarely* happens, and the overwhelming majority of models make errors). The *best* model, or best choice of coefficients B0 and B1 would be the one that gives the smallest error terms/smallests residuals.

To compute residuals, just take our y^ (predicted outcome value) and subtract it from the true y value, and repeat this for each data point. We can square these residual values and sum them up to get the **sum of squared errors (SSE)**, one measure of quality of a sregression line. E(1)^2 + E(2)^2 + ... + E(n)^2. We want a *smaller* value of SSE, as the smallest possible value indicates that we have the best regression line and therefore the best model.

Although SSE allows us to compare lines on the same data set, it's hard to interpret for 2 reasons.
<ol>
<li> It scales wtih `n`, the number of data points. If we built the same model with twice as much data, the SSE might be twice as big, but this doesn't necessarily mean it's a worse model. </li>
<li> The units are hard to understand, as SSE is in squared units of the outcome. </li>

To mitigate these issues, the **Root Means Squared Error (RMSE)** is often instead used as a measure of quality of the regression line. It take the square root of the SSE and divides by `n` to **normalize** the value, and the result is in the same units as the outcome.

**R2** is another common measure of quality of the regression line. It compares the "best model" to a baseline model (our model with no regard to predictors and predicts the average outcome value for each data point). The SSE for this baseline model is also known as **Total Sum of Squares (SST)**. We use to calculate R2 via `1 - (SSE/SST)`.

R2 is usually a good quality measure because it capture the *value added by using a specific model* and its value indicateds how much the model explains the variability of the outcome values around the mean outcome values (i.e. R2 = .66 means 66% of the variability is explained). An R2 value of 0 indicates that no improvement was made over the baseline model, and an R2 value of 1 indicates a perfect predictive model with 100% of variability explained. For the one-variable linear regression, this means the 1 predictor explains all the variability of the predicted outcome values around the mean outcome value. R2 is also good because it's unitless and is universally interpretable.

We typically want a larger R2 (which is not always the case, as in multiple linear regression). SSE and SST will always be greater than or equal to 0, as they're squared values. Also, SSE will always be less than our equal to the SST, because our model could just set `B1 = 0`, and then we'd get the baseline model. So, any linear regression model that does not just predict the average outcome value could not do worse than than the baseline model. In the worst case scenario, we get an SSE that is equal to the SST, and therefore R2 = 0, which indicates no improvemnt over baseline. In best case scenario, we have no residuals (errors) so SSE= 0 and therefore R2 = 1. However, R2 can still be hard to compare between models, as good models for *easy* problems will have an R2 = ~1, but *good* models for *hard* problems could end up with an R2 = ~0.

### MULTIPLE-VARIABLE LINEAR REGRESSION 

If we use each variable in its own, single-variable linear regression model, we get the following R2 values:
<ul>
<li> `AGST` = 0.44 (best) </li>
<li> `HarvRain` = 0.32 </li>
<li> `FrancePop` = 0.22 </li>
<li> `Age` = 0.20 </li>
<li> `WintRain` = 0.02 (worst) </li>
</ul>

With multiple linear regression, we can use all/some of these variables in a model to improve its predictive ability. The equation would be `y(i) = B0 + B1X1(i) + B2X2(i) + ... BkXk(i) + E(i)`, which is almost the same equation as a single-variable linear regression, but this formula has a coefficient/Beta for each predictor. For example, `B2X2(i)` is the regression coefficient `B` for the 2nd predictor multiplied by the 2nd predictor's value for the `i`th observation.

To find the best model, we do the same thing as before, where we minimize the SSE by minimizing our using Epsilon values's (error terms)

We can start wtih a linear regression model that uses the single predictor that gave us the best R2, which was `AGST`, and then we can add more predictors in consecutive iterations and compare the R2 values of each iteration:
<ol> 
<li> `AGST` = 0.44
<li> `AGST + HR` = 0.71
<li> `AGST + HR + Age` = 0.79
<li> `AGST + HR + Age + WR` = 0.83
<li> `AGST + HR + Age + WR + Pop` = 0.83 </li>
</ul>

So, in this case, as we add predictors to the model, our R2 improves, but there are diminishing returns for each predictor added. But, it should be noted that not all predictors should be used, because the addition of each new predictor means we have more data in the model, and therefore we have a more complex/flexible. And, unfortunately, more flexible models means that they can fit to training data too well and perform poorly on new/out-of-sample observations, which is **overfitting** the data with the model. This can be occur when one is mislead by a high R2 value that occurs from making a model that fits *too* well to the data used to train the model, as well as the random noise of the training data. This model would have bad performance on unseen data (i.e. prediction of year 2013), since R2 can never decrease as we add more variables. (ELABORATE)

```{r}
wine <- read.csv("../data/wine.csv")
str(wine)
summary(wine)
```
Now, let's create a one-variable linear regression model using `AGST`.
```{r}
agst_model <- lm(Price ~ AGST, data = wine)
summary(agst_model)
```

Wee that for each increase of 0.6351 in units of `AGST`, the wine's `Price` will increase by 1 `Price` unit. We also see that `AGST` is also very significant (three \*'s), with an *okay* R2 and Adjusted R2 values. The adjusted R2 has a penalty on the R2 value for the number of predictors used to create the model in order to create system of checks and balances so that one cannot just keep adding predictors to raise R2. If an added predictor does *not* improve the model, there is a penalty assessed which will decrease the adjusted R2 value. Standard error, `Std. Err`, is a  measure of how likely it is that the associated coefficient will vary from the estimated value. The `t value` is the coefficient values divided by the standard error value. The larger the absolute value of the t value, the more likely the coefficient is to be significant. Therefore, we want larger values in this column. The final summary column `Pr(>|t)` is our **p-value**, a measure of how plausible it is that the coefficient is actually 0, given the data used to build the model (in other words, the probability that the null hypotheses of the coefficient value being 0 is true, given the data).

Let's look at the the stored residuals of this model.
```{r}
agst_model$residuals
```

We compute `SEE` by taking the sum of each of these values squared.
```{r}
(sse <- sum(agst_model$residuals**2))
```

We want to minimize this value.

Let's add `HarvestRain` to the model.
```{r}
agst_hrain_model <- lm(Price ~ AGST + HarvestRain, data = wine)
summary(agst_hrain_model)
```

We see that `AGST`'s coefficient has increased from the previous model, and that `HarvestRain`'s is slightly negative, also noting that both predictors are very significant. We can also see that both R2 and Adjusted R2 have increased as well. 
<ul>
<li> For each .60262 increase in `AGST` units, `Price` will increase by 1 `Price` unit </li>
<li> For each -0.00457 decrease in `HarvestRain` units, `Price` will increase by 1 `Price` unit  </li>
</ul>

Now check the new SSE
```{r}
(sse2 <- sum(agst_hrain_model$residuals**2))
```

Now our `SSE` is 2.97, which is a smaller value than the previous `SEE` = 5.73, which is an improvement since we want to minimize this.

Now let's add *all* predictors to the model.
```{r}
fullModel <- lm(Price ~ ., data = wine)
summary(fullModel)
```

Here, we see that only `AGST` and `HarvestRain` are significant, while all predictor coefficient values are quite small. The R2 and Adjusted R2 values have increased compared to the second model, but we must note that most predictors aren't significant. Remember, a coefficient value that is basically 0 means that the respective predictor value does *not* affect the outcome variable value. We want to remove predictors that are not significantly different than 0/have no effect on the outcome variable and are therefore pointless to have in the model.

First, let's check the full model's `SSE`.
```{r}
(sse3 <- sum(fullModel$residuals**2))
```

The `SEE` is now at its lowest (i.e. best) value yet.

Let's remove the insignificant predictor `FrancePop` (which we wouldn't really expect to impact wine price/quality anyway).
```{r}
model4 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, wine)
summary(model4)
```

We can see that all predictors are now significant, with `AGST` and `HarvRain` being the "most" significant. All of our predictor coefficient values are very small, except `AGST`, which implies it has a pretty large impact on the value of `Price`, relative to the other predictors. We should also note that before, `Age` was not significant, but now is so after removing FrancePop. Finally, we see that R2 has decreased compared to the full model, while the Adjusted R2 has actually *increased*, so we can be more confident that this model is stronger than the full model.

## CORRELATION AND MULTICOLLINEARITY

Since `Age` was not significant, when `FrancePop` was included in the model but became significant after removing `FrancePop`, it suggests that `Age` and `FrancePop` were **correlated**.
```{r}
plot(wine$Age,wine$FrancePop)
cor(wine$Age,wine$FrancePop)
```
Indeed, there is a strong negative correlation between these 2 predictors. This makes sense because France's population has increased with time. So, the older a wine is, the earlier in France's history it was bottled, and therefore the smaller the value of `FrancePop`.

Let's look at the correlation of other predictors
```{r}
cor(wine)
```

For example, we see that `WinterRain` and `Price` have an ever-so-slightly positive correlation. But, our very strong negative correlation between `Age` and `FrancePop` shows us that the 2 variables are highly correlated, which has caused a multicollinearity problem in the model. This occurs in a situation when two *predictors* are highly correlated (the outcome variable has nothing to do w/ it). A high correlation between a predictor and the outcome is typically good because we're trying to predict which predictors do, in fact, affect the outcome value.

Due to the possibility of multicollinearity occuring, we remove 1 predictor at a time in a process called **backwards elimination**. What happens if we remove `Age` from our most recent model?
```{r}
model5 <- lm(Price ~ AGST + HarvestRain + WinterRain, wine)
summary(model4)
summary(model5)
```
We get *worse* R2 and Adjusted R2 values. We expect `Age` to be a significant predictor for the price of wine, since older wines are typically more expensive, while population measures wouldn't really have an effect on wine price.

Multicollinearity reminds us that *coefficient's are only interpretable in the presence of other variables being used*. High correlations can even cause coefficients to have an unintuitive sign. There is no definitive cut-off value for what makes a correlation "too high"", but typically, a correlation coefficient value above 0.7 or below  -0.7 is cause for concern.

### MAKING PREDICTIONS
Our final wine model had `R2 = 0.83`, which gives us a measure of how accurate our model is *on the data used to construct the model*, the **training data**. So, our model does a good job predicting the data it's already seen. But we also want a model that does well on *new* data that it's never seen before so that we can use the model to make predictions for wine prices in later years. For this particular application, Bordeaux wine buyers profit from being able to predict quality of a wine years before it actually matures. They know the values of the predictor, but don't know the price the wine will eventually sell for. So, it's important to build a model that does well at predicting data it's never seen before, the **test data**. We then hope to get a good **out-of-sample accuracy** on this data.

Let's see how well our model performs on some test data in R.
```{r}
wine_test <- read.csv("../data/wine_test.csv")
str(wine_test) 
```
So, we've got 2 wines from 1979 and 1980 with prices of \$6.95 and \$6.50, respectively. Now we will use our model on the new data to predict their prices and compare to the actual `Price` values.
```{r}
(wine_predictions <- predict(model4, wine_test))
```

For our 1st wine (the 1979), we predicted `Price = $6.77`, and for our 2nd wine (the 1980), we predicted `Price = $6.69`. Wine 1's predicted `Price` is below the actual value by \$0.18, and Wine 2's is above the actual value by `$0.19`. It looks like our predictions are pretty good, and we can quantify this by computing the R2 for the test set by hand.
<ul>
<li> R2 = 1 - (SSE / SST) </li>
<li> SSE = sum of (actual - predicted)^2 </li>
<li> SST = sum of (actual - baseline average)^2 </li>
</ul>
```{r}
sse_model4 = sum((wine_test$Price - wine_predictions)**2)
sst_model4 = sum((wine_test$Price - mean(wine$Price))**2)
(R2_test_model4 = 1 - (sse_model4/sst_model4))
```

Our test set `R2 = 0.7944278`, which is pretty good, but we must note our test set is useless because its so small. We need to increase test set size to be more confident about the out-of-sample accuracy of our model. As we add more predictors, our test set's R2 will not always increase (it can even be negative because while our model can never do worse than baseline on the *training* data, it *can* do worse than baseline on the *test* data). This is because since SSE and SST are the sums of squared terms, we know that both of them will be positive. Thus (SSE/SST) must always greater than or equal to 0. This means it is not possible to have an out-of-sample R2 value of 2.4 b/c 1 - positive  =  > 1
 - However, all other values are valid (even negative ones!), since SSE can be more or less than SST, due to 
     the fact that this is an out-of-sample R?, not a model R?.

Model 4 does best on both the training set model's R2 and the test set's R2, but we need more data points in 
 our test set to reach any real, confident conclusion


Wine expert Robert Parker predicted that the 1986 Bordeaux wine is VERY GOOD to SOMETIMES EXCEPTIONAL. On the 
 other hand, Ashenfelter said the 1986 Bordeaux wine is MEDIOCRE + made the prediction that the 1989 Bordeaux 
 would be the wine of the century + the 1990 Bordeaux would be even better.
In wine options, the 1989 Bordeaux sold for more than 2x the price of 1986 + the 1990 Bordeaux sold for even 
 higher prices.
Later, Ashenfelter predicted that the 2000 + 2003 Bordeaux wines would be GREAT + Robert Parker stated the 
 2000 was the greatest vintage Bordeaux has ever produced, in agreement with Ashenfelter.

So what is the analytics edge in this case?
 - What we have developed is a linear regression model, a simple but rather powerful model for predicting 
     quality of wines.
 - It only used few variables + we have seen that it predicted wine prices quite well, + in fact, in many 
     cases, outperformed wine expert's opinions.
What is impressive, in this 1st introductory lecture to linear regression, is that an analytics approach that
 uses data to build a model that improves decision making is effective in a traditionally QUALITATIVE problem.
When grapes are harvested, we'd have the temperature data, and then the age data when the wine is bottled.
 - We can plug these data points into our best model to predict what price (+ quality) the wine will have.