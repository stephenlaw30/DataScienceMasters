---
title: "01b_USDA"
output: html_document
---
# UNIT 2 - LINEAR REGRESSION

## STATISTICAL SOMMELIER - INTRO TO LINEAR REGRESSION

### PREDICTING QUALITY OF WINE

Bordeaux is a region in France that is popular for producing wine, and while it has been produced in much the same way for hundreds of years, there are differences in price and in quality from year to year that are sometimes very significant.

Bordeaux wines are widely believed to taste better when older, so there's an incentive to store young wines until they are mature. The main problem here is that it's hard to determine quality of wine when it is so young just by tasting it, since tastes of the wine will change very significantly by the time it will actually be consumed. This is why wine tasters and experts are helpful, who taste the wines and then predict which ones will be the best wines later.

### Can analytics model this process better + make stronger predictions?

On March 4, 1990, the NYT announced that Princeton economics professor Orley Ashenfelter could predict the quality of Bordeaux wine without tasting a single drop via the results of a mathematical model. Ashenfelter used linear regression to predict an outcome using a set of what predictors. For the outcome (dependent variable), Ashenfelter chose to use a typical `price` in 1990-1991 for Bordeaux wine in an auction, as price approximates quality. For predictors (independent variables), he used wine `age` (older wines are more expensive) and some weather-related info, specifically the average growing season temperature, the harvest rain measurements, and the winter rain measurements. 

Ashenfelter believed that his predictions were more accurate than those of the world's most influential wine critic, Robert Parker, who had called Ashenfelter "an absolute total sham," and added that Ashenfelter was "rather like a movie critic who never goes to see the movie but tells you how good it is based on the actors and the director."

In Ashenfelter's model, more `harvest rain` was associated wtih a lower logarithmic (realization) `price`, + higher temperatures were associated with a higher logarithmic (realization) price.

# ONE VARIABLE LINEAR REGRESSION

The goal of linear regression is to create a best-fit predictive line through the data plotted: `y(i) = B0 + B1X(i) + epsilon(i)`. A baseline model would be just to predict the average value of the outcome variables in the training set for each data point's outcome, regardless of their predictor values. Our goal is to beat this baseline model.

So, we take the intercept coefficient, `B0`, and add to it the regression predictor coefficient, `B1` (or slope of line) multiplied by the `i`th observation of `X` (a predictor), as well as the **standard error** for the `i`th obs (the true y actual - predicted y value) to get the predicted outcome value for i. We add `epsilon(i)` because our slope, `B1`, must the same for all data points, so our prediction may be a bit off from the actual value of y(i), so we add it back in the **residual** (the error term), Our epsilon value for an observation would be 0 if the point was lying exactly on the regression line (which *rarely* happens, and the overwhelming majority of models make errors). The *best* model, or best choice of coefficients B0 and B1 would be the one that gives the smallest error terms/smallests residuals.

To compute residuals, just take our y^ (predicted outcome value) and subtract it from the true y value, and repeat this for each data point. We can square these residual values and sum them up to get the **sum of squared errors (SSE)**, one measure of quality of a sregression line. E(1)^2 + E(2)^2 + ... + E(n)^2. We want a *smaller* value of SSE, as the smallest possible value indicates that we have the best regression line and therefore the best model.

Although SSE allows us to compare lines on the same data set, it's hard to interpret for 2 reasons.
<ol>
<li> It scales wtih `n`, the number of data points. If we built the same model with twice as much data, the SSE might be twice as big, but this doesn't necessarily mean it's a worse model. </li>
<li> The units are hard to understand, as SSE is in squared units of the outcome. </li>

To mitigate these issues, the **Root Means Squared Error (RMSE)** is often instead used as a measure of quality of the regression line. It take the square root of the SSE and divides by `n` to **normalize** the value, and the result is in the same units as the outcome.

**R2** is another common measure of quality of the regression line. It compares the "best model" to a baseline model (our model with no regard to predictors and predicts the average outcome value for each data point). The SSE for this baseline model is also known as **Total Sum of Squares (SST)**. We use to calculate R2 via `1 - (SSE/SST)`.

R2 is usually a good quality measure because it capture the *value added by using a specific model* and its value indicateds how much the model explains the variability of the outcome values around the mean outcome values (i.e. R2 = .66 means 66% of the variability is explained). An R2 value of 0 indicates that no improvement was made over the baseline model, and an R2 value of 1 indicates a perfect predictive model with 100% of variability explained. For the one-variable linear regression, this means the 1 predictor explains all the variability of the predicted outcome values around the mean outcome value. R2 is also good because it's unitless and is universally interpretable.

We typically want a larger R2 (which is not always the case, as in multiple linear regression). SSE and SST will always be greater than or equal to 0, as they're squared values. Also, SSE will always be less than our equal to the SST, because our model could just set `B1 = 0`, and then we'd get the baseline model. So, any linear regression model that does not just predict the average outcome value could not do worse than than the baseline model. In the worst case scenario, we get an SSE that is equal to the SST, and therefore R2 = 0, which indicates no improvemnt over baseline. In best case scenario, we have no residuals (errors) so SSE= 0 and therefore R2 = 1. However, R2 can still be hard to compare between models, as good models for *easy* problems will have an R2 = ~1, but *good* models for *hard* problems could end up with an R2 = ~0.

### MULTIPLE-VARIABLE LINEAR REGRESSION 
If we use each variable in its own single variable LR, we get:
   - AGST R2 = 0.44 (best)
   - HarvRain = 0.32
   - FrancePop = 0.22
   - Age = 0.20
   - WintRain 0.02 (worst)

W/ multiple LR, we can use all/some of these to improve predictive ability

 y(i) = B0 + B1X1(i) + B2X2(i) + ... BkXk(i) + E(i)
   - same equation as single LR, but but has a CoE/Beta for EACH IV
   - so B2X2(i) = The regression CoE B for the 2nd IV * the 2nd IV for the ith obs

best model is same as before --> minimize SSE using E's (error terms)

Can start w/ a LR model that uses the IV that gave the best R2, AGST, + add IV's for each model iteration 
   - VAR                       R2
   - AGST                      0.44
   - AGST, HR                  0.71
   - AGST, HR, Age             0.79
   - AGST, HR, Age, WR         0.83
   - AGST, HR, Age, WR, Pop    0.83

so as we add IV's to the model, R2 improves, but there are diminishing returns for each IV added

Not ALL var's should be used b/c each new var = more data = a MORE COMPLICATED MODEL
   - also risks OVERFITTING the model = misleading high R2 from data used to create the model w/ bad 
       performance on unseen data (i.e. prediction of year 2013)
         - it begins to model random noise
   - R2 CANNOT DECREASE W/ MORE VARIABLES

wine <- read.csv("wine.csv")
str(wine) we got 25 wines
summary(wine)

1-variable LR model using AGST via linear model lm(DV ~ IV, datasetToUse) function
model1 <- lm(wine$Price ~ wine$AGST, data = wine)
summary(model1)
see description of function, residuals/error terms, CoE estimates, error, t-value, p-value, R1, Adj. R2, etc.
See that for each 0.6351 increase in AGST in units of AGST, price will increase by 1 unit of price
AGST is also VERY significant, w/ an "okay" R2 and Adj. R2 (adjusted for  of IV's used to  of data points in
   the model to create system of checks and balances)
   - as R2 increases w/ each IV added, Adj. R2 adds a penalization factor if an added IV does NOT improve the
       model

Compute SSE for the model w/ out residuals that are stored in the vector --> model1$residuals
model1$residuals
see values for each residual. Compute SEE by taking the sum of each of these squares
sse <- sum(model1$residuals**2) 
SSE = 5.73 --> want to minimize this.

Add Harvest Rain to the model
model2 <- lm(Price ~ AGST + HarvestRain, data = wine)
summary(model2)
See that AGST's CoE has increased, and HarvestRain's is slightly negative, while both are V. SIG.
Both R2 and Adj. R2 have increased as well. 
   - For each .60262 increase in AGST in units of AGST, Price will increase by 1 in units of Price
   - For each -0.00457 decrease in HarvestRain in units of HarvestRain, Price will increase by 1.....
Check the SSE
sse2 <- sum(model2$residuals**2)
so it's 2.97, an improvement

Add ALL IV's
model3 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data = wine)
summary(model3)
See that only AGST and HarvestRain are SIG and they are V. SIG., while all CoE's are very small.
 - R2 and Adj. R2 have increased, but the variables aren't sig.
 - CoE of 0 = does NOT affect the DV --> REMOVE VARS THAT AREN'T SIGNIFICANTLY DIFF FROM 0
 - Std. Err = measure of how likely it is the CoE will vary from the estimate value
 - T-value = Estimate/SE
     - larger the ABSV of t-value, the MORE likely the CoE is SIG
 - Pr(>|t) = how plausible it is that the CoE is actually 0, given the data used to build the model

sse3 <- sum(model3$residuals**2)
SEE is now at its lowest/best yet

rainModel <- lm(Price ~ HarvestRain + WinterRain, data = wine)
summary(rainModel)

Remove all COMPLETELY INSIG var FrancePop (wouldn't expect to impace wine price (expected quality) anyway)
model4 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
summary(model4)
See that all vars are now SIG, while only AGST + HarvRain remain V. SIG.
All CoE's are very small, except AGST, which has a decent impact on Price here
 - R2 has decreased while Adj. R2 has INCREASED, so this model is stronger
So before, Age was NOT SIG, but is now SIG after removing FrancePop. Why?



CORRELATION AND MULTICOLLINEARITY**********************
Age and FrancePop were highly CORRELATED in a NEGATIVE MANNER
plot(wine$Age,wine$FrancePop)
Makes sense b/c France's Population has increased w/ time (decreased w/ how old the wine is)

Compute correlation of Winter Rain and Price
cor(wine$WinterRain,wine$Price)
It's 0.13665, so very slightly positive
cor(wine$Age,wine$FrancePop)
Corr = -.99, so Very highly negatively correlated
Get correlations of ALL IV's
cor(wine)

So Age and FrancePop were highly CORRELATED, and caused MULTICOLLINEARITY problems in the model
   - this refers to the situation when 2 INDEPENDANT var's are highly correlated (DV has nothing to do w/ it)
   - High corr between an IV + the DV is good b/c we're trying to predict which IV's DO IN FACT affect the DV

Due to possibilites of multicollinearity, ALWAYS REMOVE 1 IV AT A TIME (BACKWARDS ELIMINATION)
what is we removed BOTH Age + FrancePop b/c they were both INSIG + compare w/ model4
   model4 <- lm(Price ~ AGST + HarvestRain + WinterRain + Age, data = wine)
model5 <- lm(Price ~ AGST + HarvestRain + WinterRain, data = wine)
summary(model4)
summary(model5)
We get a worse R2 and Adj. R2

We removed FrancePop and not AGe b/c you'd expect wine to be significant for price of wine, since older wines
   are typically more expensive, + population wouldn't really have an effect on wine price

Multicollinearity reminds us that CoE's are only interpretable IN THE PRESENCE OF OTHER VARIABLES BEING USED.
 - High correlations can even cause coefficients to have an unintuitive sign.

Do we have any other highly-correlated IV's?
 There is no definitive cut-off value for what makes a correlation TOO high, but typically, correlation > 0.7
 or <  -0.7 is cause for concern.
If you look back at all of the correlations we computed for data set: 
cor(wine)
Doesn't look like we have any other highly-correlated IV's
So we'll stick w/ model, using AGST, HarvestRain, WinterRain, and Age as IV's



MAKING PREDICTIONS
Our wine model had R2 = 0.83, which tells us how accurate our model is on the data used to construct the model.
 So our model does a good job predicting the data it's seen.
But we also want a model that does well on NEW data/data it's never seen before so we can use the model to 
 make predictions for LATER years.

For this particular application, Bordeaux wine BUYERS PROFIT from being able to PREDICT QUALITY of a wine years
 BEFORE IT MATURES
They know the values of the IV's (age + weather), don't know the price the wine will eventually sell for.
 So it's important to build a model that does well at predicting data it's never seen before.
Data used to build a model is often called the TRAINING DATA + new data = the TEST DATA
 ACCURACY OF A MODEL ON TEST DATA = OUT-OF-SAMPLE ACCURACY

Let's see how well our model performs on some test data in R.
wine_test <- read.csv("wine_test.csv")
str(wine_test) got 2 wines from 1979 + 1980 w/ prices of $6.95 and $6.50
summary(wine_test)

use PREDICT() to use the model on the new data
wine_predictions <- predict(model4,newdata=wine_test)
For out 1st wine (1979), we predict Price = $6.77, and for our 2nd wine (1980), we predict Price = $6.69
 Wine1's predicted price is BELOW the actual by $0.18, and Wine2's is ABOVE by $0.19

It looks like our predictions are pretty good, and we can QUANTIFY THIS COMPUTING R2 FOR THE TEST SET
   - R2 = 1 - (SSE / SST)
   - SSE = sum of actual-predicted ^ 2
   - SST = sum of acutal-ACTUALSaverageFromTRAINING ^ 2
sse4 = sum((wine_test$Price - wine_predictions)**2)
sst4 = sum((wine_test$Price - mean(wine$Price))**2)
R2_predictions = 1 - (sse4/sst4)
So our R2 = 0.7944278, which is pretty good, but note our test set is very small.
 We need to increase test set size to be more confident about our OUT-OF-SAMPLE ACCURACY of our model

As we add more IV's our Test Set's R2 does not always increase (can even be negative b/c our model can never
 do worse than baseline on the TRAINING data, but CAN do worse than baseline on the TEST data)
 - Since SSE + SST are the sums of squared terms, we know that both will be positive. Thus SSE/SST must always 
     be >= 0. This means it is NOT possible to have an out-of-sample R? value of 2.4 b/c 1 - positive  =  > 1
 - However, all other values are valid (even negative ones!), since SSE can be more or less than SST, due to 
     the fact that this is an out-of-sample R?, not a model R?.

Model 4 does best on both the training set model's R2 and the test set's R2, but we need more data points in 
 our test set to reach any real, confident conclusion


Wine expert Robert Parker predicted that the 1986 Bordeaux wine is VERY GOOD to SOMETIMES EXCEPTIONAL. On the 
 other hand, Ashenfelter said the 1986 Bordeaux wine is MEDIOCRE + made the prediction that the 1989 Bordeaux 
 would be the wine of the century + the 1990 Bordeaux would be even better.
In wine options, the 1989 Bordeaux sold for more than 2x the price of 1986 + the 1990 Bordeaux sold for even 
 higher prices.
Later, Ashenfelter predicted that the 2000 + 2003 Bordeaux wines would be GREAT + Robert Parker stated the 
 2000 was the greatest vintage Bordeaux has ever produced, in agreement with Ashenfelter.

So what is the analytics edge in this case?
 - What we have developed is a linear regression model, a simple but rather powerful model for predicting 
     quality of wines.
 - It only used few variables + we have seen that it predicted wine prices quite well, + in fact, in many 
     cases, outperformed wine expert's opinions.
What is impressive, in this 1st introductory lecture to linear regression, is that an analytics approach that
 uses data to build a model that improves decision making is effective in a traditionally QUALITATIVE problem.
When grapes are harvested, we'd have the temperature data, and then the age data when the wine is bottled.
 - We can plug these data points into our best model to predict what price (+ quality) the wine will have.