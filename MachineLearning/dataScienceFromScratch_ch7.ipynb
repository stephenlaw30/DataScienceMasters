{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch7 - Hypothesis and Inference\n",
    "\n",
    "The *science* part of data science frequently involves forming and testing hypotheses about data and the processes that generate it.\n",
    "\n",
    "## Statistical Hypothesis Testing\n",
    "\n",
    "Often we want to test whether a certain hypothesis (assertions like “this coin is fair” or “data scientists prefer Python to R” that can be translated into statistics about data) is likely to be true. \n",
    "\n",
    "*Under various assumptions*, those statistics can be thought of as **observations of random variables from known distributions**, which allows us to make statements about how likely those assumptions are to hold.\n",
    "\n",
    "In the classical setup, we have a null hypothesis `H0` that represents some default position, and some alternative hypothesis `H1` that we’d like to compare it with, and we use statistics to decide whether we can reject `H0` as false or not, or fail to reject `H0`.\n",
    "\n",
    "#### Example: Flipping a Coin\n",
    "Imagine we have a coin to test if it’s fair. Make the assumption\n",
    "that the coin has some probability p of landing heads, so our `H0` = the\n",
    "coin is fair — that is, that `p = .5` . We’ll test this against the alternative hypothesis, `p != .5`\n",
    "\n",
    "In particular, our test will involve flipping the coin some number `n` times and counting the number of heads `X`. *Each coin flip is a Bernoulli trial*, which means that `X` = a *Binomial(n,p) random variable*, which we can approximate using the Gaussian distribution:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000.0, 50.0)\n",
      "(8000.0, 39.99999999999999)\n",
      "(2000.0, 40.0)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def normal_approx_to_binomial(n,p):\n",
    "    \"\"\"Find mu and sigma corresponding to \n",
    "    a Binomial(n,p) random variable\"\"\"\n",
    "    mu = n*p\n",
    "    sigma = math.sqrt(n*p*(1-p))\n",
    "    return mu, sigma\n",
    "\n",
    "print(normal_approx_to_binomial(10000,.5))\n",
    "print(normal_approx_to_binomial(10000,.8))\n",
    "print(normal_approx_to_binomial(10000,.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use `normal_cdf()` to figure out the probability its realized value lies within (or outside) a particular interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Normal/Gaussian CDF = probability the variable is BELOW a threshold\n",
    "def normal_cdf(x,mu=0,sigma=1):\n",
    "    return (1+math.erf((x-mu)/math.sqrt(2)/sigma))/2\n",
    "normal_prob_below = normal_cdf\n",
    "\n",
    "# it is ABOVE threshold if it's NOT BELOW threshold\n",
    "def normal_prob_above(lo,mu=0,sigma=1):\n",
    "    return 1 - normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# it is WITHIN if it's LESS THAN HI + MORE THAN LO\n",
    "def normal_prob_between(lo,hi,mu=0,sigma=1):\n",
    "    return normal_cdf(hi,mu,sigma) - normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# it is OUTSIDE if it's BETWEEN HI AND LOW\n",
    "def normal_prob_outside(lo,hi,mu=0,sigma=1):\n",
    "    return 1 - normal_prob_between(lo,hi,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse == *find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood*. \n",
    "\n",
    "For example, to find an interval centered at the mean + containing 60% probability, then we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p,mu=0,sigma=1,tolerance=.00001):\n",
    "    \"\"\"Find the approximate inverse using binary search\"\"\"\n",
    "    \n",
    "    # if not standard, standardize and re-scale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma*inverse_normal_cdf(p,tolerance=tolerance)\n",
    "    \n",
    "    low_z, low_p = -10.0, 0  # normal_cdf(-10) = very close to 0\n",
    "    hi_z, hi_p = 10.0, 1     # normal_cdf(10) = very close to 1\n",
    "    \n",
    "    while hi_z - low_z > tolerance: \n",
    "        mid_z = (low_z + hi_z) / 2 # consider midpoint\n",
    "        mid_p = normal_cdf(mid_z)  # and the CDF's value there\n",
    "        \n",
    "        if mid_p < p:\n",
    "            # if midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # if midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p \n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return mid_z\n",
    "\n",
    "def normal_upper_bound(p,mu=0,sigma=1):\n",
    "    \"\"\"Return the z for which P(Z <= z) = given p\"\"\"\n",
    "    return inverse_normal_cdf(p,mu,sigma)\n",
    "\n",
    "def normal_lower_bound(p,mu=0,sigma=1):\n",
    "    \"\"\"Return the z for which P(Z >= z) = given p\"\"\"\n",
    "    return inverse_normal_cdf(1-p,mu,sigma)\n",
    "\n",
    "def normal_two_sided_bounds(p,mu=0,sigma=1):\n",
    "    \"\"\"Returns symmetric (about the mean) interval\n",
    "    that contains given p\"\"\"\n",
    "    tail_probability = (1-p)/2\n",
    "    \n",
    "    # upper bound = tail probability value above it\n",
    "    upper_bound = normal_lower_bound(tail_probability,mu,sigma)\n",
    "    \n",
    "    # lower bound = tail probability value below it\n",
    "    lower_bound = normal_upper_bound(tail_probability,mu,sigma)\n",
    "    \n",
    "    return lower_bound,upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we choose to flip `n=100` times. If `H0` is true, then `X` should be distributed approximately normally with mean 50 and SD 15.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approx_to_binomial(1000,.5)\n",
    "print(mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to make a decision about **significance** — how willing we are to make a **type 1 error (“false positive”, or FP)**, in which we reject `H0` even though it’s true. This willingness is often set at 5% or 1%.\n",
    "\n",
    "Consider the test that rejects if X falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5% significance level\n",
    "normal_two_sided_bounds(.95,mu_0,sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming `p` *really* equals 0.5 (i.e., `H0` is true), there is just a 5% chance we observe an `X` that lies outside this interval, `(469,531)`, which is the *exact* significance we wanted. \n",
    "\n",
    "Said differently, if `H0` is true, then, approximately 19 times out of 20, this test will give the correct result.\n",
    "\n",
    "We are also often interested in **power** of a test = **the probability of not making a type 2 error (\"false negative\", or FN) = fail to reject `H0` even though it’s false**. \n",
    "\n",
    "To measure this, we must specify *exactly* what \"`H0` being false\" means. (merely knowing p is *not* 0.5 doesn’t give a ton of info about the distribution of `X`). \n",
    "\n",
    "In particular, let’s check what happens if p is *actually* 0.55, so that the coin is slightly biased toward heads. In that case, we can calculate the power of the test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886548001295367\n"
     ]
    }
   ],
   "source": [
    "# 95% bounds based on assumption that p=.5 (coin is fair)\n",
    "lo,hi = normal_two_sided_bounds(.95,mu_0,sigma_0)\n",
    "\n",
    "# actual mu + sigma based on p=.55\n",
    "mu_1, sigma_1 = normal_approx_to_binomial(1000,.55)\n",
    "\n",
    "# Type II Error (FN) = fail to reject H0 when it is indeed false\n",
    "# Occurs when random variable X is still in the original interval\n",
    "type_2_probability = normal_prob_between(lo,hi,mu_1,sigma_1)\n",
    "\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that `H0` was that the coin is *not* biased toward heads, or that `p<=.5`. In that case we want a *one-sided test* that rejects  when X is much larger than 50, but *not* when X is smaller than 50. \n",
    "\n",
    "So a 5%-significance test involves using `normal_probability_below` to find the cutoff below which 95% of the probability lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n"
     ]
    }
   ],
   "source": [
    "hi = normal_upper_bound(.95,mu_0,sigma_0)\n",
    "print(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value of 526 is less than 531 (upper bound of the interval, since we need more probability in the upper tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9363794803307173\n"
     ]
    }
   ],
   "source": [
    "type_2_probability = normal_prob_below(hi,mu_1,sigma_1)\n",
    "power = 1-type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more powerful test, since it no longer rejects `H0` when X is below 469 (very unlikely to happen if `H0` is true) and instead rejects when X is between 526 and 531 (somewhat likely to happen if `H0` is true). \n",
    "\n",
    "### p-values\n",
    "\n",
    "An alternative way of thinking about the preceding test involves **p-values**. Instead of choosing bounds based on some probability cutoff, we compute the probability (assuming `H0` is true) that we'd see a value at *least as extreme* as the one actually observed.\n",
    "\n",
    "For our two-sided test of whether the coin is fair,  compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_sided_p_val(x,mu=0,sigma=1):\n",
    "    # if observed value > mean, the tail is what's GREATER than x\n",
    "    if x >= mu:\n",
    "        return 2*normal_prob_above(x,mu,sigma)\n",
    "    # if observed value < mean, the tail is what's LESS than x\n",
    "    else:\n",
    "        return 2*normal_prob_below(x,mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we sat 530 heads in 100 flips, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 15.811388300841896\n",
      "0.05777957112359733\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approx_to_binomial(1000,.5)\n",
    "print(mu_0, sigma_0)\n",
    "\n",
    "print(two_sided_p_val(530,mu_0,sigma_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0 15.811388300841896\n",
      "0.06207721579598857\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approx_to_binomial(1000,.5)\n",
    "print(mu_0, sigma_0)\n",
    "\n",
    "print(two_sided_p_val(529.5,mu_0,sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using `529.5` instead of `530` = a **continuity correction**, which reflects the fact that `normal_probability_between(529.5,530.5,mu_0,sigma_0)` is a better estimate of the probability of seeing 530 heads than `normal_probability_between(530, 531, mu_0, sigma_0)` is.\n",
    "\n",
    "* Correspondingly, `normal_probability_above(529.5,mu_0,sigma_0)` is a better estimate of the probability of seeing at least 530 heads.\n",
    "\n",
    "One way to convince yourself this is a sensible estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06217\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    # count heads in 1000 flips\n",
    "    num_heads = sum(1 if random.random() < .5 else 0\n",
    "                   for _ in range(1000))\n",
    "    \n",
    "    # count how often this value is at least \"extreme\"\n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        extreme_value_count += 1\n",
    "    \n",
    "print(extreme_value_count/100000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this p-value > .05 and therefore > our 5% significance, we do NOT reject `H0`. If instead we saw 532 heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046345287837786575\n"
     ]
    }
   ],
   "source": [
    "print(two_sided_p_val(531.5,mu_0,sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we CAN reject `H0`. This is the exact same test as before, just a different way of approaching the statistics.\n",
    "\n",
    "Similarly, we'd have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_p_value = normal_prob_above\n",
    "lower_p_value = normal_prob_below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our one-sided test, if we saw 525 heads vs. 527 heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06062885772582083\n",
      "0.04686839508859242\n"
     ]
    }
   ],
   "source": [
    "print(upper_p_value(524.5, mu_0, sigma_0))\n",
    "print(upper_p_value(526.5, mu_0, sigma_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'd fail to reject and then reject, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "\n",
    "Make sure the data is roughly normally distributed before using `normal_probability_above` to compute p-values. The annals of bad data science are filled with examples of people opining that the chance of some\n",
    "observed event occurring at random is one in a million, when what they really mean is “the chance, assuming the data is distributed normally,” which is pretty meaningless if the data isn’t.\n",
    "\n",
    "There are various statistical tests for normality, but even plotting the data is a good start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
