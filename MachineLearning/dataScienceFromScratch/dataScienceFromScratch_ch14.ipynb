{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 14: Simple Linear Regression\n",
    "\n",
    "### The Model\n",
    "Investigating relationship between a user’s # of friends + amount of time spent on the site each day. Assume having more friends causes people to spend more time on the site, rather than an alternative explanation (Ch 5)\n",
    "\n",
    "VP of Engagement asks you to build a model describing this relationship. Since you\n",
    "found a pretty strong linear relationship, a natural place to start = a linear model.\n",
    "In particular, you hypothesize that there are constants, alpha + beta such that `y_i = B*x_i + alpha + eps_i` where `y_i` = # of minutes user *i* spends on site daily, `x_i` = # of friends user *i* has, and `eps_i` = (hopefully small) irreducible error term, representing the fact that there're other factors not accounted for by this simple model.\n",
    "\n",
    "Assuming we’ve determined such an alpha + beta, we make predictions simply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(alpha,beta,x_i):\n",
    "    return beta*x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we choose alpha and beta? Any choice of alpha and beta gives a predicted output for each input `x_i`, and since we know the actual output `y_i` we can compute error for each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual(alpha,beta,x_i,y_i):\n",
    "    \"\"\"Error from predicting y_i with beta*x_i + alpha\"\"\"\n",
    "    return y_i - predict(alpha,beta,x_i) # - (beta*x_i + alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we’d *really* like to know = **total error over the entire data set**. But we don’t want to just add the errors (too high + too low predictions may cancel), so instead add up squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sse(alpha,beta,x,y):\n",
    "    return sum(residual(alpha,beta,x_i,y_i)**2 for\n",
    "              x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **least squares solution** = choose alpha + beta that minimize `sse`. Using calculus (or tedious algebra), the error-minimizing alpha + beta are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SETUP\n",
    "from __future__ import division # error for mean() without\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"assumes x has at least two elements\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    import math\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n - 1)\n",
    "\n",
    "def correlation(x, y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x, y) / stdev_x / stdev_y\n",
    "\n",
    "def least_squares(x,y):\n",
    "    \"\"\"Given training data (x,y), find least-squares values \n",
    "    of alpha and beta\"\"\"\n",
    "    beta = correlation(x,y)*(standard_deviation(y)/standard_deviation(x))\n",
    "    alpha = mean(y) - beta*mean(x)\n",
    "    return alpha,beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of alpha simply says = when we see the average value of the independent variable `x`, predict average value of dependent variable `y`. The choice of beta means that when the input value increases by `standard_deviation(x)`, prediction increases by `correlation(x, y)*standard_deviation(y)`. \n",
    "\n",
    "In the case when x + y are *perfectly correlated*, a 1 SD increase in `x` results in a 1-SD-of-y increase in the prediction. When they’re perfectly *anticorrelated*, the increase in `x` results in a decrease in the prediction. And when correlation = 0, beta = zero, which means that changes in x don’t affect the prediction at all.\n",
    "\n",
    "It’s easy to apply this to the outlierless data from Chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  22.94755241346903   beta:  0.9038659456058649\n"
     ]
    }
   ],
   "source": [
    "num_friends = [100,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "\n",
    "outlier = num_friends.index(100) # index of outlier\n",
    "\n",
    "num_friends_good = [x for i, x in enumerate(num_friends)\n",
    "                    if i != outlier]\n",
    "\n",
    "daily_minutes_good = [x for i, x in enumerate(daily_minutes)\n",
    "                      if i != outlier]\n",
    "\n",
    "alpha,beta = least_squares(num_friends_good,daily_minutes_good)\n",
    "print(\"alpha: \",alpha,\"  beta: \",beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model says we expect a user w/ `n` friends to spend `22.95 + 0.903n` min on the site each day. That is, we predict a user w/ no friends would still spend about 23 min a\n",
    "day on the site + for each additional friend, we expect a user to spend almost 1 min. more on the site each day.\n",
    "\n",
    "Plot the prediction line to get a sense of how well the model fits the observed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7e8dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for friends in num_friends_good:\n",
    "    #print(friends*beta)\n",
    "    predictions.append(friends*beta + alpha)\n",
    "\n",
    "plt.scatter(num_friends_good, daily_minutes_good)\n",
    "plt.plot(num_friends_good, predictions, c = \"r\")\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.xlabel(\"# of Friends\")\n",
    "plt.ylabel(\"# of Minutes/Day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we need a better way to figure out how well we’ve fit the data than staring at\n",
    "the graph. A common measure = **coefficient of determination, or R-squared, or R2**, which = the fraction of the *total* variation in the dependent variable/outcome that is captured/explained by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3291078377836305"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tss(y):\n",
    "    \"\"\"Total Sum of Squares variation of y_i's from the mean\"\"\"\n",
    "    return sum(v**2 for v in de_mean(y))\n",
    "\n",
    "def r2(alpha,beta,x,y):\n",
    "    \"\"\"Fraction of variation in y capture by model, which equals\n",
    "    1 - fraction of variation in y NOT capture by model\"\"\"\n",
    "    return 1.0 - (sse(alpha,beta,x,y) / tss(y))\n",
    "\n",
    "r2(alpha,beta,num_friends_good,daily_minutes_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose alpha + beta that minimized the sum of the squared *prediction* errors. 1 linear model we *could've* chosen = “always predict mean(y)” (corresponding to alpha = mean(y) + beta = 0), whose SSE exactly = its TSS, which means an R2 = 0, which indicates a model\n",
    "that (obviously, in this case) performs no better than just predicting the mean (**baseline model**)\n",
    "\n",
    "Clearly, least squares model must be at *least as good as that one*, which means SSE is *at most* = TSS, which means that the R2 must be *at least* = 0, *and* the SSE *must be* *at least* 0, which means R2 can be *at most* 1.\n",
    "\n",
    "The higher the R2, the better a model fits the data. Here `R2 = 0.329` = model is only sort of okay at fitting the data + clearly there are other factors at play.\n",
    "\n",
    "### Using Gradient Descent\n",
    "\n",
    "If we write **`theta = [alpha, beta]`** (theta = our parameters), we can also solve this using **gradient descent**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.93746417548679 0.9043371597664965\n"
     ]
    }
   ],
   "source": [
    "def sq_error(x_i,y_i,theta):\n",
    "    alpha,beta = theta\n",
    "    return residual(alpha,beta,x_i,y_i)**2\n",
    "\n",
    "def sq_error_gradient(x_i,y_i,theta):\n",
    "    alpha,beta = theta\n",
    "    return [-2*residual(alpha,beta,x_i,y_i),       # partial derivative of alpha\n",
    "            -2*residual(alpha,beta,x_i,y_i)*x_i]   # partial derivative of beta\n",
    "\n",
    "# choose random values for theta to start\n",
    "random.seed(0)\n",
    "theta = [random.random(), random.random()]\n",
    "alpha,beta = minimize_stochastic(target_fn=sq_error,\n",
    "                                 gradient_fn=sq_error_gradient, \n",
    "                                 x=num_friends_good, \n",
    "                                 y=daily_minutes_good,\n",
    "                                 theta_0=theta,\n",
    "                                 alpha_0=0.0001)\n",
    "print(alpha,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same data w/ gradient descent, we get `alpha = 22.94`, `beta = 0.904`, which are very close to the exact answers.\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "Why choose least squares? 1 justification involves **maximum likelihood estimation**. Imagine we have a sample of data `{v_1, ..., v_n}` that comes from a distribution that\n",
    "depends on some unknown parameter `theta`: `p(v_1, ..., v_n | theta)`.\n",
    "\n",
    "If we didn’t know `theta`, we could turn around + think of this quantity as the **likelihood** of `theta` given the sample: `L(theta | {v_1, ..., v_n})`. Under this approach, the *most likely `theta`* = value that *maximizes* this likelihood function, `L` (i.e. value that makes the observed data the most probable). \n",
    "\n",
    "In the case of a *continuous* distribution, in which we have a **probability distribution function (PDF)** rather than a **probability mass function (PMF)** for *discrete* data, we can do the same thing.\n",
    "\n",
    "1 assumption often made about the simple regression model = **the regression errors are normally distributed w/ mean 0 + some (known) SD (i.e. e ~ N(mu,sigma))**. If that’s the case, then the likelihood based on seeing a pair `(x_i, y_i)` is:\n",
    " * `L(alpha,beta | x_i,y_i,sigma) = (1/sqrt(2*Pi*sigma))*exp((-(y_i - alpha - beta*x_i)^2/(2*sigma^2)))`\n",
    " \n",
    "**Likelihood based on the entire data set = product of the individual likelihoods**,\n",
    "which is **largest precisely when alpha and beta are chosen to minimize the SSE**. \n",
    "\n",
    "That is, in this case (**and w/ these assumptions**), **minimizing SSE is equivalent to maximizing the likelihood of the observed data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
