{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 15: Multiple Linear Regression\n",
    "\n",
    "VP is impressed w/ the predictive model, but thinks we can do better. We’ve collected additional data: for each of user, we know how many hours he works each day + whether he has a PhD + we'll use this additional data to improve the model.\n",
    "\n",
    "Hypothesize a linear model w/ more independent variables: `minutes = alpha + B1*friends +  + B2*workHours + B3*phd + epsilon` and introduce a dummy variable for users w/ PhDs (1 if have PhD, 0 without).\n",
    "\n",
    "Now out `x_i` = not a single # but a vector of *k* #'s `{x_i1, ..., x_ik}` = our predictors, + a multiple linear regression model assumes `y_i = alpha + B1*x_i1 + ... + Bk*x_ik + eps_i` where we have a vector of parameters `B`,  which should include the constant term as well (by adding a col of ones to the data): `beta = [alpha, beta_1, ..., beta_k]` and `x_i = [1, x_i1, ..., x_ik]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our model is just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x_i,beta):\n",
    "    \"\"\"Assumes the 1st element of each x_i = 1\"\"\"\n",
    "    return dot(x_i,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our independent variable `x` will be a list of vectors, each of which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 49, 4, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Assumptions of the Least Squares Model\n",
    "\n",
    "Assumptions required for this model (+ our solution) to make sense.\n",
    "\n",
    "* Columns of `x` = linearly independent  (no way to write any 1 as a weighted sum of some of the others. \n",
    "    * If this assumption fails, it’s impossible to estimate beta.\n",
    "    * To see this in an extreme case, imagine we had an extra field `num_acquaintances` that, for every user, was exactly = `num_friends`.\n",
    "    * Then, starting w/ any beta, if we add any amount to the `num_friends` coefficient + subtract that same amount from the `num_acquaintances coefficient`, the model’s predictions remain unchanged = means there’s no way to find the coefficient for `num_friends`.\n",
    "    * **Usually violations of this assumption won’t be so obvious**\n",
    "* Cols of x = all uncorrelated with the errors\n",
    "    * If this fails, estimates of beta will be systematically wrong.\n",
    "    * Ex: Ch14 = built a model that predicted each additional friend was associated w/ an extra 0.90 daily min. on the site.\n",
    "    * Imagine that it’s *also* the case that: \n",
    "        * People who work more hours spend less time on the site.\n",
    "        * People w/ more friends tend to work more hours.\n",
    "    * That is, imagine that the “actual” model is: `minutes = alpha _+ B1*friends + B2*workHours + epsilon` + that work hours + friends are positively correlated. \n",
    "     * In that case, when we minimize the errors of the single variable model: `minutes = alpha _+ B1*friends + + epsilon`, we will underestimate `B1`\n",
    "     * Think about what would happen if we made predictions using the single variable model W/ the “actual” value of B1\n",
    "         * That is, the value that arises from minimizing the errors of what we called the “actual” model\n",
    "      * The predictions would tend to be too small for users who work many hours + too large for users who work few hours, because B2 > 0 + we “forgot” to include it\n",
    "      * B/c work hours is positively correlated w/ friends, this means predictions tend to be too small for users w/ many friends + too large for users w/ few friends.\n",
    "      * The result = we can reduce the errors (in the single-variable model) by decreasing our estimate of B1, which means that the error-minimizing B1 = smaller than the “actual” value. \n",
    "          * That is, in this case the single-variable least squares solution is **biased** to underestimate B1. \n",
    "       * In general, whenever independent variables are correlated w/ the errors like this, our least squares solution will give us a biased estimate of B\n",
    "\n",
    "### Fitting the Model\n",
    "\n",
    "As in a simple linear model, choose beta to minimize the SSE. Finding an exact solution is not simple to do by hand, which means we’ll need to use **gradient descent** + start by creating an **error function** to minimize. For **stochastic gradient descent**, we’ll just want the **squared error corresponding to a single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(x_i,y_i,beta):\n",
    "    return y_i - predict(x_i,beta)\n",
    "\n",
    "def squared_error(x_i,y_i,beta):\n",
    "    return error(x_i,y_i,beta)**2\n",
    "\n",
    "# use calculus to compute:\n",
    "def squared_error_gradient(x_i,y_i,beta):\n",
    "    \"\"\"Gradient (with respect to beta) corresponding \n",
    "    to the ith squared error term\"\"\"\n",
    "    return [-2*x_ij * error(x_i,y_i,beta)\n",
    "           for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we’re ready to find the **optimal beta** using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.619881701311712, 0.9702056472470465, -1.8671913880379478, 0.9163711597955347]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "num_friends = [100,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "outlier = num_friends.index(100) # index of outlier\n",
    "num_friends_good = [x for i, x in enumerate(num_friends)\n",
    "                    if i != outlier]\n",
    "daily_minutes_good = [x for i, x in enumerate(daily_minutes)\n",
    "                      if i != outlier]\n",
    "\n",
    "def estimate_beta(x,y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(target_fn=squared_error,\n",
    "                              gradient_fn=squared_error_gradient,\n",
    "                              x=x,\n",
    "                              y=y,\n",
    "                              theta_0=beta_initial,\n",
    "                              alpha_0=.001)\n",
    "\n",
    "random.seed(0)\n",
    "betas = estimate_beta(x,daily_minutes_good)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our model looks like: \n",
    "\n",
    "* **`minutes = 30.61988 + .9702*friends - 1.8672*workHours + .91637*phd`**\n",
    "\n",
    "### Interpreting the Model\n",
    "\n",
    "Coefficients represent **\"all-else-being-equal\" estimates of the individual impacts of each factor**. All else being equal: \n",
    "\n",
    "* each additional friend corresponds to 1 extra min spent on the site each day. \n",
    "* each additional hour in a user’s workday corresponds to about 2 fewer min spent on the site each day. \n",
    "* having a PhD is associated with spending 1 extra minute on the site each day.\n",
    "\n",
    "What this doesn’t (*directly*) tell us = anything about the *interactions among the variables.* It’s possible that the effect of work hours is different for people w/ many friends than it for people w/ few friends. **This model doesn’t capture that**. \n",
    "\n",
    "1 way to handle this case is to introduce **interaction terms**, such as the product of “friends” and “work hours\", which effectively allows the “work hours” coefficient to increas/decrease as # of friends increases. \n",
    "\n",
    "Or it’s possible that the more friends you have, the more time you spend on the site up to a point, after which further friends cause you to spend *less* time on the site. (Perhaps w/ too many friends the experience is just too overwhelming) We could try to capture this in our model by adding another variable that’s the square of the number of friends. \n",
    "\n",
    "Once we start adding variables, we need to worry about whether their coefficients “matter.” There are *NO* limits to the #'s of products, logs, squares, and higher powers we could add.\n",
    "\n",
    "### Goodness of Fit\n",
    "\n",
    "Again we can look at the R-squared, which has now increased to 0.68:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800074955952597"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division # error for mean() without\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def tss(y):\n",
    "    \"\"\"Total Sum of Squares variation of y_i's from the mean\"\"\"\n",
    "    return sum(v**2 for v in de_mean(y))\n",
    "\n",
    "def multiple_r2(x,y,beta):\n",
    "    sse = sum(error(x_i,y_i,beta)**2\n",
    "             for x_i,y_i in zip(x,y))\n",
    "    return 1 - sse/tss(y)\n",
    "\n",
    "multiple_r2(x,daily_minutes_good,betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, however, that **adding new variables to a regression will *always* increase R2** b/c the simple regression model = just the special case of the multiple regression model where coefficients on “work hours” and “PhD” both = 0. The optimal multiple regression model has an error at least as small as that one.\n",
    "\n",
    "B/c of this, in a multiple regression, we *also* need to look @ standard errors of the\n",
    "coefficients, which measure how certain we are about our estimates of each . The regression *as a whole* may fit our data very well, but if some independent variables\n",
    "are correlated (or irrelevant), their coefficients might not mean much.\n",
    "\n",
    "Typical approach to measuring these standard errors of the coefficients starts w/ another **assumption = the errors are independent, normal random variables w/ mean 0 + some shared (unknown) SD**. In that case, use some linear algebra to find the standard error of each coefficient. \n",
    "\n",
    "* The larger it is, the less sure our model is about that coefficient. Unfortunately, we’re not set up to do that kind of linear algebra from scratch\n",
    "\n",
    "### Digression: The Bootstrap\n",
    "\n",
    "Imagine we have a sample of `n` DPs, generated by some (unknown to us) distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [random.random() for _ in range(0,1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use a  median function for observed data, which we *can use as an estimate of the median of the distribution itself.* But *how confident* can we be about our estimate? If all data in the sample are very close to 100, it seems likely that the actual median is close to 100. If approximately 1/2 the data in the sample = close to 0 + other 1/2 is close to 200, we can’t be nearly as certain about the median.\n",
    "\n",
    "If we could repeatedly get new samples, we could compute the median of each + look at\n",
    "the distribution of those medians. Usually we can’t. What we can do instead is **bootstrap** new data sets by choosing `n` DPs **with replacement** from our data + then compute the medians of those synthetic data sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    \"\"\"Randomly samples len(data) data points, with replacement\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data,stats_fn,n):\n",
    "    \"\"\"Evaluates a stats function on n bootstrap samples from given data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the two following data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 101 DPs close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 DPs, 50 close to 0, 50 close to 200\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the median of each, both will be very close to 100. However, if you look\n",
    "at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.04637035640484, 100.07450431305412, 100.04637035640484, 99.95531525534307, 99.99336507694278, 100.10110960104461, 100.0276755189074, 100.04864023078676, 100.10110960104461, 100.07450431305412, 100.1035249314939, 100.13550744528052, 100.07450431305412, 100.1310619964177, 100.11717121371706, 100.05344412850495, 100.04864023078676, 99.95172458006711, 100.04637035640484, 99.9550466424706, 100.0276755189074, 99.98897654573969, 100.07450431305412, 100.0276755189074, 100.04637035640484, 99.99336507694278, 100.05344412850495, 100.01068090616161, 100.04864023078676, 100.04864023078676, 100.01068090616161, 99.93224852312797, 100.10110960104461, 100.0763214327716, 100.04637035640484, 100.1035249314939, 99.95531525534307, 100.04637035640484, 99.9550466424706, 99.95172458006711, 100.07450431305412, 100.0276755189074, 100.0276755189074, 100.01068090616161, 100.1035249314939, 99.98897654573969, 100.04864023078676, 100.11601218691955, 100.0276755189074, 100.14121577117348, 100.05344412850495, 100.0763214327716, 99.99336507694278, 100.0763214327716, 99.95590624686454, 100.0763214327716, 100.10110960104461, 100.1035249314939, 99.98897654573969, 99.95531525534307, 99.91025598549241, 100.04637035640484, 100.11717121371706, 99.95590624686454, 100.11601218691955, 100.04864023078676, 99.99336507694278, 100.01068090616161, 99.95531525534307, 100.0276755189074, 99.93224852312797, 100.01068090616161, 100.07450431305412, 100.0276755189074, 100.04864023078676, 100.11601218691955, 99.95590624686454, 100.1310619964177, 100.13550744528052, 99.95531525534307, 100.0276755189074, 100.07450431305412, 100.0276755189074, 100.04864023078676, 100.04637035640484, 100.04637035640484, 100.0763214327716, 100.07450431305412, 99.9550466424706, 100.07450431305412, 100.04864023078676, 100.0276755189074, 100.04637035640484, 100.05344412850495, 99.98897654573969, 100.07450431305412, 100.04637035640484, 100.01068090616161, 100.0763214327716, 99.95172458006711]\n",
      "\n",
      "\n",
      "[200.09030008925873, 100.2955832767844, 100.2955832767844, 0.899392380481696, 0.8440169225198839, 200.0119930544755, 100.2955832767844, 200.0119930544755, 200.07212255699054, 200.06956980319873, 200.27871638430267, 200.0569968505966, 100.2955832767844, 0.9979024331230154, 200.12932505863537, 100.2955832767844, 0.8669054725482058, 0.8913040138229286, 0.8493688014422933, 0.8650568163710224, 100.2955832767844, 200.06956980319873, 200.17942727947926, 200.09030008925873, 200.06956980319873, 200.33951872232376, 0.8704687130969778, 0.9534614423020624, 200.33951872232376, 0.8615218769857105, 200.0569968505966, 0.9979024331230154, 0.8704687130969778, 0.8615218769857105, 0.8913040138229286, 0.899392380481696, 0.8493688014422933, 200.17942727947926, 200.24431734378467, 0.8615218769857105, 0.899392380481696, 200.12932505863537, 0.8630646958329531, 0.9979024331230154, 200.0569968505966, 0.8650568163710224, 0.9979024331230154, 200.0569968505966, 200.17942727947926, 0.8669054725482058, 200.09030008925873, 200.06956980319873, 0.899392380481696, 0.9979024331230154, 200.06956980319873, 0.9534614423020624, 100.2955832767844, 0.8669054725482058, 0.8650568163710224, 0.8913040138229286, 200.0119930544755, 0.899392380481696, 200.07212255699054, 100.2955832767844, 200.07212255699054, 0.899392380481696, 200.07212255699054, 200.09030008925873, 200.06956980319873, 0.899392380481696, 200.06956980319873, 0.8669054725482058, 100.2955832767844, 200.28471294255016, 100.2955832767844, 200.09030008925873, 0.9979024331230154, 200.12932505863537, 200.12932505863537, 200.0119930544755, 200.06956980319873, 0.8413668315442728, 200.2393797471772, 0.9534614423020624, 0.899392380481696, 0.899392380481696, 0.9979024331230154, 0.8704687130969778, 200.0569968505966, 0.8615218769857105, 200.09030008925873, 0.8704687130969778, 0.8669054725482058, 200.12932505863537, 200.07212255699054, 0.8704687130969778, 0.9979024331230154, 0.9979024331230154, 0.8630646958329531, 100.2955832767844]\n"
     ]
    }
   ],
   "source": [
    "def median(v):\n",
    "    \"\"\"finds the 'middle-most' value of v\"\"\"\n",
    "    n = len(v)\n",
    "    sorted_v = sorted(v)\n",
    "    midpoint = n // 2\n",
    "\n",
    "    if n % 2 == 1:\n",
    "        # if odd, return the middle value\n",
    "        return sorted_v[midpoint]\n",
    "    else:\n",
    "        # if even, return the average of the middle values\n",
    "        lo = midpoint - 1\n",
    "        hi = midpoint\n",
    "        return (sorted_v[lo] + sorted_v[hi]) / 2\n",
    "\n",
    "print(bootstrap_statistic(close_to_100,median,n=100))\n",
    "print(\"\\n\")\n",
    "print(bootstrap_statistic(far_from_100,median,n=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see #'s really close to 100 + #'s close to 0 and a lot of numbers close to 200.\n",
    "\n",
    "SD of 1st set of medians = close to 0, while SD of 2nd set of medians = close to 100.\n",
    "    \n",
    "* This extreme a case would be pretty easy to figure out by manually inspecting data, but in general that won’t be true\n",
    "\n",
    "### Standard Errors of Regression Coefficients\n",
    "\n",
    "Can take same approach to estimating Std Errors of  regression coefficients ==> repeatedly take a `bootstrap_sample()` of our data + estimate `beta` based on that sample. \n",
    "\n",
    "* **If the coefficient corresponding to an independent variables doesn’t vary much across samples = we can be confident our estimate is relatively tight.** \n",
    "* **If the coefficient varies greatly across samples, we can’t be at all confident in our estimate**\n",
    "\n",
    "Only subtlety = *before sampling*, need to `zip` x data + y data to make sure corresponding values of the independent + dependent variables are sampled together.\n",
    "\n",
    "* This means `bootstrap_sample()` will return a list of pairs `(x_i, y_i)`, which needs to be reassembles into an `x_sample` + a `y_sample`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_sample_beta(sample):\n",
    "    \"\"\"sample = list of zipped pairs (x_i,y_i) from x data and y data\"\"\"\n",
    "    # need to list() the unzip as it doesn't work in len() otherwise\n",
    "    x_sample, y_sample = zip(*sample) # zip(*...) = unzipping\n",
    "    return estimate_beta(x_sample,y_sample)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "bootstrap_betas = bootstrap_statistic(data=list(zip(x,daily_minutes_good)),\n",
    "                                     stats_fn=estimate_sample_beta,\n",
    "                                     n=100)\n",
    "#print(bootstrap_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which we can estimate the standard deviation of each coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant term (actual error = 1.19):  0.953551702104508 \n",
      " num_friends (actual error = 0.080):  0.06288763616183773 \n",
      " unemployed (actual error = 0.127):  0.11722269488203318 \n",
      " phd (actual error = 0.998):  0.8591786495949066\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"assumes x has at least two elements\"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "    \n",
    "bootstrap_std_errs = [standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "                      for i in range(4)]\n",
    "\n",
    "print(\"constant term (actual error = 1.19): \",bootstrap_std_errs[0],\"\\n\",\n",
    "     \"num_friends (actual error = 0.080): \",bootstrap_std_errs[1],\"\\n\",\n",
    "     \"unemployed (actual error = 0.127): \",bootstrap_std_errs[2],\"\\n\",\n",
    "     \"phd (actual error = 0.998): \",bootstrap_std_errs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use these to test hypotheses such as “does `B_i = 0`?” \n",
    "\n",
    "Under the null hypothesis `B-i = 0` (+ w/ the other assumptions about distribution of `epsilon_i`) the statistic `t_j = B^_j / sigma^_j` (estimate of `B_j` divided by our estimate of its std. error) follows a **Student’s t-distribution** with `n-k` degrees of freedom.\n",
    "\n",
    "If we had a `students_t_cdf()` function, we could compute p-values for each least-squares coefficient to indicate how likely we would be to observe such a value if the actual coefficient were zero. Unfortunately, we don’t have such a function. (Although we would if we weren’t working from scratch.)\n",
    "\n",
    "However, **as degrees of freedom get large, the t-distribution gets closer + closer to standard normal**. In a situation like this, where `n` is much larger than `k`, can use `normal_cdf()` + still feel good about ourselves:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 \n",
      "\n",
      "0.0 \n",
      "\n",
      "0.0 \n",
      "\n",
      "0.28616763748904184 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normal_cdf(x, mu=0,sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "def p_val(beta_hat_j,sigma_hat_j):\n",
    "    if beta_hat_j > 0:\n",
    "        # If coefficient > 0, need to compute 2X the probability of\n",
    "        #   seeing an even larger number\n",
    "        return 2*(1-normal_cdf(beta_hat_j/sigma_hat_j))\n",
    "    else:\n",
    "        # Otherwise 2X the probability of seeing a smaller value\n",
    "        return 2*normal_cdf(beta_hat_j/sigma_hat_j)\n",
    "\n",
    "## get p-values for each predictor\n",
    "print(p_val(betas[0], bootstrap_std_errs[0]),\"\\n\") # ~0 (constant term)\n",
    "print(p_val(betas[1], bootstrap_std_errs[1]),\"\\n\") # ~0 (num_friends)\n",
    "print(p_val(betas[2], bootstrap_std_errs[2]),\"\\n\") # ~0 (work_hours)\n",
    "print(p_val(betas[3], bootstrap_std_errs[3]),\"\\n\") # 0.36 (phd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most coefficients have very small p-values (suggesting they're indeed\n",
    "nonzero), coefficient for “PhD” = NOT “significantly” different from 0, which makes it **likely that the coefficient for “PhD” is random rather than meaningful.**\n",
    "\n",
    "In a situation *not* like this, we' probably be using statistical software that knows how to compute the t-distribution, as well as how to compute exact standard errors\n",
    "\n",
    "In more elaborate regression scenarios, sometimes want to test more elaborate hypotheses about data, such as “at least 1 `B_j` value is non-zero” or “`B1=B2` and `B3=B4`” which you can do with an **F-test** (outside scope of this book)\n",
    "\n",
    "### Regularization\n",
    "\n",
    "In practice, often like to apply linear regression to data sets w/ large #'s of variables, which creates a couple of extra wrinkles. \n",
    "\n",
    "* 1) more variables used = more likely to overfit to training\n",
    "* 2) more nonzero coefficients = harder it is to make sense of them. \n",
    "\n",
    "If goal = explain some phenomenon, a **sparse model** w/ 3 factors might be more useful than a slightly better model w/ 100's\n",
    "\n",
    "**Regularization** = add a penalty to the error term that gets larger as beta gets larger, then minimizing combined error + penalty. \n",
    " \n",
    "The more importance placed on penalty term = the more we discourage large coefficients\n",
    "\n",
    "Ex: **Ridge regression** = add a penalty proportional to the sum of squares of `beta_i` values (Except typically don’t penalize `beta_0`, the constant term.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha = *hyperparameter* controlling how harsh penalty is\n",
    "#  - sometimes called \"lambda\" (don't use in Python)\n",
    "def ridge_penalty(beta,alpha):\n",
    "    # ignore error term in beta's vector\n",
    "    return alpha*dot(beta[1:],beta[1:])\n",
    "\n",
    "def squared_error_ridge(x_i,y_i,beta,alpha):\n",
    "    \"\"\"Estimate error + ridge penaly on beta\"\"\"\n",
    "    return error(x_i,y_i,beta)**2 + ridge_penalty(beta,alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can then plug into gradient descent in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def ridge_penalty_gradient(beta,alpha):\n",
    "    \"\"\"Gradient of ridge penalty only\"\"\"\n",
    "    return [0] + [2*alpha*beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def squared_error_ridge_gradient(x_i,y_i,beta,alpha):\n",
    "    \"\"\"Gradient corresponding to the ith squared error term, \n",
    "    including the ridge penalty\"\"\"\n",
    "    return vector_add(squared_error_gradient(x_i,y_i,beta),\n",
    "                     ridge_penalty_gradient(beta,alpha))\n",
    "\n",
    "def estimate_beta_ridge(x,y,alpha):\n",
    "    \"\"\"Use gradient descent to fit a ridge regression with penalty = alpha\"\"\"\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    \n",
    "    return minimize_stochastic(target_fn=partial(squared_error_ridge\n",
    "                                                 ,alpha=alpha)\n",
    "                               ,gradient_fn=partial(squared_error_ridge_gradient\n",
    "                                                   ,alpha=alpha)\n",
    "                               ,x=x\n",
    "                               ,y=y\n",
    "                               ,theta_0=beta_initial\n",
    "                               ,alpha_0=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W/ alpha set = 0, there’s *no penalty* at all + we get the same results as before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.619881701311712, 0.9702056472470465, -1.8671913880379478, 0.9163711597955347]\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "beta_0 = estimate_beta_ridge(x=x,y=daily_minutes_good,alpha=0)\n",
    "print(beta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.267438780018153\n"
     ]
    }
   ],
   "source": [
    "print(dot(beta_0[1:],beta_0[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6800074955952597\n"
     ]
    }
   ],
   "source": [
    "print(multiple_r2(x,daily_minutes_good,beta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase alpha, goodness of fit gets worse, but size of beta gets smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.637971777349424, 0.966256348082445, -1.860156744084185, 0.866726206923135]\n",
      "5.145048760538865\n",
      "0.6799977193553848\n",
      "\n",
      "\n",
      "[30.753196848251086, 0.9004786098363016, -1.6968038434690242, 0.07517914853682801]\n",
      "3.6956569143586937\n",
      "0.6756080368777941\n",
      "\n",
      "\n",
      "[28.40096237219874, 0.7292520835324475, -0.9178168391782275, -0.019167017750511075]\n",
      "1.3745637261849766\n",
      "0.5743493477100823\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta_0_0 = estimate_beta_ridge(x,daily_minutes_good,.01)\n",
    "print(beta_0_0)\n",
    "print(dot(beta_0_0[1:],beta_0_0[1:]))\n",
    "print(multiple_r2(x,daily_minutes_good,beta_0_0))\n",
    "print(\"\\n\")\n",
    "\n",
    "beta_0_1 = estimate_beta_ridge(x,daily_minutes_good,1)\n",
    "print(beta_0_1)\n",
    "print(dot(beta_0_1[1:],beta_0_1[1:]))\n",
    "print(multiple_r2(x,daily_minutes_good,beta_0_1))\n",
    "print(\"\\n\")\n",
    "\n",
    "beta_0_2 = estimate_beta_ridge(x,daily_minutes_good,10)\n",
    "print(beta_0_2)\n",
    "print(dot(beta_0_2[1:],beta_0_2[1:]))\n",
    "print(multiple_r2(x,daily_minutes_good,beta_0_2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, PhD coefficient vanishes as we increase the penalty, which accords w/ our previous result that it wasn’t significantly different from zero.\n",
    "\n",
    "* ***NOTE*** Usually want to rescale data before using this approach. After all, if you changed years of experience to centuries of experience, its least squares coefficient would increase by a factor of 100 + suddenly get penalized much more, even though it’s the same model.\n",
    "\n",
    "**Lasso regression** = Another approach which uses a penalty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_regression(beta,alpha):\n",
    "    return alpha*sum(abs(beta_i) for beta_i in beta[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whereas ridge penalty shrank coefficients overall, a lasso penalty tends to force coefficients to be 0** = makes it good for learning sparse models. \n",
    "\n",
    "Unfortunately, lasso = not amenable to gradient descent, which means we won’t be able to solve it from scratch.\n",
    "\n",
    "### For Further Exploration\n",
    "\n",
    "* scikit-learn has a linear_model module that provides a LinearRegression model similar to ours, as well as Ridge regression, Lasso regression, + other types of regularization too.\n",
    "* Statsmodels is another Python module that contains (among other things) linear regression models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
