{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 18. Neural Networks\n",
    "\n",
    "An artificial neural network (or neural network for short) is a predictive model motivated\n",
    "by the way the brain operates. Think of the brain as a collection of neurons wired together.\n",
    "Each neuron looks at the outputs of the other neurons that feed into it, does a calculation,\n",
    "and then either fires (if the calculation exceeds some threshhold) or doesn’t (if it doesn’t).\n",
    "Accordingly, artificial neural networks consist of artificial neurons, which perform similar\n",
    "calculations over their inputs. Neural networks can solve a wide variety of problems like\n",
    "handwriting recognition and face detection, and they are used heavily in deep learning,\n",
    "one of the trendiest subfields of data science. However, most neural networks are “black\n",
    "boxes” — inspecting their details doesn’t give you much understanding of how they’re\n",
    "solving a problem. And large neural networks can be difficult to train. For most problems\n",
    "you’ll encounter as a budding data scientist, they’re probably not the right choice.\n",
    "Someday, when you’re trying to build an artificial intelligence to bring about the\n",
    "Singularity, they very well might be\n",
    "\n",
    "### Perceptrons\n",
    "Pretty much the simplest neural network is the perceptron, which approximates a single\n",
    "neuron with n binary inputs. It computes a weighted sum of its inputs and “fires” if that\n",
    "weighted sum is zero or greater:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(class_probs):\n",
    "    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n",
    "    return sum(-p*math.log(p,2) for p in class_probs if p) # ignore p = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data = consist of pairs `(input, label)`, which means we’ll need to compute the class probabilities ourselves. *We don’t actually care which label is associated w each probability, only what the probabilities are:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Counter({'blue': 3, 'red': 2, 'green': 1})\n",
    "\n",
    "def class_probs(labels):\n",
    "    total_count = len(labels)\n",
    "    # for each count of a label, find the fraction of that label count\n",
    "    # over total DP count\n",
    "    return [count / total_count for count in Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probs = class_probs(labels)\n",
    "    return entropy(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Entropy of a Partition\n",
    "\n",
    "We've computed entropy (“uncertainty”) of a single set of labeled data. Each stage of a DT involves asking a question in which the answer partitions data into 1 or (hopefully) more subsets. \n",
    "* Ex: “does it have > 5 legs?” partitions animals into those w/ > 5 legs + those that don’t\n",
    "\n",
    "Correspondingly, want some notion of the entropy that will result from partitioning a set of data in a certain way = **want a partition to:\n",
    "* have low entropy if it splits data into subsets that *themselves* have low entropy (i.e., are highly certain)\n",
    "* have high entropy if it contains subsets that (are large +) have high entropy (i.e., are highly uncertain).**\n",
    "\n",
    "Ex: “does it have > 5 legs?” question = pretty dumb, as it partitioned remaining animals at that point into = `S1={echidna}` and `S2={everything else}`, where `S2` = both large + high-entropy. (`S1` has no entropy but represents a small fraction of the remaining “classes.”)\n",
    "\n",
    "Mathematically, if we partition data `S` into subsets `{S1, ..., Sm}` , containing proportions `{q1, ..., qm}` of the data, then we compute **entropy of the partition** as a weighted sum:\n",
    "* `H = q_1*H(S1) + ... + q_m*H(Sm)`\n",
    "\n",
    "which we can implement as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"Find entropy from a parition of data into subsets, where\n",
    "    subsets = a list of lists of labeled data\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum(data_entropy(subset) * len(subset) / total_count\n",
    "               for subset in subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: 1 problem w/ this approach = partitioning by an attribute w/ many different values results in very low entropy due to overfitting. \n",
    "* Ex: trying to build a DT predict which customers of a bank = likely to default on mortgages using some historical data as a training set. \n",
    "* Imagine further that the data set contains each customer’s SSN, + partitioning on SSN will produce 1-person subsets, each of which necessarily has 0 entropy. \n",
    "* But a model that relies on SSN is *CERTAIN* not to generalize beyond training\n",
    "* For this reason, probably avoid (or bucket, if appropriate) attributes w/ large #'s of possible values when creating DT's\n",
    "\n",
    "### Creating a Decision Tree\n",
    "\n",
    "VP provides you w/ interviewee data, consisting of (per your specification) pairs `(input, label)`, where each `input` = a `dict` of candidate attributes, + each `label` =  either `True` (candidate interviewed well) or `False` (candidate interviewed poorly).\n",
    "\n",
    "In particular, you're provided w/ each candidate’s `level`, preferred `language`, whether active on `Twitter`, + whether has a `PhD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'}, False),\n",
    "    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'}, False),\n",
    "    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    "    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    "    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, False),\n",
    "    ({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, True),\n",
    "    ({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, False),\n",
    "    ({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'}, True),\n",
    "    ({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),\n",
    "    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, True),\n",
    "    ({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'}, True),\n",
    "    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DT = consists of decision **nodes** (ask a question + direct us differently depending on answer) + **leaf** nodes (give a prediction). We will build it using relatively simple **ID3 algorithm**, which operates in the following manner. \n",
    "* Given some labeled data + a list of attributes to consider branching on.\n",
    "* If the data all have the same label, create a leaf node that predicts that label + stop.\n",
    "* If the list of attributes = empty (i.e., no more possible questions to ask), create a leaf node that predicts the most common label + stop.\n",
    "* Otherwise, try partitioning the data by each of the attributes\n",
    "* Choose the partition w/ lowest partition entropy\n",
    "* Add a decision node based on the chosen attribute\n",
    "* Recur on each partitioned subset using remaining attributes\n",
    "\n",
    "This = a **“greedy” algorithm** b/c, at each step, it chooses the **most\n",
    "immediately best option**. Given a data set, there may be a better tree w/ a worse-looking first move. If so, this algorithm won’t find it. Nonetheless, it is relatively easy to understand + implement, which makes it a good place to begin exploring DTs.\n",
    "\n",
    "Interviewee data set has both `True` + `False` labels w/ 4 attributes to split on. 1st step = find the partition w/ least entropy via a function that does the partitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1st step = find partition w/ least entropy\n",
    "from collections import defaultdict\n",
    "\n",
    "# do the partitioning\n",
    "def partition_by(inputs,attribute):\n",
    "    \"\"\"Each input = pair (attribute_dict, label)\n",
    "    This returns a dict: attribute_value -> inputs\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key=input[0][attribute] # get value of specified attribute\n",
    "        groups[key].append(input)\n",
    "    return groups\n",
    "\n",
    "# compute entropy\n",
    "def partition_entropy_by(inputs, attributes):\n",
    "    \"\"\"Computes entropy corresponding to given partition\"\"\"\n",
    "    partitions = partition_by(inputs,attributes)\n",
    "    \n",
    "    return partition_entropy(partitions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then find the minimum-entropy partition for the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs,key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest entropy = splitting on `level` ==> make a subtree for each possible `level` value. Every `Mid` candidate = labeled `True`, which means that `Mid` subtree = leaf node predicting `True`. For `Senior` candidates, we have a mix of `True`s and `False`s, so split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [(input,label)\n",
    "                for input,label in inputs if input[\"level\"] == \"Senior\"]\n",
    "\n",
    "for key in ['lang','tweets','phd']:\n",
    "    print(key,partition_entropy_by(senior_inputs,key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next split = on `tweets` = a 0-entropy partition. For these Senior-level candidates, “yes” tweets always = `True` while “no” tweets always = `False`.\n",
    "\n",
    "Finally, if we do the same thing for the `Junior` candidates, end up splitting on `phd`, after which we find that no PhD always results in `True` and PhD always = `False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "Now we’ve seen how the algorithm works, implement it more generally = means we need to decide how we want to represent trees. We’ll use pretty\n",
    "much the most lightweight representation possible + define a tree to be one of the following:\n",
    "* True\n",
    "* False\n",
    "* a tuple (attribute, subtree_dict)\n",
    "\n",
    "Here `True` = leaf node that returns `True` for any input, `False` = leaf node that returns `False` for any input, + a `tuple` represents a decision node that, for any input, finds its attribute value + classifies the input using the corresponding subtree. W/ this representation, our hiring tree would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('level',\n",
       " {'Junior': ('phd', {'no': True, 'yes': False}),\n",
       "  'Mid': True,\n",
       "  'Senior': ('tweets', {'no': False, 'yes': True})})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('level',\n",
    " # if their level = junior, check PHD in new subtree\n",
    " {'Junior': ('phd',\n",
    "             {'no': True, 'yes': False}),\n",
    "  # if their level = mid, predict paid\n",
    "  'Mid': True,\n",
    "  # if their level = senior, check PHD in new subtree\n",
    "  'Senior': ('tweets',\n",
    "             {'no': False, 'yes': True})})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s still the question of what to do if we encounter an *unexpected* (or missing) attribute value, like if it encounters a candidate whose `level` = “Intern”? We’ll handle this case by adding a `None` key that just predicts the most common label. (bad idea if `None` is actually a value that appears in the data). Given such a representation, we can classify an input with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_dt(tree,input):\n",
    "    \"\"\"Classify an input using the given decision tree\"\"\"\n",
    "    # if on a leaf node, return its value (i.e. level = Mid)\n",
    "    if tree in [True,False]:\n",
    "        return tree\n",
    "    \n",
    "    # otherwise, tree consists of an attribute to split on and a \n",
    "    #  dict whose keys = values of that attribute and \n",
    "    #  values = subtrees to consider next\n",
    "    attribute, subtree_dict = tree\n",
    "    \n",
    "    subtree_key = input.get(attribute) # returns None is input is missing attribute\n",
    "    \n",
    "    # if no subtree for key, use the None subtree\n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None\n",
    "        \n",
    "    # choose appropriate subtree + use it to classify input\n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    return classify_dt(subtree,input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the tree representation from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def build_tree_id3(inputs,split_candidates=None):\n",
    "    ## If on 1st pass, all keys of 1st input = split candidates\n",
    "    if split_candidates is None:\n",
    "        split_candidates = inputs[0][0].keys() # ['level','lang','tweets','phd']\n",
    "    \n",
    "    ## count Trues + Falses for inputs\n",
    "    num_inputs = len(inputs)\n",
    "    # item = dictionary, label = single boolean label\n",
    "    num_trues = len([label for item, label in inputs if label])\n",
    "    num_false = num_inputs - num_trues\n",
    "    \n",
    "    ## if we have no trues/falses, return a false/true leaf\n",
    "    if num_trues == 0:\n",
    "        return False\n",
    "    if num_false == 0:\n",
    "        return True\n",
    "    \n",
    "    ## if we have no split candidates left, return majority leaf\n",
    "    if not split_candidates:\n",
    "        return num_trues >= num_false\n",
    "    \n",
    "    ## if still have split candidates, split on 'best' attribute\n",
    "    best_attribute = min(split_candidates,\n",
    "                        key = partial(partition_entropy_by,inputs))\n",
    "    partitions = partition_by(inputs,best_attribute)\n",
    "    new_candidates = [a for a in split_candidates if a != best_attribute]\n",
    "    \n",
    "    ## recursively build subtrees\n",
    "    subtrees = {attribute_value: build_tree_id3(subset,new_candidates)\n",
    "               for attribute_value, subset in partitions.items()}\n",
    "    subtrees[None] = num_trues >= num_false # default case\n",
    "    \n",
    "    return (best_attribute,subtrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tree, every leaf consisted entirely of `True` inputs or entirely of `False` inputs = means the tree predicts perfectly on training, but we can also apply it to new data that wasn’t in training and data w/ missing or unexpected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tree = build_tree_id3(inputs)\n",
    "\n",
    "print(classify_dt(tree,\n",
    "            { \"level\" : \"Junior\",\n",
    "             \"lang\" : \"Java\",\n",
    "             \"tweets\" : \"yes\",\n",
    "             \"phd\" : \"no\"}))\n",
    "print(classify_dt(tree,\n",
    "            { \"level\" : \"Junior\",\n",
    "             \"lang\" : \"Java\",\n",
    "             \"tweets\" : \"yes\",\n",
    "             \"phd\" : \"yes\"}))\n",
    "# new/unexpected values\n",
    "print(classify_dt(tree,\n",
    "            {\"level\" : \"Intern\"}))\n",
    "print(classify_dt(tree,\n",
    "            {\"level\" : \"Senior\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Since our goal was mainly to demonstrate how to build a tree, we built it using the entire data set. As always, if we were really trying to create a good model for something, we would've (collected more data +) split the data into train/validation/test subsets.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Given how closely DT'd can fit themselves to training, it’s not surprising they have a tendency to overfit. 1 way of avoiding this = **random forests** = build multiple DTs + let them \"vote\" on how to classify inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_classify(trees,input):\n",
    "    votes = [classify_dt(tree,input) for tree in trees]\n",
    "    \n",
    "    vote_counts = Counter(votes)\n",
    "    # return 1st most common vote + its \n",
    "    return vote_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-building process was **deterministic**, so how do we get **random** trees?\n",
    "* 1) **bootstrapping** data = Rather than training each tree on all inputs in training, train each tree on a result of `bootstrap_sample(inputs)`. \n",
    "    * Since each tree = built using different data, each tree will be different from every other tree\n",
    "    * Side benefit = it’s totally fair to use the nonsampled data to test each tree, which means you can get away w/ using all data as training if you're clever in how you measure performance\n",
    "    * This technique = **bootstrap aggregating** or **bagging**.\n",
    "* 2) Changing the way we chose `best_attribute` to split on.\n",
    "    * Rather than looking @ all remaining attributes, 1st choose a random subset of them + then split on whichever of those is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# if already have few-enough split candidates, look @ all of them\n",
    "if len(split_candidates) <= self.num_split_candidates:\n",
    "    sampled_split_candidates = split_candidates\n",
    "# otherwise pick a random sample\n",
    "else:\n",
    "    sampled_split_candidates = random.sample(split_candidates,\n",
    "                                            self.num_split_candidates)\n",
    "    \n",
    "# now choose best attribute only from those candidates\n",
    "best_attribute = min(sampled_split_candidates),\n",
    "                    key=partial(partition_entropy_by,inputs))\n",
    "paritions = partition_by(inputs,best_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This = an example of a broader technique called **ensemble learning** = combine several weak learners (typically high-bias, low-variance models) in order to produce an overall strong model.\n",
    "\n",
    "Random forests = 1 of the most popular and versatile models around.\n",
    "\n",
    "### For Further Exploration\n",
    "* scikit-learn has many DT models + an ensemble module that includes a RandomForestClassifier as well as other ensemble methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
