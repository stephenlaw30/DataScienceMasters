{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 23. MapReduce\n",
    "\n",
    "**MapReduce** = programming model for performing **parallel processing** on large data sets = powerful technique w/ basics = relatively simple. Imagine we have a collection of items we’d like to process somehow (website logs, texts of various books, image files, etc.). A basic version of the MapReduce algorithm consists of the following steps:\n",
    "1. Use a **mapper/map function** to turn each item into 0 or more KV-pairs\n",
    "2. Collect together all pairs w/ identical keys.\n",
    "3. Use a **reducer function** on each collection of grouped values to produce output\n",
    "values for the corresponding key.\n",
    "\n",
    "Specific example: There're few *absolute rules* of data science, but 1 = your 1st MapReduce example has to involve counting words. Our site has grown to millions of users = great for job security, but it makes routine analyses slightly more difficult. VP of Content wants to know what sorts of things people are talking about in status updates. As a 1st attempt, you decide to count the words that appear so that you can prepare a report on the most frequent ones. When you had a few hundred users this was simple to do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import re\n",
    "#from naive_bayes import tokenize\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0, './../../../00_DataScience/DSFromScratch/code')\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract words\n",
    "    return set(all_words)                           # remove duplicates\n",
    "\n",
    "def word_count_old(docs):\n",
    "    \"\"\"Counts words NOT using MapReduce\"\"\"\n",
    "    return Counter(word for doc in docs for word in tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W/ millions of users, the set of documents (status updates) is suddenly too big to fit on\n",
    "your CPU. If you can just fit this into the **MapReduce model**, you can use some “big\n",
    "data” infrastructure our engineers have implemented.\n",
    "1st, we need a function that turns a document into a sequence of KV-pairs, where keys = words b/c we want our output to be grouped by word, + for each word, emit the value `1` to indicate this pair corresponds to 1 occurrence of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrd_cnt_mapper(doc):\n",
    "    \"\"\"For each word in the document, emit `(word,1)`\"\"\"\n",
    "    for word in tokenize(doc):\n",
    "        yield (word, 1) # yield = like 'return' but returns a generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping the “plumbing” step 2 for the moment, imagine that for some word, we’ve collected a list of corresponding counts emitted. Then, to produce the overall count for that word we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wrd_cnt_reducer(word,counts):\n",
    "    \"\"\"Sum up the counts for a word\"\"\"\n",
    "    yield (word,sum(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to step 2: need to collect results from `wc_mapper()` + feed them to `wc_reducer`. Let’s think about how we would do this on just one CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def word_count(docs):\n",
    "    \"\"\"Count the word in the list of input documents via MapReduce\"\"\"\n",
    "    # store grouped values\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word,count in wrd_cnt_mapper(doc):\n",
    "            collector[word].append(count)\n",
    "    \n",
    "    return [output for word,counts in collector.items()\n",
    "           for output in wrd_cnt_reducer(word,counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have 3 documents `[\"data science\", \"big data\", \"science fiction\"]`. Then, `wc_mapper` applied to the 1st document yields the 2 pairs `(\"data\", 1)` + `(\"science\", 1)`. After we’ve gone through all 3 documents, the collector contains 3 lists of KV-pairs of words and counts, then `wc_reducer` produces the count for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 2), ('science', 2), ('big', 1), ('fiction', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count([\"data science\", \"big data\", \"science fiction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why MapReduce?\n",
    "\n",
    "Primary benefit of **MapReduce = allows us to distribute computations by moving the processing to the data**. Imagine we want to word-count across billions of docs.\n",
    "Our original (non-MapReduce) approach requires the CPU doing the processing to actually have access to every doc = means docs all need to either live on that machine or else be transferred to it during processing. More important, it means that the machine can only process 1 document at a time.\n",
    "\n",
    "***NOTE***L Possibly it can process up to a few at a time if it has multiple cores + if code is rewritten to take advantage of them. But even so, all the docs still have to get to that machine.\n",
    "\n",
    "Imagine our billions of docs are scattered across 100 machines. W/ the right infrastructure (+ glossing over some details), we can do the following:\n",
    "\n",
    "* Have each machine run the mapper on its own docs, producing lots of (K,V) pairs.\n",
    "* Distribute those (K, V) pairs to a # of “reducing” machines, *making sure the pairs corresponding to any given key all end up on the same machine.*\n",
    "* Have each reducing machine group pairs by key + then run the reducer on each set of values.\n",
    "* Return each (key, output) pair.\n",
    "\n",
    "What is amazing about this = **it scales horizontally**. If we double the # of machines, then (ignoring certain fixed-costs of running a MapReduce system) our computation should run approximately 2x as fast. Each mapper machine will only need to do 1/2 as much work, + (assuming there're enough distinct keys to further distribute reducer work) the same is true for the reducer machines.\n",
    "\n",
    "### MapReduce More Generally\n",
    "If you think about it for a minute, all of the word-count-specific code in previous\n",
    "example = contained in the `wrd_cnt_mapper` + `wrd_cnt_reducer` functions. This means that w/ a couple of changes, we have a much more general framework (that still runs on a single machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('science', 2), ('big', 1), ('fiction', 1)]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"data science\", \"big data\", \"science fiction\"]\n",
    "\n",
    "def map_reduce(inputs,mapper,reducer):\n",
    "    \"\"\"Runs MapReduce on the inputs using given mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for input in inputs:\n",
    "        for k,v in mapper(input):\n",
    "            collector[k].append(v)\n",
    "            \n",
    "    return [output for k,vs in collector.items()\n",
    "            for output in reducer(k,vs)]\n",
    "\n",
    "# count words\n",
    "word_counts = map_reduce(docs,wrd_cnt_mapper,wrd_cnt_reducer)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the flexibility to solve a wide variety of problems. Before we proceed, observe `wc_reducer()` = just summing the values corresponding to each key. This kind of aggregation is common enough that it’s worth abstracting it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def reduce_values_using(aggregation_fn,k,vs):\n",
    "    \"\"\"Reduces a K-Vs pairs by appying aggregation_fn to the values\"\"\"\n",
    "    yield (k, aggregation_fn(vs))\n",
    "\n",
    "def values_reducer(aggregation_fn):\n",
    "    \"\"\"Turns a function(values -> output) into a reduce that\n",
    "    maps (K, V's) -> (K, outputs)\"\"\"\n",
    "    return partial(reduce_values_using, aggregation_fn)\n",
    "\n",
    "sum_reducer = values_reducer(sum)\n",
    "max_reducer = values_reducer(max)\n",
    "min_reducer = values_reducer(min)\n",
    "\n",
    "count_distinct_reducer = values_reducer(lambda values: len(set(values)))\n",
    "\n",
    "#print(count_distinct_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Analyzing Status Updates\n",
    "\n",
    "Content VP was impressed w/ the word counts + asks what else you can learn from people’s status updates. You manage to extract a data set of status updates that look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "status_updates = [{\"id\": 1,\n",
    "                   \"username\" : \"joelgrus\",\n",
    "                   \"text\" : \"Is data anyone interested in a data science book, data baby?\",\n",
    "                   \"created_at\" : datetime.datetime(2013, 12, 21, 11, 47, 0),\n",
    "                   \"liked_by\" : [\"data_guy\", \"data_gal\", \"mike\"]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we need to figure out which day of the week people talk the most about data\n",
    "science. To find this, count how many \"data science updates\" on each day of the week = *group by the day of week* = our key. If we emit a value = 1 for each update that contains “data science,” can simply get the total number using `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1)]\n"
     ]
    }
   ],
   "source": [
    "def ds_day_mapper(status_update):\n",
    "    \"\"\"Yields (DOW,1) if status_update contains \"data science\" \"\"\"\n",
    "    if \"data science\" in status_update[\"text\"].lower():\n",
    "        dow = status_update[\"created_at\"].weekday()\n",
    "    \n",
    "    yield(dow,1)\n",
    "\n",
    "ds_days = map_reduce(status_updates, ds_day_mapper, sum_reducer)\n",
    "print(ds_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a slightly more complicated example, imagine we need to find out, for each user, the\n",
    "most common word they put in their status updates. There are 3 possible approaches that spring to mind for the mapper:\n",
    "\n",
    "* Put username = key; put words + counts = values.\n",
    "* Put word = key; put usernames + counts = values.\n",
    "* Put username + word = key; put counts = values.\n",
    "\n",
    "If you think about it a bit more, definitely want to group by `username`, b/c we want to consider each person’s words separately, + we don’t want to group by `word`, since our `reducer` will need to see all words for each person to find out the most popular. This means the *1st option is the right choice*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', ('science', 1))]\n"
     ]
    }
   ],
   "source": [
    "def words_per_user_mapper(status_update):\n",
    "    \n",
    "    user = status_update[\"username\"]\n",
    "    \n",
    "    for word in tokenize(status_update[\"text\"]):\n",
    "        yield(user, (word, 1))\n",
    "\n",
    "def most_popular_word_reducer(user,words_and_counts):\n",
    "    \"\"\"Given a sequence of (word,count) pairs,\n",
    "    return word with highest total count\"\"\"\n",
    "    word_counts = Counter()\n",
    "    \n",
    "    for word,count in words_and_counts:\n",
    "        word_counts[word] += count\n",
    "        \n",
    "    word,count = word_counts.most_common(1)[0]\n",
    "    \n",
    "    yield(user,(word,count))\n",
    "\n",
    "user_words = map_reduce(status_updates, words_per_user_mapper, most_popular_word_reducer)\n",
    "print(user_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could find out the number of distinct status-likers for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('joelgrus', 3)]\n"
     ]
    }
   ],
   "source": [
    "def liker_mapper(status_update):\n",
    "    user = status_update[\"username\"]\n",
    "    for liker in status_update[\"liked_by\"]:\n",
    "        yield(user,liker)\n",
    "\n",
    "dist_likers_per_user = map_reduce(status_updates,liker_mapper,count_distinct_reducer)\n",
    "print(dist_likers_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Matrix Multiplication\n",
    "Recall from “Matrix Multiplication” that given a `m*n` matrix A + `n*k` matrix B B, we multiply them to form a `m*k` matrix C, where the element of C in row i + column j is given by:\n",
    "\n",
    "* **`C_ij = A_i1B_1j + A_i2B_2j + ... + A_inB_nj`**\n",
    "\n",
    "As we’ve seen, a “natural” way to represent an `m*n` matrix is with a `list` of `list`s,\n",
    "where the element `A_ij` = jth element of ith list.\n",
    "\n",
    "But large matrices are sometimes **sparse = most of elements = 0**. For large sparse matrices, a list of lists can be a very wasteful representation. **A more compact representation = a list of tuples (name, i, j, value)** where `name` ID's the matrix, +  `i`, `j`, `value` indicates a location w/ non-zero value.\n",
    "\n",
    "Ex: Billion × billion matrix has a quintillion entries, which would not be easy to store on a CPU. But if there are only a few non-zero entries in each row, this alternative representation is many orders of magnitude smaller.\n",
    "\n",
    "**Given this sort of representation, it turns out we can use MapReduce to perform matrix multiplication in a distributed manner.** To motivate our algorithm, notice each element `A_ij` is only used to compute the elements of C in row `i`, + each element `B_ij`is only used to compute the elements of C in column `j`. \n",
    "\n",
    "**Goal = each output of our `reducer` = a single entry of C**, which means we’ll need our `mapper` to emit keys IDing a single entry of C. This suggests the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0), 32), ((0, 1), -3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mtx_multiply_mapper(m,element):\n",
    "    \"\"\"m is the common dimension (cols of A, rows of B), and \n",
    "    element is a tuple (mtx_name, i, j, value)\"\"\"\n",
    "    mtx, i, j, value = element\n",
    "    \n",
    "    if mtx == \"A\":\n",
    "        # A_ij = jth entry in su, for each C_ik, k=1..m\n",
    "        for col in range(m):\n",
    "            # group with other entries for C_ij = C_i_col\n",
    "            yield((i,col), (j,value))\n",
    "    else:\n",
    "        # B_ij = ith entry in su, for each C_kj = C_row_j\n",
    "        for row in range(m):\n",
    "            # group with other entries for C_kj = C_row_j\n",
    "            yield((row,j), (i,value)) \n",
    "            \n",
    "def mtx_multiply_reducer(m,key,indexed_values):\n",
    "    results_by_index = defaultdict(list)\n",
    "    \n",
    "    for index,value in indexed_values:\n",
    "        results_by_index[index].append(value)\n",
    "        \n",
    "    # sum up all products of the positions w/ 2 results\n",
    "    sum_product = sum(results[0]*results[1]\n",
    "                     for results in results_by_index.values()\n",
    "                     if len(results) == 2)\n",
    "    \n",
    "    if sum_product != 0:\n",
    "        yield(key,sum_product)\n",
    "\n",
    "        # matrices\n",
    "A = [[3,2,0],\n",
    "     [0,0,0]]\n",
    "B = [[4,-1,0],\n",
    "    [10,0,0],\n",
    "    [0,0,0]]\n",
    "\n",
    "# matrices as tuples\n",
    "entries = [(\"A\",0,0,3),(\"A\",0,1,2),\n",
    "          (\"B\",0,0,4),(\"B\",0,1,-1),(\"B\",1,0,10)]\n",
    "\n",
    "mapper = partial(mtx_multiply_mapper,3)\n",
    "reducer = partial(mtx_multiply_reducer,3)\n",
    "\n",
    "map_reduce(entries,mapper,reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn’t terribly interesting on such small matrices, but if you had millions of rows + millions of columns, it could help you a lot.\n",
    "\n",
    "### An Aside: Combiners\n",
    "1 thing you've probably noticed: many of our `mappers` seem to include a bunch of extra info. For example, when counting words, rather than emitting `(word, 1)` + summing over the values, we could've emitted `(word, None)` + just taken the `length`.\n",
    "\n",
    "1 reason we didn’t do this = in the distributed setting, we sometimes want to use **combiners** to reduce the amount of data that has to be transferred around from machine to machine. If 1 of our mapper machines sees the word “data” 500 times, we can tell it to\n",
    "combine the 500 instances of (\"data\", 1) into a single (\"data\", 500) before handing off to the reducing machine = results in a lot less data getting moved around, which can make our algorithm substantially faster still. B/c of the way we wrote our `reducer`, it would handle this combined data correctly. (If we’d written it using `len` it would not have.)\n",
    "\n",
    "### For Further Exploration\n",
    "* Most widely used MapReduce system = **Hadoop**, which itself merits many books. There are various commercial and noncommercial distributions and a huge ecosystem of Hadoop-related tools.\n",
    "    * In order to use it, must set up your own cluster (or find someone to let you use theirs), which is not necessarily a task for the faint-hearted. Hadoop mappers + reducers are commonly written in Java, although there is a facility known as “Hadoop streaming” that allows you to write them in other languages (including Python).\n",
    "* Amazon.com offers an Elastic MapReduce service that can programmatically create + destroy clusters, charging you only for the amount of time that you’re using them.\n",
    "* mrjob = a Python package for interfacing with Hadoop (or Elastic MapReduce).\n",
    "* Hadoop jobs = typically high-latency, which makes them a poor choice for “realtime” analytics. There are various “real-time” tools built on top of Hadoop, but there are also several alternative frameworks that are growing in popularity. 2 of the most popular = **Spark** and **Storm**.\n",
    "\n",
    "All that said, by now it’s quite likely that the flavor of the day is some hot new\n",
    "distributed framework that didn’t even exist when this book was written. You’ll have to\n",
    "find that one yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
