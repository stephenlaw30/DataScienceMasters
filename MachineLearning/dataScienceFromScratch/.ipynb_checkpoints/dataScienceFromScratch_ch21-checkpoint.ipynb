{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 21 Network Analysis\n",
    "\n",
    "Many interesting data problems can be fruitfully thought of in terms of **networks** consisting of **nodes** of some type + the **edges** that join them.\n",
    "Ex: FB friends form the nodes of a network whose edges = friendship relations, or the Web itself, w/ each web page a node, + each hyperlink from 1 page to another an edge.\n",
    "\n",
    "FB friendship = mutual, so in this case, edges = **undirected**. Hyperlinks are *not*, so those edges = **directed**\n",
    "\n",
    "### Betweenness Centrality\n",
    "\n",
    "Ch1: Computed the key connectors in the network by counting # of friends each user had. Now we have enough machinery to look @ other approaches. Recall that the network comprised users and friendships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n",
    "               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added friend lists to each user `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "    \n",
    "for i,j in friendships:\n",
    "    # add i as friend of j and vice versa\n",
    "    users[i][\"friends\"].append(users[j])\n",
    "    users[j][\"friends\"].append(users[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we left off, we were dissatisfied w/ our notion of **degree centrality**, which didn’t really agree w/ intuition about the **key connectors** of the network.\n",
    "\n",
    "An alternative metric = **betweenness centrality** = ID's people who frequently are on the shortest paths between pairs of other people. In particular, the betweenness centrality of node `i` = adding up, for every other pair of nodes `j` and `k`, the proportion of shortest paths between node `j` and node `k` that pass through `i`.\n",
    "\n",
    "That is, to figure out Thor’s betweenness centrality, compute all the shortest\n",
    "paths between all pairs of people who aren’t Thor + then count how many of those shortest paths pass through Thor. \n",
    "\n",
    "* Ex: Only shortest path between Chi (id 3) and Clive (id 5) passes through Thor, while neither of the 2 shortest paths between Hero (id 0) and Chi (id 3) does.\n",
    "\n",
    "So, as a 1st step, figure out the shortest paths between all pairs of people. There are some pretty sophisticated algorithms for doing so efficiently, but (as is almost always the case) we will use a less efficient, easier-to-understand algorithm.\n",
    "\n",
    "This algorithm = an implementation of **breadth-first search**:\n",
    "\n",
    "1. Goal = a function that takes a `from_user` + finds all shortest paths to every other user.\n",
    "2. Represent a path = list of user IDs. Since every path starts at `from_user`, don’t include that ID in the list (length of list representing the path will be the length of the path itself).\n",
    "3. Maintain a dictionary `shortest_paths_to` where keys = user IDs + values = lists of paths that end at the user w/ the specified ID. If there is a unique shortest path, list will just contain that 1 path. If there are multiple shortest paths, list will contain all of them.\n",
    "4. Also maintain a queue frontier that contains the users we want to explore in the order we want to explore them, stored as pairs `(prev_user, user)` so we know how we got to each one. \n",
    "5. Initialize the queue w/ all neighbors of `from_user` (**queues** = data structures optimized for “add to the end” + “remove from the front” operations that, in Python, are implemented as **collections.deque**, which is actually a double-ended queue)\n",
    "6. Explore the graph, + whenever we find new neighbors we don’t already know shortest paths to, add them to the end of the queue to explore later, w/  current user as `prev_user`.\n",
    "7. When we take a user off the queue + we’ve never encountered that user before, we’ve definitely found 1+ shortest paths to him — each of the shortest paths to `prev_user` w/ 1 extra step added.\n",
    "8. When we take a user off the queue + have encountered that user before, either we’ve found another shortest path (in which case we should add it) or we’ve found a longer path (in which case we shouldn’t).\n",
    "9. When no more users are left on the queue, we’ve explored the whole graph (or, at least, the parts that are reachable from the starting user) + we’re done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\",\"'\")\n",
    "\n",
    "url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html,\"html5lib\")\n",
    "\n",
    "content = soup.find(\"div\",\"article-body\") # find entry content div\n",
    "regex = r\"[\\w]+|[\\.]\" # match either a word or period\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex,fix_unicode(paragraph.text))\n",
    "    document.extend(words)\n",
    "    \n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly could (+ likely should) clean this data further. There is still some amount of extraneous text in the doc (ex: “Section”), we’ve split on midsentence periods (ex; “Web 2.0”), + there's are a handful of captions + lists sprinkled throughout. Having said that, we’ll work w/ the doc it is.\n",
    "\n",
    "Now that we have the text as a sequence of words, can model language in the\n",
    "following way: \n",
    "\n",
    "* given some starting word, look @ all words that follow it in the source doc \n",
    "* randomly choose 1 of these to be the next word\n",
    "* repeat process until we get to a period = signifies end of sentence.\n",
    "\n",
    "This = a **bigram model** = is determined completely by frequencies of **bigrams (word pairs)** in the original data. For a starting word, just pick randomly from words that follow a period.\n",
    "\n",
    "To start, precompute the possible word transitions (Recall `zip` stops when any of its inputs is done, so that `zip(document, document[1:])` gives us precisely the pairs of consecutive elements of document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigrams = zip(document,document[1:])\n",
    "transitions = defaultdict(list)\n",
    "for prev,current in bigrams:\n",
    "    transitions[prev].append(current)\n",
    "    \n",
    "#print(bigrams,\"\\n\")\n",
    "#print(transitions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_using_bigrams():\n",
    "    current = \".\" # next word starts a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current] # bigrams(current, _)\n",
    "        current = random.choice(next_word_candidates) # choose one randomly\n",
    "        result.append(current) # append to results\n",
    "        if current == \".\": return \" \".join(result) # if \".\", we're done\n",
    "    return next_word_candidates\n",
    "        \n",
    "print(generate_using_bigrams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences produced = gibberish, but the the kind of gibberish you might put on a website if trying to sound data-science-y. \n",
    "\n",
    "* Ex: If you may know which are you want to data sort the data feeds web friend someone on trending topics as the data in Hadoop is the data science requires a book demonstrates why visualizations are but we do massive correlations across many commercial disk drives in Python language and creates more tractable form making connections then use and uses it to solve a data.\n",
    "\n",
    "We can make sentences less gibberishy by looking at **trigrams**, triplets of consecutive words. (More generally, look at **n-grams** consisting of `n` consecutive words, but 3 will be plenty for now.) Now the transitions will depend on the previous *two* words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = zip(document,document[1:],document[2:])\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in trigrams:\n",
    "    if prev == \".\": # if previous \"word\" was = a period\n",
    "        starts.append(current) # then it's a start word\n",
    "    trigram_transitions[(prev,current)].append(next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we now have to track  starting words separately, but can generate sentences in pretty much the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_trigrams():\n",
    "    current = random.choice(starts) # choose random word from list above\n",
    "    prev = \".\" # precede this word with a period\n",
    "    result = [current]\n",
    "    \n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev,current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        \n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "        \n",
    "        if current == \".\":\n",
    "            return \" \".join(result)\n",
    "        \n",
    "generate_using_trigrams()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces better sentences like:\n",
    "\n",
    "* In hindsight MapReduce seems like an epidemic and if so does that give us new insights into how economies work That’s not a question we could even have asked a few years there has been instrumented. \n",
    "\n",
    "Of course, they sound better b/c at each step, generation process has fewer choices, + at many steps only a single choice. This means you frequently generate sentences (or at least long phrases) that were seen verbatim in the original data. Having more data would help; it would also work better if you collected n-grams from multiple essays about data science.\n",
    "\n",
    "### Grammars\n",
    "\n",
    "Different approach to modeling language = w/ **grammars** = rules for generating acceptable sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = {\"_S\" : [\"_NP _VP\"],\n",
    "           \"_NP\" : [\"_N\",\n",
    "                    \"_A _NP _P _A _N\"],\n",
    "           \"_VP\" : [\"_V\",\n",
    "                    \"_V _NP\"],\n",
    "           \"_N\" : [\"data science\", \"Python\", \"regression\"],\n",
    "           \"_A\" : [\"big\", \"linear\", \"logistic\"],\n",
    "           \"_P\" : [\"about\", \"near\"],\n",
    "           \"_V\" : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names starting w/ underscores = rules that need further expanding, + other names = terminals that don’t need further processing.\n",
    "\n",
    "* i.e \"_S\" = “sentence” rule, which produces a \"_NP\" (“noun phrase”) rule followed by a \"_VP\" (“verb phrase”) rule.\n",
    "* verb phrase rule can produce either the \"_V\" (“verb”) rule, or the verb rule followed by the noun phrase rule.\n",
    "* Notice the \"_NP\" rule contains itself in 1 of its productions. \n",
    "    * Grammars can be recursive = allows even finite grammars like this to generate infinitely many different sentences.\n",
    "\n",
    "How do we generate sentences from this grammar? = start w/ a list containing the sentence rule [\"_S\"], repeatedly expand each rule by replacing it w/ a randomly chosen one of its productions, + stop when we have a list consisting solely of terminals.\n",
    "\n",
    "* Ex: 1 such progression might look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['_S']\n",
    "['_NP','_VP']\n",
    "['_N','_VP']\n",
    "['Python','_VP']\n",
    "['Python','_V','_NP']\n",
    "['Python','trains','_NP']\n",
    "['Python','trains','_A','_NP','_P','_A','_N']\n",
    "['Python','trains','logistic','_NP','_P','_A','_N']\n",
    "['Python','trains','logistic','_N','_P','_A','_N']\n",
    "['Python','trains','logistic','data science','_P','_A','_N']\n",
    "['Python','trains','logistic','data science','about','_A', '_N']\n",
    "['Python','trains','logistic','data science','about','logistic','_N']\n",
    "['Python','trains','logistic','data science','about','logistic','Python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement this? == start by creating a simple helper function to ID terminals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_terminal(token):\n",
    "    return token[0] != \"_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function to turn a list of tokens into a sentence, 1st looking for 1st non-terminal token + if we can’t find one, it means we have a completed sentence + we’re done. If we *do* find a nonterminal, we randomly choose 1 of its productions. If that production = a terminal (i.e., a word), simply replace the token w it. Otherwise it’s a sequence of space-separated non-terminal tokens that we need to split + then splice into the current tokens. Either way, we repeat the process on the new set of tokens. Putting it all together we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(grammer,tokens):\n",
    "    for i,token in enumerate(tokens):\n",
    "        # skip over terminals\n",
    "        if is_terminal(token):\n",
    "            continue\n",
    "        # if we get here, we have a non-terminal token, \n",
    "        #   so choose random replacements\n",
    "        replacement = random.choice(grammar[token])\n",
    "        \n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "        \n",
    "        # now recursively call expand on new list of tokens\n",
    "        return expand(grammar,tokens)\n",
    "\n",
    "    # if we get here, we have all terminals + we're done\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can start generating sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(grammar):\n",
    "    return expand(grammar, [\"_S\"])\n",
    "\n",
    "generate_sentences(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the grammar — add more words, more rules, your own parts of\n",
    "speech — until you’re ready to generate as many web pages as the company needs.\n",
    "\n",
    "Grammars = actually more interesting when used in the *other direction*: Given a sentence, use a grammar to parse the sentence which allows us to ID\n",
    "subjects + verbs + helps us make sense of the sentence. Using data science to generate text = a neat trick; using it to understand text is more magical. \n",
    "\n",
    "### An Aside: Gibbs Sampling\n",
    "\n",
    "Generating samples from some distributions is easy. We can get uniform random variables with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and normal random variables w/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './../../../00_DataScience/DSFromScratch/code')\n",
    "\n",
    "from probability import inverse_normal_cdf\n",
    "\n",
    "inverse_normal_cdf(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some distributions = harder to sample from. **Gibbs sampling** = technique for generating samples from multidimensional distributions when we only know some of the conditional distributions.\n",
    "\n",
    "Ex: Rolling 2 dice + let x = value of 1st die + let y = sum of dice, + imagine you wanted to generate lots of `(x, y)` pairs. In this case it’s easy to\n",
    "generate the samples directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But imagine you only knew the *conditional* distributions = **distribution of y conditional on x is easy == if you know value of x, y = equally likely to be x + 1, x + 2, x + 3, x + 4, x + 5, or x + 6:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_y_given_x(x):\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other direction = more complicated. If we know y = 2, necessarily x = 1 (since only way 2 dice sum to 2 is both = 1). If we know y = 3, x = equally likely to be 1 or 2. Similarly, if y is 11, x is either 5 or 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_x_given_y():\n",
    "    if y <= 7:\n",
    "        # first die is equally likely to be 1, 2, ..., total - 7\n",
    "        return random.randrange(1,y)\n",
    "    else:\n",
    "        # first die is equally likely to be total - 6, total - 5, ..., 6\n",
    "        return random.randrange(y-6,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gibbs sampling works by starting w/ any (valid) value for x + y + then repeatedly alternate-replacing x w/ a random value, picked conditional on y + replacing y w/ a random value picked conditional on x. After a # of iterations, resulting values of x + y will represent a sample from the unconditional joint distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sample(num_iters=100):\n",
    "    x,y = 1,2 # arbitrary\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x,y\n",
    "\n",
    "def compare_distributions(num_samples=100):\n",
    "    counts = defaultdict(lambda: [0,0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "A more sophisticated approach to understanding users’ interests for You Should Know recommenders might try to ID topics that underlie those interests. A technique called **Latent Dirichlet Analysis (LDA)** = commonly used to ID common topics in a set of docs. We’ll apply it to docs that consist of each user’s interests.\n",
    "\n",
    "LDA = some similarities to Naive Bayes Classifier in that *it assumes a probabilistic model for docs*. Glossing over over hairier mathematical details, LDA model assumes that:\n",
    "\n",
    "* There is some fixed number `K` of topics.\n",
    "* There is a random variable that assigns each topic an associated probability distribution over the words (think of this distribution as the probability of seeing word `w` given a topic `k`.\n",
    "* There is another random variable that assigns each doc a probability distribution over topics (this distribution = the mixture of topics in doc `d`).\n",
    "* Each word in a doc was generated by 1st randomly picking a topic (from doc's distribution of topics) + then randomly picking a word (from chosen topic’s distribution of words).\n",
    "\n",
    "In particular, we have a collection of docs, each of which = a list of words, + we\n",
    "have a corresponding collection of `document_topics` that assigns a topic (here a # between 0 + K – 1) to each word in each document. so that the 5th word in the 4th doc = `doc[3][4]` + the topic from which that word was chosen is: `document_topics[3][4]`.\n",
    "\n",
    "This very explicitly defines each doc’s distribution over topics, + implicitly defines each topic’s distribution over words. We can estimate the likelihood topic 1 produces a certain word by comparing how many times topic 1 produces said word w/ how many times topic 1 produces *any* word.(Similarly, Ch13 spam filter = compared how many times each\n",
    "word appeared in `spams` w/ total # of words appearing in `spams`.)\n",
    "\n",
    "Although these topics are just #'s, we can give them descriptive names by looking at\n",
    "the words on which they put the *heaviest weight*, we just have to somehow generate the\n",
    "`document_topics` = where Gibbs sampling comes into play.\n",
    "\n",
    "Start by assigning every word in every doc a topic completely at random, then go through each doc, 1 word at a time. For that word + doc, construct weights for each topic that depend on the (current) distribution of topics in that doc + the (current) distribution of words for that topic. Then use those weights to sample a new topic for that word. If we iterate this process many times, we end up w/ a joint sample from the topic-word distribution + the doc-topic distribution.\n",
    "\n",
    "To start, need a function to randomly choose an index based on an arbitrary set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    \"\"\"Returns i with p = weights[i]/sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random() # uniform random variable between 0 and total\n",
    "    for i,w in enumerate(weights):\n",
    "        rnd -= w   # return smallest i such that weights[0] + ... + weights[i] >= rnd\n",
    "        if rnd <= 0: return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: If we give it weights = `[1, 1, 3]`, then 1/5 of the time it will return 0,\n",
    "1/5 of the time it will return 1, + 3/5 of the time it will return 2.\n",
    "\n",
    "Our docs = users’ interests, which look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find K = 4 topics. In order to calculate the sampling weights, must keep track of several counts. 1st, create the data structures for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "K = 4\n",
    "\n",
    "# How many times each topic is assigned to each doc:\n",
    "    # a list of Counters, one for each document\n",
    "doc_topic_counts = [Counter() for _ in docs]\n",
    "\n",
    "# how many times each word is assigned to each topic\n",
    "    # list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# total # of words assinged in each topic\n",
    "    # list of #'s, 1 for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# total # of words contained in each doc\n",
    "    # list of #'s 1 for each doc\n",
    "doc_lengths = [len(d) for d in docs]\n",
    "\n",
    "# # of distinct words\n",
    "distinct_words = set(word for doc in docs for word in doc)\n",
    "W = len(distinct_words)\n",
    "\n",
    "# # of docs\n",
    "D = len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the above populate these, we can find, for example, the # of words in `documents[3]` associated w/ topic 1 as `doc_topic_counts[3][1]` and the # of times \"nlp\" is associated with topic 2 as `topic_word_counts[2]['nlp']`. \n",
    "\n",
    "Now to define the conditional probability functions, each with a smoothing term that ensures every topic has a non-zero chance of being chosen in any doc and that every word has a non-zero chance of being chosen for any topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic,d,alpha=.1):\n",
    "    \"\"\"Reurns fraction of words in document 'd' that are\n",
    "    assigned to 'topic' (including some smoothing)\"\"\"\n",
    "    return ((doc_topic_counts[d][topic] + alpha) / \n",
    "            (doc_lengths[d] + K*alpha))\n",
    "\n",
    "def p_word_given_topic(word,topic,beta=.1):\n",
    "    \"\"\"Returns fraction of words assigned to 'topic'\n",
    "    that equals 'words' (plus some smoothing)\"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) / \n",
    "           (topic_counts[topic] + W*beta))\n",
    "\n",
    "# use the above to create weights for updating topics\n",
    "def topic_weight(d,word,k):\n",
    "    \"\"\"Given a document and a word within it\n",
    "    return the weight for the kth topic\"\"\"\n",
    "    return p_word_given_topic(word,k)*p_topic_given_document(k,d)\n",
    "\n",
    "def choose_new_topic(d,word):\n",
    "    return sample_from(weights=[topic_weight(d,word,k) for k in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There're solid mathematical reasons why `topic_weight` is defined the way it is, but details would lead us too far afield. Intuitive sense = given a word + its document, the **likelihood of any topic choice depends on both how likely that topic is for the document  how likely that word is for the topic.**\n",
    "\n",
    "Now, we start by assigning every word to a random topic + populating counters appropriately:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "doc_topics = [[random.randrange(K) for word in doc] for doc in docs]\n",
    "#print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    for word,topic in zip(docs[d],doc_topics[d]):\n",
    "        doc_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "print(doc_topic_counts,\"\\n\",topic_word_counts,\"\\n\",topic_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal = to get a joint sample of the topics-words distribution + the documents-topics distribution using a form of Gibbs sampling that uses the conditional probabilities defined previously:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word,topic) in enumerate(zip(docs[d],doc_topics[d])):\n",
    "            # remove word/topic from counts so that\n",
    "            # it doesn't influence weights\n",
    "            doc_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            doc_lengths[d] -= 1\n",
    "            \n",
    "            # choose new topic based on weights\n",
    "            new_topic = choose_new_topic(d,word)\n",
    "            doc_topics[d][i] = new_topic\n",
    "            #print(new_topic)\n",
    "            # now add back into counts\n",
    "            doc_topic_counts[d][topic] += 1\n",
    "            topic_word_counts[topic][word] += 1\n",
    "            topic_counts[topic] += 1\n",
    "            doc_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics = #'s 0-3, and to get names for them must add them ourselves.\n",
    "\n",
    "Can look @ the 5 most heavily weighted words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,word_counts in enumerate(topic_word_counts):\n",
    "    for word,count in word_counts.most_common(5):\n",
    "        if count > 0: print(k,word,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these, probably assign topic names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = ['Machine Learning','Deep Learning','Languages','Big Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at which point we can see how the model assigns topics to each user’s interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc,topic_counts in zip(docs,doc_topic_counts):\n",
    "    print(doc)\n",
    "    for topic,count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "doc_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in docs]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(docs[d], doc_topics[d]):\n",
    "        doc_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(docs[d],\n",
    "                                              doc_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            doc_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            doc_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            doc_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            doc_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            doc_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k, word_counts in enumerate(topic_word_counts):\n",
    "#    for word, count in word_counts.most_common():\n",
    "#        if count > 0: print(k, word, count)\n",
    "\n",
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"databases\",\n",
    "               \"machine learning\",\n",
    "               \"statistics\"]\n",
    "\n",
    "for document, topic_counts in zip(docs, doc_topic_counts):\n",
    "    print(doc)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "#\n",
    "# TOPIC MODELING\n",
    "#\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()       # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                        # return the smallest i such that\n",
    "        if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K = 4\n",
    "\n",
    "document_topic_counts = [Counter()\n",
    "                         for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the k-th topic\"\"\"\n",
    "\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "print(document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(documents[2], document_topics[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "print(document_topic_counts,\"\\n\",topic_word_counts,\"\\n\",topic_counts)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(zip(documents[d],document_topics[d])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            #document_topics[d][i] = new_topic\n",
    "            print(new_topic)\n",
    "            # and now add it back to the counts\n",
    "            #document_topic_counts[d][new_topic] += 1\n",
    "            #topic_word_counts[new_topic][word] += 1\n",
    "            #topic_counts[new_topic] += 1\n",
    "            #document_lengths[d] += 1\n",
    "\n",
    "#for k,word_counts in enumerate(topic_word_counts):\n",
    "  #  for word,count in word_counts.most_common(5):\n",
    "    #    if count > 0: print(k,word,count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    " # topic MODELING\n",
    "\n",
    "   # for k, word_counts in enumerate(topic_word_counts):\n",
    "     #   for word, count in word_counts.most_common():\n",
    "     #       if count > 0: print(k, word, count)\n",
    "#\n",
    "    topic_names = [\"Big Data and programming languages\",\n",
    "                   \"databases\",\n",
    "                   \"machine learning\",\n",
    "                   \"statistics\"]\n",
    "\n",
    "    for document, topic_counts in zip(documents, document_topic_counts):\n",
    "        print(document)\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            if count > 0:\n",
    "                print(topic_names[topic], count)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "print document\n",
    "for topic, count in topic_counts.most_common():\n",
    "if count > 0:\n",
    "print topic_names[topic], count,\n",
    "print\n",
    "which gives:\n",
    "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
    "Big Data and programming languages 4 databases 3\n",
    "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
    "databases 5\n",
    "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
    "Python and statistics 5 machine learning 1\n",
    "and so on. Given the “ands” we needed in some of our topic names, it’s possible we\n",
    "should use more topics, although most likely we don’t have enough data to successfully\n",
    "learn them.\n",
    "For Further Exploration\n",
    "Natural Language Toolkit is a popular (and pretty comprehensive) library of NLP tools\n",
    "for Python. It has its own entire book, which is available to read online.\n",
    "gensim is a Python library for topic modeling, which is a better bet than our fromscratch\n",
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
