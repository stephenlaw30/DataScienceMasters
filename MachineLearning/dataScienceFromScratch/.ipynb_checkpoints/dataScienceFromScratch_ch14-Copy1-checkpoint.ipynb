{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 15: Multiple Linear Regression\n",
    "\n",
    "VP is impressed w/ the predictive model, but thinks we can do better. We’ve collected additional data: for each of user, we know how many hours he works each day + whether he has a PhD + we'll use this additional data to improve the model.\n",
    "\n",
    "Hypothesize a linear model w/ more independent variables: `minutes = alpha + B1*friends +  + B2*workHours + B3*phd + epsilon` and introduce a dummy variable for users w/ PhDs (1 if have PhD, 0 without).\n",
    "\n",
    "Now out `x_i` = not a single # but a vector of *k* #'s `{x_i1, ..., x_ik}` +  a multiple linear regression model assume `y_i = alpha + B1*x_i1 + ... + Bk*x_ik + eps_i` where we have a vector of parameters `B`.,  which should include the constant term as well (by adding a col of ones to the data): `beta = [alpha, beta_1, ..., beta_k]` and `x_i = [1, x_i1, ..., x_ik]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our model is just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(x_i,beta):\n",
    "    \"\"\"Assumes the 1st element of each x_i = 1\"\"\"\n",
    "    return dot(x_i,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our independent variable `x` will be a list of vectors, each of which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 49, 4, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,    # constant term\n",
    " 49,   # number of friends\n",
    " 4,    # work hours per day\n",
    " 0]    # doesn't have PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Assumptions of the Least Squares Model\n",
    "\n",
    "Assumptions required for this model (+ our solution) to make sense.\n",
    "\n",
    "* Columns of `x` = linearly independent  (no way to write any 1 as a weighted sum of some of the others. \n",
    "    * If this assumption fails, it’s impossible to estimate beta.\n",
    "    * To see this in an extreme case, imagine we had an extra field `num_acquaintances` that, for every user, was exactly = `num_friends`.\n",
    "    * Then, starting w/ any beta, if we add any amount to the `num_friends` coefficient + subtract that same amount from the `num_acquaintances coefficient`, the model’s predictions remain unchanged = means there’s no way to find the coefficient for `num_friends`.\n",
    "    * **Usually violations of this assumption won’t be so obvious**\n",
    "* Cols of x = all uncorrelated with the errors\n",
    "    * If this fails, estimates of beta will be systematically wrong.\n",
    "    * Ex: Ch14 = built a model that predicted each additional friend was associated w/ an extra 0.90 daily min. on the site.\n",
    "    * Imagine that it’s *also* the case that: \n",
    "        * People who work more hours spend less time on the site.\n",
    "        * People w/ more friends tend to work more hours.\n",
    "    * That is, imagine that the “actual” model is: `minutes = alpha _+ B1*friends + B2*workHours + epsilon` + that work hours + friends are positively correlated. \n",
    "     * In that case, when we minimize the errors of the single variable model: `minutes = alpha _+ B1*friends + + epsilon`, we will underestimate `B1`\n",
    "     * Think about what would happen if we made predictions using the single variable model W/ the “actual” value of B1\n",
    "         * That is, the value that arises from minimizing the errors of what we called the “actual” model\n",
    "      * The predictions would tend to be too small for users who work many hours + too large for users who work few hours, because B2 > 0 + we “forgot” to include it\n",
    "      * B/c work hours is positively correlated w/ friends, this means predictions tend to be too small for users w/ many friends + too large for users w/ few friends.\n",
    "      * The result = we can reduce the errors (in the single-variable model) by decreasing our estimate of B1, which means that the error-minimizing B1 = smaller than the “actual” value. \n",
    "          * That is, in this case the single-variable least squares solution is **biased** to underestimate B1. \n",
    "       * In general, whenever independent variables are correlated w/ the errors like this, our least squares solution will give us a biased estimate of B\n",
    "\n",
    "### Fitting the Model\n",
    "\n",
    "As in a simple linear model, choose beta to minimize the SSE. Finding an exact solution is not simple to do by hand, which means we’ll need to use **gradient descent** + start by creating an **error function** to minimize. For **stochastic gradient descent**, we’ll just want the **squared error corresponding to a single prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_i, beta):\n",
    "    return dot(x_i, beta)\n",
    "\n",
    "def error(x_i,y_i,beta):\n",
    "    return y_i - predict(x_i,beta)\n",
    "\n",
    "def squared_error(x_i,y_i,beta):\n",
    "    return error(x_i,y_i,beta)**2\n",
    "\n",
    "# use calculus to compute:\n",
    "def squared_error_gradient(x_i,y_i,beta):\n",
    "    \"\"\"Gradient (with respect to beta) corresponding \n",
    "    to the ith squared error term\"\"\"\n",
    "    return [-2*x_ij * error(x_i,y_i,beta)\n",
    "           for x_ij in x_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we’re ready to find the **optimal beta** using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.619881701311712, 0.9702056472470465, -1.8671913880379478, 0.9163711597955347]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "x = [[1,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "num_friends = [100,49,41,40,25,21,21,19,19,18,18,16,15,15,15,15,14,14,13,13,13,13,12,12,11,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "daily_minutes = [1,68.77,51.25,52.08,38.36,44.54,57.13,51.4,41.42,31.22,34.76,54.01,38.79,47.59,49.1,27.66,41.03,36.73,48.65,28.12,46.62,35.57,32.98,35,26.07,23.77,39.73,40.57,31.65,31.21,36.32,20.45,21.93,26.02,27.34,23.49,46.94,30.5,33.8,24.23,21.4,27.94,32.24,40.57,25.07,19.42,22.39,18.42,46.96,23.72,26.41,26.97,36.76,40.32,35.02,29.47,30.2,31,38.11,38.18,36.31,21.03,30.86,36.07,28.66,29.08,37.28,15.28,24.17,22.31,30.17,25.53,19.85,35.37,44.6,17.23,13.47,26.33,35.02,32.09,24.81,19.33,28.77,24.26,31.98,25.73,24.86,16.28,34.51,15.23,39.72,40.8,26.06,35.76,34.76,16.13,44.04,18.03,19.65,32.62,35.59,39.43,14.18,35.24,40.13,41.82,35.45,36.07,43.67,24.61,20.9,21.9,18.79,27.61,27.21,26.61,29.77,20.59,27.53,13.82,33.2,25,33.1,36.65,18.63,14.87,22.2,36.81,25.53,24.62,26.25,18.21,28.08,19.42,29.79,32.8,35.99,28.32,27.79,35.88,29.06,36.28,14.1,36.63,37.49,26.9,18.58,38.48,24.48,18.95,33.55,14.24,29.04,32.51,25.63,22.22,19,32.73,15.16,13.9,27.2,32.01,29.27,33,13.74,20.42,27.32,18.23,35.35,28.48,9.08,24.62,20.12,35.26,19.92,31.02,16.49,12.16,30.7,31.22,34.65,13.13,27.51,33.2,31.57,14.1,33.42,17.44,10.12,24.42,9.82,23.39,30.93,15.03,21.67,31.09,33.29,22.61,26.89,23.48,8.38,27.81,32.35,23.84]\n",
    "\n",
    "outlier = num_friends.index(100) # index of outlier\n",
    "num_friends_good = [x for i, x in enumerate(num_friends)\n",
    "                    if i != outlier]\n",
    "daily_minutes_good = [x for i, x in enumerate(daily_minutes)\n",
    "                      if i != outlier]\n",
    "\n",
    "def estimate_beta(x,y):\n",
    "    beta_initial = [random.random() for x_i in x[0]]\n",
    "    return minimize_stochastic(target_fn=squared_error,\n",
    "                              gradient_fn=squared_error_gradient,\n",
    "                              x=x,\n",
    "                              y=y,\n",
    "                              theta_0=beta_initial,\n",
    "                              alpha_0=.001)\n",
    "\n",
    "random.seed(0)\n",
    "betas = estimate_beta(x,daily_minutes_good)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our model looks like: \n",
    "\n",
    "* **`minutes = 30.61988 + .9702*friends - 1.8672*workHours + .91637*phd`**\n",
    "\n",
    "### Interpreting the Model\n",
    "\n",
    "Coefficients represent **all-else-being-equal estimates of the impacts of each factor**. All else being equal: \n",
    "\n",
    "* each additional friend corresponds to 1 extra min spent on the site each day. \n",
    "* each additional hour in a user’s workday corresponds to about 2 fewer min spent on the site each day. \n",
    "* having a PhD is associated with spending 1 extra minute on the site each day.\n",
    "\n",
    "What this doesn’t (*directly*) tell us is anything about the *interactions among the variables.* It’s possible that the effect of work hours is different for people w/ many friends than it for people w/ few friends. **This model doesn’t capture that**. \n",
    "\n",
    "1 way to handle this case is to introduce **interaction terms**, such as the product of “friends” and “work hours\", which effectively allows the “work hours” coefficient to increas/decrease as # of friends increases. \n",
    "\n",
    "Or it’s possible that the more friends you have, the more time you spend on the site up to a point, after which further friends cause you to spend *less* time on the site. (Perhaps w/ too many friends the experience is just too overwhelming) We could try to capture this in our model by adding another variable that’s the square of the number of friends. \n",
    "\n",
    "Once we start adding variables, we need to worry about whether their coefficients “matter.” There are *NO* limits to the #'s of products, logs, squares, and higher powers we could add.\n",
    "\n",
    "### Goodness of Fit\n",
    "\n",
    "Again we can look at the R-squared, which has now increased to 0.68:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800074955952597"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division # error for mean() without\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def tss(y):\n",
    "    \"\"\"Total Sum of Squares variation of y_i's from the mean\"\"\"\n",
    "    return sum(v**2 for v in de_mean(y))\n",
    "\n",
    "def multiple_r2(x,y,beta):\n",
    "    sse = sum(error(x_i,y_i,beta)**2\n",
    "             for x_i,y_i in zip(x,y))\n",
    "    return 1 - sse/tss(y)\n",
    "\n",
    "multiple_r2(x,daily_minutes_good,betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, however, that **adding new variables to a regression will *always* increase R2** b/c the simple regression model = just the special case of the multiple regression model where coefficients on “work hours” and “PhD” both = 0. The optimal multiple regression model has an error at least as small as that one.\n",
    "\n",
    "B/c of this, in a multiple regression, we *also* need to look @ standard errors of the\n",
    "coefficients, which measure how certain we are about our estimates of each . The regression *as a whole* may fit our data very well, but if some independent variables\n",
    "are correlated (or irrelevant), their coefficients might not mean much.\n",
    "\n",
    "Typical approach to measuring these errors starts w/ another **assumption = the errors are independent, normal random variables w/ mean 0 + some shared (unknown) SD**. In that case, use some linear algebra to find the standard error of each coefficient. The larger it is, the less sure our model is about that coefficient. Unfortunately, we’re not set up to do that kind of linear algebra from scratch\n",
    "\n",
    "### Digression: The Bootstrap\n",
    "\n",
    "Imagine we have a sample of `n` DPs, generated by some (unknown to us) distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [random.random() for _ in range(0,1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch5 = function to compute the median of observed data, which we *can use as an estimate of the median of the distribution itself.* But *how confident* can we be about our estimate? If all data in the sample are very close to 100, it seems likely that the actual median is close to 100. If approximately 1/2 the data in the sample = close to 0 + other 1/2 is close to 200, we can’t be nearly as certain about the median.\n",
    "\n",
    "If we could repeatedly get new samples, we could compute the median of each + look at\n",
    "the distribution of those medians. Usually we can’t. What we can do instead is **bootstrap** new data sets by choosing `n` DPs **with replacement** from our data + then compute the medians of those synthetic data sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_sample(data):\n",
    "    \"\"\"Randomly samples len(data) data points, with replacement\"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "def bootstrap_statistic(data,stats_fn,n):\n",
    "    \"\"\"Evaluates a stats function on n bootstrap samples from given data\"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, consider the two following data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 101 DPs close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(101)]\n",
    "\n",
    "# 101 DPs, 50 close to 0, 50 close to 200\n",
    "far_from_100 = ([99.5 + random.random()] +\n",
    "                [random.random() for _ in range(50)] +\n",
    "                [200 + random.random() for _ in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compute the median of each, both will be very close to 100. However, if you look\n",
    "at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.12876259370469, 100.01686151713602, 100.02640176878856, 100.090230931417, 100.1465387006212, 100.03025806473246, 100.03025806473246, 100.02640176878856, 100.07155165592334, 100.02640176878856, 100.02640176878856, 99.96302001178988, 99.96302001178988, 100.05185266660301, 100.0701283454352, 100.1311447607967, 100.03178355998205, 99.95784995980286, 99.9500684326348, 100.05185266660301, 100.0701283454352, 100.01686151713602, 100.02640176878856, 99.95955860262485, 100.06860290118087, 100.01686151713602, 100.02640176878856, 100.12876259370469, 100.090230931417, 100.02640176878856, 100.01686151713602, 99.95955860262485, 100.05185266660301, 100.01686151713602, 100.12876259370469, 100.02640176878856, 100.05185266660301, 100.05185266660301, 100.03025806473246, 100.03025806473246, 99.95784995980286, 100.03178355998205, 100.03025806473246, 100.04068269986026, 100.04068269986026, 100.01686151713602, 99.95784995980286, 100.01686151713602, 99.95955860262485, 100.04068269986026, 100.06860290118087, 99.96302001178988, 100.02640176878856, 100.06183202848534, 100.04068269986026, 100.05185266660301, 100.03025806473246, 99.95784995980286, 100.04068269986026, 100.12876259370469, 100.0701283454352, 99.96302001178988, 99.99570206317709, 100.03025806473246, 100.090230931417, 100.03178355998205, 100.06183202848534, 100.01686151713602, 99.95955860262485, 99.96302001178988, 99.95784995980286, 100.05185266660301, 100.06860290118087, 99.95784995980286, 100.06860290118087, 100.06860290118087, 99.95784995980286, 99.99570206317709, 100.13088144582171, 99.95955860262485, 100.03178355998205, 99.96302001178988, 100.02640176878856, 100.06860290118087, 100.05185266660301, 100.0701283454352, 99.98475211522766, 99.99570206317709, 100.0701283454352, 100.03025806473246, 100.04068269986026, 99.95784995980286, 99.95955860262485, 100.0701283454352, 100.05185266660301, 100.06860290118087, 99.98475211522766, 100.05185266660301, 100.03178355998205, 100.13088144582171]\n",
      "\n",
      "\n",
      "[200.03294297541487, 200.12394600008471, 200.1752101349865, 200.03294297541487, 100.10148118173831, 0.821233331372386, 0.9325453197274712, 100.10148118173831, 0.9302989135541425, 100.10148118173831, 0.9558797964332076, 200.03294297541487, 0.9095171842478343, 200.03294297541487, 0.9627755497247148, 0.8805129995312786, 0.8449041014803238, 0.8449041014803238, 200.07545305463995, 0.9558797964332076, 200.16564031069535, 200.07545305463995, 200.09973321186393, 0.9302989135541425, 200.07545305463995, 200.03294297541487, 200.00149975566973, 200.00149975566973, 0.8486850147194479, 200.07545305463995, 0.9095171842478343, 100.10148118173831, 0.9302989135541425, 0.9095171842478343, 200.00149975566973, 0.9558797964332076, 200.00149975566973, 0.8486850147194479, 0.6963198059234846, 200.1145307508955, 0.9627755497247148, 200.16496209524507, 200.18765986870704, 0.9627755497247148, 0.8805129995312786, 200.06700784368127, 200.09973321186393, 200.07545305463995, 0.9627755497247148, 200.03294297541487, 0.9302989135541425, 200.00149975566973, 200.1145307508955, 200.2220277459516, 200.07545305463995, 0.9627755497247148, 0.9558797964332076, 0.8805129995312786, 0.9558797964332076, 0.9302989135541425, 0.9325453197274712, 0.9558797964332076, 200.16496209524507, 200.09973321186393, 0.9627755497247148, 200.06700784368127, 200.03294297541487, 200.07545305463995, 200.07545305463995, 0.8486850147194479, 0.9558797964332076, 0.9325453197274712, 200.09973321186393, 200.09973321186393, 100.10148118173831, 200.06700784368127, 0.9558797964332076, 0.8805129995312786, 0.8486850147194479, 0.7021559661482255, 0.9627755497247148, 200.06700784368127, 0.9302989135541425, 200.12394600008471, 200.03294297541487, 200.16496209524507, 200.06700784368127, 200.1145307508955, 200.12394600008471, 0.8486850147194479, 0.9558797964332076, 100.10148118173831, 0.9095171842478343, 200.06700784368127, 200.03294297541487, 0.9325453197274712, 200.09973321186393, 0.9558797964332076, 200.07545305463995, 200.12132523831437]\n"
     ]
    }
   ],
   "source": [
    "def median(v):\n",
    "    \"\"\"finds the 'middle-most' value of v\"\"\"\n",
    "    n = len(v)\n",
    "    sorted_v = sorted(v)\n",
    "    midpoint = n // 2\n",
    "\n",
    "    if n % 2 == 1:\n",
    "        # if odd, return the middle value\n",
    "        return sorted_v[midpoint]\n",
    "    else:\n",
    "        # if even, return the average of the middle values\n",
    "        lo = midpoint - 1\n",
    "        hi = midpoint\n",
    "        return (sorted_v[lo] + sorted_v[hi]) / 2\n",
    "\n",
    "print(bootstrap_statistic(close_to_100,median,n=100))\n",
    "print(\"\\n\")\n",
    "print(bootstrap_statistic(far_from_100,median,n=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see #'s really close to 100 + #'s close to 0 and a lot of numbers close to 200.\n",
    "\n",
    "SD of 1st set of medians = close to 0, while SD of 2nd set of medians = close to 100.\n",
    "    \n",
    "* This extreme a case would be pretty easy to figure out by manually inspecting data, but in general that won’t be true\n",
    "\n",
    "### Standard Errors of Regression Coefficients\n",
    "We can take the same approach to estimating the standard errors of our regression\n",
    "coefficients. We repeatedly take a bootstrap_sample of our data and estimate beta based\n",
    "on that sample. If the coefficient corresponding to one of the independent variables (say\n",
    "num_friends) doesn’t vary much across samples, then we can be confident that our\n",
    "estimate is relatively tight. If the coefficient varies greatly across samples, then we can’t\n",
    "be at all confident in our estimate.\n",
    "The only subtlety is that, before sampling, we’ll need to zip our x data and y data to make\n",
    "sure that corresponding values of the independent and dependent variables are sampled\n",
    "together. This means that bootstrap_sample will return a list of pairs (x_i, y_i), which\n",
    "we’ll need to reassemble into an x_sample and a y_sample:\n",
    "def estimate_sample_beta(sample):\n",
    "\"\"\"sample is a list of pairs (x_i, y_i)\"\"\"\n",
    "x_sample, y_sample = zip(*sample) # magic unzipping trick\n",
    "return estimate_beta(x_sample, y_sample)\n",
    "random.seed(0) # so that you get the same results as me\n",
    "bootstrap_betas = bootstrap_statistic(zip(x, daily_minutes_good),\n",
    "estimate_sample_beta,\n",
    "100)\n",
    "After which we can estimate the standard deviation of each coefficient:\n",
    "bootstrap_standard_errors = [\n",
    "standard_deviation([beta[i] for beta in bootstrap_betas])\n",
    "for i in range(4)]\n",
    "# [1.174, # constant term, actual error = 1.19\n",
    "# 0.079, # num_friends, actual error = 0.080\n",
    "# 0.131, # unemployed, actual error = 0.127\n",
    "# 0.990] # phd, actual error = 0.998\n",
    "We can use these to test hypotheses such as “does equal zero?” Under the null\n",
    "hypothesis (and with our other assumptions about the distribution of ) the\n",
    "statistic:\n",
    "which is our estimate of divided by our estimate of its standard error, follows a\n",
    "Student’s t-distribution with “ degrees of freedom.”\n",
    "If we had a students_t_cdf function, we could compute p-values for each least-squares\n",
    "coefficient to indicate how likely we would be to observe such a value if the actual\n",
    "coefficient were zero. Unfortunately, we don’t have such a function. (Although we would\n",
    "if we weren’t working from scratch.)\n",
    "However, as the degrees of freedom get large, the t-distribution gets closer and closer to a\n",
    "standard normal. In a situation like this, where n is much larger than k, we can use\n",
    "normal_cdf and still feel good about ourselves:\n",
    "def p_value(beta_hat_j, sigma_hat_j):\n",
    "if beta_hat_j > 0:\n",
    "# if the coefficient is positive, we need to compute twice the\n",
    "# probability of seeing an even *larger* value\n",
    "return 2 * (1 - normal_cdf(beta_hat_j / sigma_hat_j))\n",
    "else:\n",
    "# otherwise twice the probability of seeing a *smaller* value\n",
    "return 2 * normal_cdf(beta_hat_j / sigma_hat_j)\n",
    "p_value(30.63, 1.174) # ~0 (constant term)\n",
    "p_value(0.972, 0.079) # ~0 (num_friends)\n",
    "p_value(-1.868, 0.131) # ~0 (work_hours)\n",
    "p_value(0.911, 0.990) # 0.36 (phd)\n",
    "(In a situation not like this, we would probably be using statistical software that knows\n",
    "how to compute the t-distribution, as well as how to compute the exact standard errors.)\n",
    "While most of the coefficients have very small p-values (suggesting that they are indeed\n",
    "nonzero), the coefficient for “PhD” is not “significantly” different from zero, which makes\n",
    "it likely that the coefficient for “PhD” is random rather than meaningful.\n",
    "In more elaborate regression scenarios, you sometimes want to test more elaborate\n",
    "hypotheses about the data, such as “at least one of the is non-zero” or “ equals and\n",
    "equals ,” which you can do with an F-test, which, alas, falls outside the scope of this\n",
    "book.\n",
    "Regularization\n",
    "In practice, you’d often like to apply linear regression to data sets with large numbers of\n",
    "variables. This creates a couple of extra wrinkles. First, the more variables you use, the\n",
    "more likely you are to overfit your model to the training set. And second, the more\n",
    "nonzero coefficients you have, the harder it is to make sense of them. If the goal is to\n",
    "explain some phenomenon, a sparse model with three factors might be more useful than a\n",
    "slightly better model with hundreds.\n",
    "Regularization is an approach in which we add to the error term a penalty that gets larger\n",
    "as beta gets larger. We then minimize the combined error and penalty. The more\n",
    "importance we place on the penalty term, the more we discourage large coefficients.\n",
    "For example, in ridge regression, we add a penalty proportional to the sum of the squares\n",
    "of the beta_i. (Except that typically we don’t penalize beta_0, the constant term.)\n",
    "# alpha is a *hyperparameter* controlling how harsh the penalty is\n",
    "# sometimes it's called \"lambda\" but that already means something in Python\n",
    "def ridge_penalty(beta, alpha):\n",
    "return alpha * dot(beta[1:], beta[1:])\n",
    "def squared_error_ridge(x_i, y_i, beta, alpha):\n",
    "\"\"\"estimate error plus ridge penalty on beta\"\"\"\n",
    "return error(x_i, y_i, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "which you can then plug into gradient descent in the usual way:\n",
    "def ridge_penalty_gradient(beta, alpha):\n",
    "\"\"\"gradient of just the ridge penalty\"\"\"\n",
    "return [0] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "def squared_error_ridge_gradient(x_i, y_i, beta, alpha):\n",
    "\"\"\"the gradient corresponding to the ith squared error term\n",
    "including the ridge penalty\"\"\"\n",
    "return vector_add(squared_error_gradient(x_i, y_i, beta),\n",
    "ridge_penalty_gradient(beta, alpha))\n",
    "def estimate_beta_ridge(x, y, alpha):\n",
    "\"\"\"use gradient descent to fit a ridge regression\n",
    "with penalty alpha\"\"\"\n",
    "beta_initial = [random.random() for x_i in x[0]]\n",
    "return minimize_stochastic(partial(squared_error_ridge, alpha=alpha),\n",
    "partial(squared_error_ridge_gradient,\n",
    "alpha=alpha),\n",
    "x, y,\n",
    "beta_initial,\n",
    "0.001)\n",
    "With alpha set to zero, there’s no penalty at all and we get the same results as before:\n",
    "random.seed(0)\n",
    "beta_0 = estimate_beta_ridge(x, daily_minutes_good, alpha=0.0)\n",
    "# [30.6, 0.97, -1.87, 0.91]\n",
    "dot(beta_0[1:], beta_0[1:]) # 5.26\n",
    "multiple_r_squared(x, daily_minutes_good, beta_0) # 0.680\n",
    "As we increase alpha, the goodness of fit gets worse, but the size of beta gets smaller:\n",
    "beta_0_01 = estimate_beta_ridge(x, daily_minutes_good, alpha=0.01)\n",
    "# [30.6, 0.97, -1.86, 0.89]\n",
    "dot(beta_0_01[1:], beta_0_01[1:]) # 5.19\n",
    "multiple_r_squared(x, daily_minutes_good, beta_0_01) # 0.680\n",
    "beta_0_1 = estimate_beta_ridge(x, daily_minutes_good, alpha=0.1)\n",
    "# [30.8, 0.95, -1.84, 0.54]\n",
    "dot(beta_0_1[1:], beta_0_1[1:]) # 4.60\n",
    "multiple_r_squared(x, daily_minutes_good, beta_0_1) # 0.680\n",
    "beta_1 = estimate_beta_ridge(x, daily_minutes_good, alpha=1)\n",
    "# [30.7, 0.90, -1.69, 0.085]\n",
    "dot(beta_1[1:], beta_1[1:]) # 3.69\n",
    "multiple_r_squared(x, daily_minutes_good, beta_1) # 0.676\n",
    "beta_10 = estimate_beta_ridge(x, daily_minutes_good, alpha=10)\n",
    "# [28.3, 0.72, -0.91, -0.017]\n",
    "dot(beta_10[1:], beta_10[1:]) # 1.36\n",
    "multiple_r_squared(x, daily_minutes_good, beta_10) # 0.573\n",
    "In particular, the coefficient on “PhD” vanishes as we increase the penalty, which accords\n",
    "with our previous result that it wasn’t significantly different from zero.\n",
    "NOTE\n",
    "Usually you’d want to rescale your data before using this approach. After all, if you changed years of\n",
    "experience to centuries of experience, its least squares coefficient would increase by a factor of 100 and\n",
    "suddenly get penalized much more, even though it’s the same model.\n",
    "Another approach is lasso regression, which uses the penalty:\n",
    "def lasso_penalty(beta, alpha):\n",
    "return alpha * sum(abs(beta_i) for beta_i in beta[1:])\n",
    "Whereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to force\n",
    "coefficients to be zero, which makes it good for learning sparse models. Unfortunately, it’s\n",
    "not amenable to gradient descent, which means that we won’t be able to solve it from\n",
    "scratch.\n",
    "For Further Exploration\n",
    "Regression has a rich and expansive theory behind it. This is another place where you\n",
    "should consider reading a textbook or at least a lot of Wikipedia articles.\n",
    "scikit-learn has a linear_model module that provides a LinearRegression model\n",
    "similar to ours, as well as Ridge regression, Lasso regression, and other types of\n",
    "regularization too.\n",
    "Statsmodels is another Python module that contains (among other things) linear\n",
    "regression models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
