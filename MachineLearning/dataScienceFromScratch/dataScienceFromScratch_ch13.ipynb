{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 13: Naive Bayes\n",
    "\n",
    "A social network isn’t much good if people can’t network. We have a popular feature that allows members to send messages to other members. And while most members are responsible, a few miscreants persistently spam others, who have begun to complain, so VP of Messaging\n",
    "has asked you to figure out a way to filter out these spam messages\n",
    "\n",
    "### A Really Dumb Spam Filter\n",
    "\n",
    "Imagine a “universe” that consists of receiving a message chosen randomly from all possible messages. Let `S` = event “the message is spam”, `V` = event “the message contains the word 'viagra'.” \n",
    "\n",
    "Then Bayes’s Theorem tells us that the probability the message = spam conditional on containing the word 'viagra' is: `P(S|V) = [P(V|S)P(S)] / [P(V|S)P(S) + P(V|Sc)P(Sc)]`\n",
    "\n",
    "Numerator = probability a message = spam *and* contains viagra, while denominator = probability a message contains 'viagra'. Hence you can think of this calculation as simply representing the proportion of 'viagra' messages that *are* spam.\n",
    "\n",
    "If we have a large collection of messages we *know* = spam, + a large collection of\n",
    "messages we *know* are *not* spam, we can easily estimate `P(V|Sc)` and `P(V|Sc)`. If we further assume any message is *equally likely* to be spam or not (so that `P(S) = P(Sc) = .5`), then: `P(S|V) = P(V|S) / [P(V|S) + P(V|Sc)]`\n",
    "\n",
    "For example, if 50% of spam messages have the word 'viagra', but only 1% of nonspam\n",
    "messages do, the probability any given viagra-containing email is spam is `.5/(.5+.01) = .98`= 98%.\n",
    "\n",
    "### A More Sophisticated Spam Filter\n",
    "\n",
    "Imagine now we have a vocab of *many* words, `{w1, ..., wn}`. To move this into the realm of probability theory, write `Xi` = event “a message contains word `w_i`.\" Also imagine that (through some unspecified-at-this-point process) we’ve come up w/ an estimate `P(Xi|S)` for probability a spam message contains the ith word, + a similar estimate `P(Xi|Sc)` for probability a *non*spam message contains the ith word.\n",
    "\n",
    "**Key to Naive Bayes** = making the (**big**) assumption that the presences (or absences) of each word = independent of one another, conditional on a message being spam or not. Intuitively, this assumption means that knowing whether a certain spam message contains\n",
    "the word “viagra” gives you NO info about if that same message contains the word “rolex.” In math terms, this means that: `P(X1 = x_1, ..., Xn = x_n | S] = P(X1 = x_1 | S) * ... * P(Xn = x_n | S)`\n",
    "\n",
    "This = an *extreme* assumption. (hence “naive”). Imagine our vocab consists only of the words “viagra” and “rolex,” + that 1/2 of all spam messages are for “cheap viagra” + the other 1/2 are for “authentic rolex.” In this case, the **Naive Bayes estimate** that a spam message contains *both* “viagra” and “rolex” is: `P(X1 = x_1, X2 = x_2 | S] = P(X1 = x_1 | S) * P(X2 = x_2 | S) = .5*.5=.25` = 25%, since we’ve assumed away the knowledge that “viagra” and “rolex” actually never occur together (unrealistic) \n",
    "\n",
    "Despite the unrealisticness of this assumption, this model often performs well + is used in actual spam filters. The same Bayes’s Theorem reasoning we used for our “viagra-only” spam filter tells us we can calculate probability a message is spam using the equation: `P(S|X=x) = P(X=x|S) / [P(X=x|S) + P(X=x|Sc)]`\n",
    "\n",
    "**The Naive Bayes assumption** allows us to compute each probability on the RHS by multiplying together the *individual* probability estimates for each vocab word. In practice = usually want to avoid multiplying lots of probabilities together, to avoid a\n",
    "problem = **underflow** = CPU's don’t deal well w/ floating-point #'s that're too close to 0. Recalling from algebra that `log(ab) = log(a) + log(b)` and that `exp(log(x)) = x`, we usually compute `{p_1 * ... * p_n}` as the equivalent (but floating-point-friendlier):\n",
    "`exp(log(p_1) + ... + log(p_n))`. \n",
    "\n",
    "The only challenge left is coming up w/ estimates for `P(X_i|S)` and `P(X_i|Sc)`, the\n",
    "probabilities a spam or nonspam message contains the word `w_i`. If we have a fair number of “training” messages labeled as spam + not-spam, an obvious 1st try = estimate simply as the fraction of spam messages containing word `w_i`.\n",
    "\n",
    "This causes a big problem == Imagine that in our training set, the vocab word “data” only occurs in nonspam messages. Then we’d estimate `P(\"data\"|S) = `. The result is that our Naive Bayes classifier would *always* assign spam probability 0 to any message containing the word “data,” *even a message like “data on cheap viagra + authentic rolex watches.”* \n",
    "\n",
    "To avoid this problem, we usually use some kind of **smoothing**. In particular, choose a **pseudocount** — `k` — + estimate probability of seeing the ith word in a spam as: `P(X_i|S) = (k + # of spams containing w_i)/(2k + # of spams)`, w/ something similar for `P(X_i|Sc)`. That is, **when computing spam probabilities for the ith word, assume we also saw `k` additional spams containing the word + `k` additional spams not containing the word.**\n",
    "\n",
    "Ex: If “data” occurs in 0/98 spam documents, + if `k` = 1, estimate 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word “data.”\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Now we have all the pieces we need to build our classifier. 1st, create a simple function to **tokenize** messages into distinct words by 1st converting each message to lowercase, use `re.findall()` to extract “words” consisting of letters, numbers, + apostrophes, + finally use `set()` to get just distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    import re\n",
    "    message = message.lower()\n",
    "    all_words = re.findall(\"[a-z0-9]+\",message) # find all words\n",
    "    return set(all_words) # get just unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd function = count the words in a labeled training set of messages + return a `dictionary` whose keys = words + values = 2-element lists `[spam_count, non_spam_count]`, corresponding to how many times we saw that word in both spam + nonspam messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(train):\n",
    "    \"\"\"Training set consits of pairs (message, is_spam)\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    counts = defaultdict(lambda: [0,0])\n",
    "    \n",
    "    for message, is_spam in train:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next = turn counts into estimated probabilities using the smoothing described before. The function returns a list of triplets containing each word, probability of seeing that word in a spam message, + probability of seeing that word in a nonspam message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probs(counts,total_spams,total_nonspams,k=.5):\n",
    "    \"\"\"Turn the word_counts into a list of triplets = [w,p(w|spam),p(w|nonspam)]\"\"\"\n",
    "    return [(w,\n",
    "           # smoothing\n",
    "           (spam + k)/(total_spams + 2*k),\n",
    "           (non_spam + k) / (total_nonspams + 2*k))\n",
    "           for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last piece = to use these word probabilities + our Naive Bayes assumptions to assign probabilities to messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spam_prob(word_probs,message):\n",
    "    import math\n",
    "    \n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0\n",
    "    \n",
    "    # iterate through each word in vocabulary\n",
    "    for word, prob_if_spam, prob_if_nonspam in word_probs:\n",
    "        #if 'word' appears in message, add log prob of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_nonspam)\n",
    "            \n",
    "        # if 'word' doesn't appear in message, add log prob of NOT \n",
    "        # seeing it == log(1-prob of seeing it)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1 - prob_if_nonspam)\n",
    "    \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_nonspam = math.exp(log_prob_if_not_spam)\n",
    "    \n",
    "    return prob_if_spam/(prob_if_spam+prob_if_nonspam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can put this all together into our Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassify:\n",
    "    \n",
    "    def __init__(self,k=.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self,training):\n",
    "        # count spam + nonspam messages\n",
    "        num_spams = len([is_spam for message, is_spam in training if is_spam])\n",
    "        num_nonspams = len(training)- num_spams\n",
    "        \n",
    "        # run training  data through pipeline\n",
    "        word_counts = count_words(training)\n",
    "        self.word_probs = word_probs(word_counts, num_spams, num_nonspams, self.k)\n",
    "        \n",
    "    def classify(self, message):\n",
    "        return spam_prob(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Model\n",
    "\n",
    "A good (if somewhat old) data set = SpamAssassin public corpus. We’ll look @ files prefixed w/ `20021010`. (On Windows, might need a program like 7-Zip to decompress + extract them.)\n",
    "\n",
    "After extracting the data, we should have 3 folders: `spam`, `easy_ham`, `hard_ham`. Each folder contains many emails, each contained in a single file. To keep things really simple, we’ll just look @ subject lines of each email. How do we ID the subject line? Looking through the files, they all seem to start w/ `“Subject:”`. So we’ll look for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 3082: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cc97a9508ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[1;31m# open each file, search through lines until we find subject line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Subject:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[1;31m# remove the leading \"Subject: \" and keep what's left\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\NEWNSS\\AppData\\Local\\Continuum\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 3082: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "\n",
    "# modify the path with wherever you've put the files\n",
    "path = r\"C:\\spam\\*\\*\"\n",
    "data = []\n",
    "\n",
    "## glob.glob returns every filename that matches the wildcarded path\n",
    "# for each file in our path (folder --> subfolder --> files)\n",
    "for filename in glob.glob(path):\n",
    "    # if the word 'harm' is not in the file name, then we have spam\n",
    "    is_spam = \"ham\" not in filename\n",
    "    \n",
    "    # open each file, search through lines until we find subject line\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                # remove the leading \"Subject: \" and keep what's left\n",
    "                subject = re.sub(r\"^Subject: \", \"\", line).strip()\n",
    "                data.append((subject, is_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data into training + test data + then build a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0) # just so you get the same answers as me\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "\n",
    "train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "# create instance of NaiveBayesClassify class\n",
    "classifier = NaiveBayesClassify()\n",
    "\n",
    "# use NaiveBayesClassify .train() method\n",
    "classifier.train(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can check how our model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 737, (True, False): 25, (True, True): 4})\n"
     ]
    }
   ],
   "source": [
    "# triplets = (subject, actual is_spam, predicted spam probability)\n",
    "classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "from collections import Counter\n",
    "# assume spam_probability > 0.5 corresponds to spam prediction\n",
    "# + count the combinations of (actual is_spam, predicted is_spam)\n",
    "counts = Counter((is_spam, spam_probability > 0.5)\n",
    "                 for _, is_spam, spam_probability in classified)\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives 101 true positives (spam classified as “spam”), 33 false positives (ham\n",
    "classified as “spam”), 704 true negatives (ham classified as “ham”), and 38 false negatives\n",
    "(spam classified as “ham”). This means our precision is 101 / (101 + 33) = 75%, and our\n",
    "recall is 101 / (101 + 38) = 73%, which are not bad numbers for such a simple model.\n",
    "It’s also interesting to look at the most misclassified:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
