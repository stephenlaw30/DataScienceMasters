{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8. Gradient Descent\n",
    "\n",
    "Frequently when doing data science, we’ll be trying to the find the best model for a certain situation. And usually “best” means **“minimizes the error of the model”** or **“maximizes the likelihood of the data.”** In other words, it will represent the solution to some sort of optimization problem.\n",
    "\n",
    "This means we’ll need to solve a number of optimization problems. In particular,\n",
    "we’ll need to solve them from scratch, + our approach = **gradient descent**, which lends itself pretty well to a from-scratch treatment.\n",
    "\n",
    "### The Idea Behind Gradient Descent\n",
    "\n",
    "Suppose we have some function `f` that takes as input a vector of real numbers and outputs a single real number. One simple such function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(vector):\n",
    "    \"\"\"Compute sum of squared elements in given vector\"\"\"\n",
    "    return sum(element**2 for element in vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll frequently need to maximize (or minimize) such functions, i.e. find the input `vector` that produces the largest (or smallest) possible value. For functions like ours, the **gradient** (vector of partial derivatives) gives the input a direction in which the function most quickly increases.\n",
    "\n",
    "Accordingly, one approach to maximizing a function is to pick a random starting point,\n",
    "compute the gradient, take a small step in the direction of the gradient (direction that causes the function to increase the most), and repeat with the new starting point.\n",
    "\n",
    "Similarly, you can try to minimize a function by taking small steps in the opposite\n",
    "direction (negative gradient)\n",
    "\n",
    "* ***NOTE***: If a function has a unique **global minimum**, this procedure is likely to find it. If a function has **local (i.e. multiple) minima**, this procedure might “find” the wrong one of *them*, in which case you might re-run the procedure from a variety of starting points. **If a function has no minimum, then it’s possible the procedure might go on forever.**\n",
    "\n",
    "## Estimating the Gradient\n",
    "\n",
    "If `f` is a function of 1 variable, its **derivative** at a point `x` measures how f(x) changes when we make a very small change to x (**rate of change**). It is defined as the limit of the difference quotients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivative = slope of the tangent line at `(x, f(x))`**, while the **difference quotient = slope of the not-quite-tangent line that runs through `(x+h, f(x+h))`**. As h gets smaller and smaller, the not-quite-tangent line gets closer and closer to the tangent line.For many functions it’s easy to exactly calculate derivatives. For example, the `square`\n",
    "function's derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def d_square(x):\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can check, if so inclined, by explicitly computing the difference quotient and taking the limit.\n",
    "\n",
    "What if you couldn’t (or didn’t want to) find the gradient? Although we can’t take limits in Python, we can *estimate derivatives by evaluating the difference quotient for a very small `e`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## parital() = returns new partial object which when called behaves like\n",
    "##   func called w/ the positional arguments `args` and keyword \n",
    "##   arguments `keywords`. \n",
    "## If more arguments are supplied, they are appended to args. \n",
    "## If additional keyword arguments are supplied, they extend + override \n",
    "##    keywords\n",
    "from functools import partial\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h = .00001)\n",
    "#derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb2aWEU74ZmYZ4YRvZpYR/wWKiOtnalmPXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f4af8ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot derivative + its estimate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(-10,10)\n",
    "plt.title(\"Actual Derivative vs. Estimates\")\n",
    "# convert map result to list as matplotlib doesn't take generators as input\n",
    "plt.plot(x, list(map(d_square, x)), 'rx', label=\"Actual Dx\")\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+', label=\"Estimate Dx\")\n",
    "plt.legend(loc=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `f` is a function of many variables, it has *multiple* partial derivatives, each indicating how f changes when we make small changes w/ respect to just 1 of the input variables.\n",
    "We calculate ith partial derivatives by treating them  as a function of just the ith variable, holding the other variables fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f,v,i,h):\n",
    "    # add h to only the ith element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "        for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(x) - f(v)) / h\n",
    "\n",
    "# estimate gradient in same manner\n",
    "def estimate_gradient(f,v,h=.00001):\n",
    "    return [partial_difference_quotient(f,v,i,h)\n",
    "           for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE***: Major drawback to this “estimate using difference quotients” approach = computationally expensive. If `v` has length `n`, `estimate_gradient` has to evaluate `f` on `2n` different inputs. If repeatedly estimating gradients, you’re doing a whole lot of extra work\n",
    "\n",
    "### Using the Gradient\n",
    "\n",
    "Easy to see that `sum_of_squares()` is smallest when input `v` = a vector of 0's. But imagine we didn’t know that. Let’s use gradients to find the minimum among all 3D vectors. Just pick a random starting point + then take tiny steps in the *opposite* direction of the gradient until we reach a point where the gradient is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.9277120618321604e-08, -9.759040206107214e-08]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "def step(v,direction,step_size):\n",
    "    \"\"\"Move step_size units in `direction` from `v`\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v,direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2*v_i for v_i in v]\n",
    "\n",
    "def dot(v,w):\n",
    "    return sum(v_i*w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    return dot(v,v)\n",
    "\n",
    "def distance(v,w):\n",
    "    return sqrt(sum_of_squares(v))\n",
    "\n",
    "# pick random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance=.0000001\n",
    "\n",
    "while True:\n",
    "    # compute gradient @ v\n",
    "    gradient = sum_of_squares_gradient(v)\n",
    "    # take negative gradient step (towards min)\n",
    "    next_v = step(v,gradient,-.01)\n",
    "    # stop if converging, continue if not\n",
    "    if distance(next_v,v) < tolerance:\n",
    "        break\n",
    "    v = next_v\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will always ends up with a `v` very close to [0,0,0]. The smaller the `tolerance`, the closer it will get\n",
    "\n",
    "### Choosing the Right Step Size\n",
    "\n",
    "Although the rationale for moving against the gradient is clear, how far to move is not. Choosing correct step size = more of an art than a science. Popular options include:\n",
    "\n",
    "* Using a fixed step size\n",
    "* Gradually shrinking step size over time\n",
    "* At each step, choosing step size that minimizes the value of the **objective function** (sounds optimal but is, in practice, a costly computation). \n",
    "\n",
    "We can approximate it the 3rd option by trying a variety of step sizes + choosing the one that results in the smallest value of the objective function. \n",
    "\n",
    "It is possible that certain step sizes will result in invalid inputs for our function, so we’ll create a `“safe apply”` function that returns infinity (which should never be the minimum of anything) for invalid inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_sizes = [100,10,1,.1,.01,.001,.0001,.00001]\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"Return a new function that's the same as f except\n",
    "    it outputs Infinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args,**kwargs):\n",
    "        try:\n",
    "            return f(*args,**kwargs)\n",
    "        except:\n",
    "            return float('inf') # = infinity in Python\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "In the general case, we have some `target_fn` we want to minimize, and we also have its `gradient_fn`. For example, `target_fn` could represent the errors in a model as a function of its parameters, + we might want to find the parameters that make the errors as small as possible.\n",
    "\n",
    "Furthermore, let’s say we have (somehow) chosen a starting value for the parameters `theta_0`. Then we can implement gradient descent as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_batch(target_f,gradient_f,theta0,tolerance=.00001):\n",
    "    \"\"\"Use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    step_sizes = [100,10,1,.1,.01,.001,.0001,.00001]\n",
    "    \n",
    "    theta=theta0                    # set theta to initial value\n",
    "    target_f = safe(target_f)       # safe version of target function\n",
    "    value = target_f(theta)         # value to minimize\n",
    "    \n",
    "    while True:\n",
    "        gradient=gradient_f(theta)\n",
    "        next_thetas = [step(theta,gradient,-step_size)\n",
    "                      for step_size in step_sizes]\n",
    "        # choose theta that minimizes error function\n",
    "        next_theta = min(next_theta, keys=target_f)\n",
    "        next_value = target_f(next_theta)\n",
    "        # stop if converging\n",
    "        if abs(value-next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta,value = next_theta,next_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called it `minimize_batch` b/c, for each gradient step, it looks at the entire data set (b/c `target_fn` returns the error on the *whole* data set). Later, we’ll see an alternative approach that only looks at 1 DP at a time.\n",
    "\n",
    "Sometimes we’ll instead want to maximize a function, which we can do by minimizing its negative (which has a corresponding negative gradient):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"Return a function that, for any input, x, returns -f(x)\"\"\"\n",
    "    return lambda *args,**kwargs: -f(*args,**kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"Same as above but for when f returns a list of numbers\"\"\"\n",
    "    return lambda *args,**kwargs: [-y for y in f(*args,**kwargs)]\n",
    "\n",
    "def maximize_batch(target_f,gradient_f,theta0,tolerance=.00001):\n",
    "    return minimize_batch(negate(target_f),\n",
    "                         negate_all(gradient_f),\n",
    "                         theta0,\n",
    "                         tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Often we’ll be using gradient descent to choose the parameters of a model in a way that minimizes some notion of error. Using the previous batch approach, each gradient step requires us to make a prediction and compute the gradient for the *whole data set*, which makes each step take a long time.\n",
    "\n",
    "Usually these error functions are **additive** = the predictive error on\n",
    "the whole data set is simply the sum of the predictive errors for each DP. When this is the case, we can instead apply **stochastic gradient descent** = computes the gradient (+ takes a step) for only *one* point at a time. It cycles over data repeatedly until it reaches a stopping point. During each cycle, we’ll want to iterate through our data in a random order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"Generator that returns elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)] # create list of indexes\n",
    "    random.shuffle(indexes)                   # shuffle indices\n",
    "    for i in indexes:                         # return data in that order\n",
    "        yield data[i] #????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to take a gradient step for each DP, + this approach leaves the possibility that we might circle around near a minimum forever, so whenever we stop getting improvements we’ll decrease the step size and eventually quit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def minimize_stochastic(target_f,gradient_f,x,y,theta0,alpha0=.01):\n",
    "    data = zip(x,y)\n",
    "    theta=theta0                     # initial guess\n",
    "    alpha=alpha0                     # initial step size\n",
    "    min_theta,min_value = None, float(\"inf\")\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # If we get to 100 iterations w/ no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_f(x_i,y_i,theta) for x_i,y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # if a new min is found, remember it and \n",
    "            # go back to original step size\n",
    "            min_theta,min_value = theta,value\n",
    "            iterations_with_no_improvement=0\n",
    "            alpha=alpha0\n",
    "        else:\n",
    "            # otherwise, no improvement found, so try shrinking step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= .9\n",
    "        \n",
    "        # take gradient step for each data point\n",
    "        for x_i,y_i in in_random_order(data):\n",
    "            gradient_i = gradient_f(x_i,y_i,theta)\n",
    "            theta=vector_subtract(theta,scalar_multiply(alpha,gradient_i))\n",
    "            \n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic version will typically be a lot faster than the batch version. Of course, we’ll want a version that maximizes as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_f,gradient_f,x,y,theta0,alpha0=.01):\n",
    "    return minimize_stochastic(negate(target_f),\n",
    "                              negate_all(gradient_f),\n",
    "                              x,y,theta0,alpha0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
