{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11: ML\n",
    "\n",
    "DS = mostly turning business problems into data problems, collecting + \n",
    "understanding + cleaning + formatting data, after which ML is almost an afterthought.\n",
    "\n",
    "### Modeling\n",
    "\n",
    "**model** = a specification of a mathematical (or probabilistic) relationship that exists between different variables.\n",
    "\n",
    "* if trying to raise \\$ for a social networking site, might build a business model (likely in a spreadsheet) that takes inputs like “# of users”, “ad revenue per user”, “# of employees” that outputs estimated annual profit for the next several years. \n",
    "* cookbook recipe entails a model that relates inputs like “# of eaters” and “hungriness” to quantities of ingredients needed. \n",
    "* Poker on TV = estimate each player’s “win probability” in real time based on a model that takes into account revealed cards so far + the distribution of cards in the deck\n",
    "\n",
    "* The business model is probably based on simple mathematical relationships: profit = revenue - expenses, revenue = units sold * average price, etc. \n",
    "* Recipe model is probably based on trial + error (tried different combos  of ingredients until found one well-liked) \n",
    "* Poker model is based on **probability theory**, rules of poker, + some reasonably innocuous assumptions about the random process by which cards are dealt.\n",
    "\n",
    "### What Is Machine Learning?\n",
    "ML = creating + using models that are *learned from data* (predictive modeling or data mining) typically w/ goal to use *existing* data to develop models we can use to predict various outcomes for *new* data: \n",
    "\n",
    "* Predicting whether an email message is spam or not\n",
    "* Predicting whether a credit card transaction is fraudulent\n",
    "* Predicting which advertisement a shopper is most likely to click on\n",
    "* Predicting which football team is going to win the Super Bowl\n",
    "\n",
    "**supervised models** = have a set of data labeled w/ correct answers to learn from) vs. **unsupervised models** = no such labels vs. **semisupervised** = only some data are labeled vs. **online** = model needs to continuously adjust to newly arriving data\n",
    "\n",
    "There are entire universes of models that might describe a relationship we’re interested in. In most cases, we choose a **parameterized family of models** + then use data to learn parameters that are, in some way, optimal.\n",
    "\n",
    "* might assume person’s height is (roughly) a linear function of weight + then use data to learn what that linear function is.\n",
    "* might assume a decision tree = good way to diagnose diseases patients have + then use data to learn the “optimal” tree. Throughout\n",
    "\n",
    "### Overfitting and Underfitting\n",
    "Common danger in ML = **overfitting** = producing model that performs\n",
    "well on training data but *generalizes poorly* to any *new* data\n",
    "* could involve **learning noise** in the data or involve **learning to ID specific inputs rather than whatever factors are *actually* predictive for the desired output**\n",
    "\n",
    "Other side = **underfitting** = producing a model that doesn’t perform well even on training data (typically when this happens you decide your model isn’t good enough + keep looking for a better one)\n",
    "\n",
    "Models that are *too complex* lead to overfitting + don’t generalize well beyond training data. To make sure models aren’t too complex, most fundamental approach = **using different data to train a model + to *test*\n",
    "the model**\n",
    "\n",
    "Simplest way = split data set, so that (for example) 2/3 is used to train the model, after which measure model’s performance on the remaining 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(data, prob):\n",
    "    \"\"\"Split given data into fractions (prob)\"\"\"\n",
    "    results = [],[] # 2 sets (lists)\n",
    "    for row in data:\n",
    "        # add into 1st list if below given prob, add to 2nd list if not\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often given a **matrix `x` of input** variables + a **vector `y` of output** variables. In that case, need to make sure to put corresponding values together in either the training or test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(x,y,test_pct):\n",
    "    data = zip(x,y)     # combine given predictor matrix with label vector\n",
    "    train, test = split_data(data, 1 - test_pct) # split into sets\n",
    "    # zip(*... UNPACKS a list such that each element = separate arg\n",
    "    x_train, y_train = zip(*train)\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "## can now do something like\n",
    "#model = someModel()\n",
    "#x_train, y_train, x_test, y_test = train_test_split(xs,ys,.33) # make sets\n",
    "#model.train(x_train,y_train)\n",
    "#performance = model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If model was overfit to training, it will hopefully perform really poorly\n",
    "on test. If performs well on test, you can be more confident it’s fitting rather than overfitting.\n",
    "\n",
    "However, there are a couple of ways this can go wrong.\n",
    "* 1) if there are common patterns in the test *and* train data that wouldn’t generalize to an even larger data set.\n",
    "    * ex: data set consists of user activity, 1 row per user per week. \n",
    "    * In such a case, most users appear in both training + test data, + certain models might learn to ID *users* rather than *discover relationships involving attributes*\n",
    "    * not a huge worry, but can happen\n",
    "* 2) Bigger problem = use test/train split not *just* to *judge* a model but *also to choose from among many models*. \n",
    "    * In that case, although each *individual* model may not be overfit, the “choose model that performs best on test ” is a **meta-training** that makes the test set function as a *second training set*\n",
    "    * *Of course* the model that performed best on test is going to perform well on the test set\n",
    "    * In such a situation, split the data into *THREE* parts: training set for building models, **validation set** for *choosing* among trained models, + test set for *judging* the final model\n",
    "    \n",
    "### Correctness\n",
    "Ex: Cheap, noninvasive test can be given to a newborn + predicts w/ > 98% accuracy whether the newborn will ever develop leukemia. Test predicts leukemia if + only if baby is named \"Luke\". Test is indeed > 98% accurate, but nonetheless, is incredibly stupid, + a good illustration of why we **don’t typically use “accuracy” to measure how good a model is**\n",
    "\n",
    "Imagine building a model to make a binary judgment. Is this email spam? Should we hire a candidate? Is an air traveler secretly a terrorist? Given a set of labeled data + such a predictive model, every DP lies in 1 of\n",
    "4 categories:\n",
    "* **True positive**: “message is spam + we correctly predicted spam.”\n",
    "* **False positive (Type 1 Error)**: “message = *not* spam, but we predicted spam.”\n",
    "* **False negative (Type 2 Error)**: “message = spam, but we predicted not spam.”\n",
    "* **True negative**: “message = *not* spam, + we correctly predicted not spam.”\n",
    "\n",
    "We often represent these as counts in a **confusion matrix**. See how the leukemia test fits into this framework. These days approximately 5/1000 babies = named Luke, + the lifetime **prevalence** of leukemia = ~1.4%, or 14/1000 people. If we believe these **2 factors are independent** + apply my “Luke is for leukemia” test to 1M people, we’d expect to see a confusion matrix w/ 14k predicted leukimia's, 986k non-leukemias, 5k total Luke's, 995k non-Luke's, 70 Luke+leukimia (TP), 4930 Luke+non-leukimia (FP), 13930 non-Luke+leukemia (FN), 981070 non-Luke+non-leukemia (TN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98114\n"
     ]
    }
   ],
   "source": [
    "def model_acc(tp,fp,fn,tn):\n",
    "    correct = tp+tn\n",
    "    total = tp+tn+fp+fn\n",
    "    return correct/total\n",
    "print(model_acc(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Seems pretty impressive, but is clearly not a good test = means we probably **shouldn’t put a lot of credence in raw accuracy.**\n",
    "\n",
    "It’s common to look at the *combo of **precision and recall**. \n",
    "* **Precision** = how accurate \"positive\" predictions were\n",
    "* **Recall** = what fraction of positives out of all actual positive our model identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.014 \n",
      "recall:  0.005\n"
     ]
    }
   ],
   "source": [
    "def precision(tp,fp,fn,tn):\n",
    "    return tp / (tp+fp)\n",
    "\n",
    "def recall(tp,fp,fn,tn):\n",
    "    return tp / (tp+fn)\n",
    "\n",
    "print(\"precision: \",precision(70,4930,13930,981070),\"\\nrecall: \",\n",
    "     recall(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are terrible, reflecting that this is a terrible model.\n",
    "\n",
    "Sometimes precision + recall are combined into **F1 score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00736842105263158\n"
     ]
    }
   ],
   "source": [
    "def f1(tp,fp,fn,tn):\n",
    "    p = precision(tp,fp,fn,tn) # tp/(tp+fp) = correct pos / all predicted pos\n",
    "    r = recall(tp,fp,fn,tn) # tp/(tp+fn) = correct pos / all real pos\n",
    "    return 2*p*r / (p+r)\n",
    "print(f1(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 = harmonic mean** of precision + recall --> necessarily lies between them\n",
    "\n",
    "Usually choice of a model involves a **trade-off between precision + recall**. \n",
    "* model that predicts “yes” when it’s even a *little* bit confident will probably have high recall but low precision;\n",
    "* model that predicts “yes” only when it’s *extremely* confident is likely to have a low recall and a high precision.\n",
    "\n",
    "Alternatively, think of this as a trade-off between FP vs FN\n",
    "* Saying “yes” too often gives lots of FP's\n",
    "* Saying “no” too often gives lots of FN's\n",
    "\n",
    "Imagine there were 10 risk factors for leukemia, + that more you had = more likely you were to develop leukemia. In that case, imagine a continuum\n",
    "of tests: “predict leukemia if at least 1 risk factor,” “predict leukemia if at least 2 risk factors,” + so on. As you increase the **threshhold**, you increase test’s precision (since people w/ more risk factors = more likely to develop the disease), + decrease test’s recall (fewer + fewer of eventual disease-sufferers will meet threshhold). In cases like this, **choosing the right threshhold is a matter of finding the right trade-off**\n",
    "\n",
    "### The Bias-Variance Trade-off\n",
    "Another way of thinking about overfitting = trade-off between **bias and\n",
    "variance.**\n",
    "\n",
    "Both = measures of what would happen if you were to retrain a model many times on different training sets (*\\*from same larger population*).\n",
    "* Ex: degree-0 polynomial model will make a lot of mistakes for pretty much any training set (\\*drawn from same population), which means that it has a high bias (*to the model*)\n",
    "* However, any 2 randomly chosen training sets should give pretty similar models (since any 2 randomly chosen training sets should have pretty similar average values), so we say it has a low variance. \n",
    "* **High bias, low variance typically = underfitting**\n",
    "\n",
    "On the other hand\n",
    "* degree-9 polynomial model can fit a training set perfectly = very low bias \n",
    "* but very high variance (since any 2 training sets would likely give rise to very different models)\n",
    "* **Low bias, high variance typically = overfitting.**\n",
    "\n",
    "Thinking about model problems this way can help figure out what do when a model doesn’t work so well. If a model has **high bias** (performs poorly even on training data), 1 thing to try = **adding more features.** Going from the degree 0 model to the degree 1 model can  be a big improvement. \n",
    "\n",
    "If a model has **high variance**, can similarly **remove features**. But another solution = to **obtain more data** (if possible).\n",
    "* Ex: Fit a degree 9 polynomial to different size samples. \n",
    "* Model fit based on 10 DP's  = all over the place, as we saw before. \n",
    "* If instead trained on 100 DP's == much less overfitting\n",
    "* Model trained from 1K DP's looks very similar to a degree 1 model.\n",
    "\n",
    "*Holding model complexity constant*, more data = harder it is to overfit. On the other hand, **more data won’t help with bias = if a model doesn’t use enough features to capture regularities in the data, throwing more data at it won’t help**\n",
    "\n",
    "### Feature Extraction and Selection\n",
    "\n",
    "As mentioned, when data doesn’t have enough features, model is likely to underfit + when data has too many features, it’s easy to overfit\n",
    "\n",
    "**Features** = inputs provided to a model + in simplest case, are given to you. \n",
    "* ex: To predict salary based on years of experience, \"years of experience\" is the only feature you have.\n",
    "* Although,might also consider adding years of experience squared, cubed, + so on if it helps build a better model\n",
    "\n",
    "Things become more interesting as data becomes more complicated. Imagine trying to build a spam filter to predict junk or not. Most models won’t know what to do w/ a raw email (collection of text). You’ll have to **extract features**. \n",
    "* Does email contain the word “Viagra”? How many times does the letter d appear? What was the domain of the sender?\n",
    "* The first = simply a yes/no, typically encoded as 1/0. \n",
    "* The second = a number.\n",
    "* The third = a choice from a discrete set of options.\n",
    "\n",
    "Pretty much always, we’ll extract features from data that fall into 1 of these 3 categories. What’s more, the *type* of features we have constrains the type of models we can use.\n",
    "\n",
    "**Naive Bayes classifier** = suited to yes/no features, **Regression models** require numeric features (could include dummy variables = 0s/1s),\n",
    "**decision trees** can deal w/ numeric *or* categorical data.\n",
    "\n",
    "Can *create* features, or sometimes instead look for ways to *remove* features\n",
    "* Ex: inputs might be vectors of several hundred #'s + depending on the situation, might be appropriate to distill these down to handful of important dimensions (Dimensionality Reduction) + use only those small # of features. \n",
    "* Or it might be appropriate to use a technique like **regularization**, that penalizes models the more features they use\n",
    "\n",
    "*How do we choose features*? = combo of experience + domain expertise (receive lots of emails + probably sense the presence of certain words might be a good indicator of spamminess + # of `d`’s is likely not a good indicator of spamminess)\n",
    "\n",
    "But in general you’ll have to try different things, which is part of the fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
