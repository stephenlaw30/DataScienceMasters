{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 18. Neural Networks\n",
    "\n",
    "An **artificial neural network** (ANN or neural network for short) = a predictive model motivated by the way the brain operates. Brain = collection of neurons wired together, each neuron looking @ the outputs of other neurons that feed into it, then performing a calculation, + then either fires (if calculation exceeds some threshhold) or doesn’t (if it doesn’t).\n",
    "\n",
    "Accordingly, ANNs consist of artificial neurons, which perform similar\n",
    "calculations over their inputs. NNs can solve a wide variety of problems like handwriting recognition + face detection, + are used heavily in **deep learning**, a trendy subfields of data science. However, most NNs = “black boxes” — inspecting their details doesn’t give much understanding of *HOW* they’re solving a problem. Large NNs can be difficult to train. **For most problems, they’re probably not the right choice.**\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "Simplest NN = the **perceptron** = approximates a single neuron w/ `n` binary inputs + computes a *weighted sum* of its inputs + “fires” if that weighted sum >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './../../../00_DataScience/DSFromScratch/code')\n",
    "\n",
    "from linear_algebra import dot\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron_output(weights,bias,x):\n",
    "    \"\"\"Returns 1 if perceptron fires, 0 if not\"\"\"\n",
    "    calculation = dot(weights,x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron = simply distinguishing between the half spaces separated by the **hyperplane** of points `x` for which `dot(weights,x) + bias == 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W/ properly chosen weights, perceptrons can solve a # of simple problems\n",
    "\n",
    "* EX: can create an **AND gate** = returns 1 if both inputs = 1 but returns 0 if 1 input = 0) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [2, 2]\n",
    "bias = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If both inputs = 1, `calculation` = 2 + 2 - 3 = 1, so `output = 1`. If only 1 input = 1, `calculation` = 2 + 0 - 3 = -1, so `output = 0`. If *both* inputs = 0, `calculation` = -3, so output = 0. \n",
    "\n",
    "Similarly, we could build an **OR gate with:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [2, 2]\n",
    "bias = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could build a **NOT gate** (1 input + converts 1 to 0 and 0 to 1) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [-2]\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some problems simply can’t be solved by a single perceptron. \n",
    "* Ex: Cannot use a perceptron to build an **XOR gate** = outputs 1 if exactly 1 input = 1 and 0 otherwise. \n",
    "\n",
    "This needs more-complicated NNs. Of course, you don’t need to approximate a neuron in order to build a logic gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x, y: 0 if x == y else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like real neurons, artificial neurons = more interesting when connected together.\n",
    "\n",
    "### Feed-Forward Neural Networks\n",
    "\n",
    "Topology of brain = enormously complicated, so it’s common to *approximate* it w an idealized **feed-forward NN** that consists of discrete **layers** of neurons, each connected to the next, which typically entails an **input layer** (receives inputs + feeds them forward *unchanged*), 1+ **hidden layers** (each consists of neurons that take outputs of previous layer, perform some calculation, + passes\n",
    "result to next layer), + an **output layer** (produces final outputs).\n",
    "\n",
    "Just like a perceptron, each (non-input) neuron has a **weight** corresponding to each of its inputs + a **bias**. To make representation impler, add the bias to the end of the weights vector + give each neuron a bias input that always = 1.\n",
    "\n",
    "As w/ a perceptron, for each neuron --> sum up products of its inputs + its weights. But here, rather than outputting `step_function()` applied to that product, output a **smooth approximation** of a step function = **the sigmoid function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1/(1+mat.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use sigmoid instead of the simpler `step_function()`? = **In order to train a NN, need to use calculus, + to use calculus, need smooth functions**, + the **step function isn’t even continuous, + sigmoid = a good smooth approximation of it.**\n",
    "\n",
    "***NOTE*** sigmoid shape = logistic function\n",
    "\n",
    "After smoothing, calculate the output as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_output(weights,inputs):\n",
    "    return sigmoid(dot(weights,inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this function, can represent a neuron simply as a **`list` of weights w/ length = 1 more than # of inputs it takes (b/c of bias weight)**. Then can represent a NN as a **`list` of (noninput) layers, where each layer = just a `list` of neurons in that layer**\n",
    "\n",
    "* ***NN = `list` (layers) of `lists` (neurons) of `lists` (weights).***\n",
    "\n",
    "Given such a representation, *using* a NN is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(neural_network,input_vector):\n",
    "    \"\"\"Takes in a NN, represented as a list of lists of lists of weights\n",
    "    and returns the output from forward-propogating the input\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    # process 1 layer at a time\n",
    "    for layer in neural_network:\n",
    "        # add bias input\n",
    "        input_with_bias = input_vector + [1]\n",
    "        # computer layer (neuron) output\n",
    "        output = [neuron_output(neuron,input_with_bias)\n",
    "                 for neuron in layer]\n",
    "        # remember the output\n",
    "        outputs.append(output)\n",
    "        \n",
    "        # then send output to next layer as an input\n",
    "        input_vector = output\n",
    "    \n",
    "    # return final outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s easy to build the **XOR gate** we couldn’t build w/ a single perceptron. We just need to scale the weights up so the `neuron_outputs` are either really close to 0 or really close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "xor_network = [# hidden layer\n",
    "[[20, 20, -30], # 'and' neuron\n",
    "[20, 20, -10]], # 'or' neuron\n",
    "# output layer\n",
    "[[-60, 60, -30]]] # '2nd input but not 1st input' neuron\n",
    "for x in [0, 1]:\n",
    "for y in [0, 1]:\n",
    "# feed_forward produces the outputs of every neuron\n",
    "# feed_forward[-1] is the outputs of the output-layer neurons\n",
    "print x, y, feed_forward(xor_network,[x, y])[-1]\n",
    "# 0 0 [9.38314668300676e-14]\n",
    "# 0 1 [0.9999999999999059]\n",
    "# 1 0 [0.9999999999999059]\n",
    "# 1 1 [9.383146683006828e-14]\n",
    "By using a hidden layer, we are able to feed the output of an “and” neuron and the output\n",
    "of an “or” neuron into a “second input but not first input” neuron. The result is a network\n",
    "that performs “or, but not and,” which is precisely XOR (Figure 18-3).\n",
    "Figure 18-3. A neural network for XOR\n",
    "Backpropagation\n",
    "Usually we don’t build neural networks by hand. This is in part because we use them to\n",
    "solve much bigger problems — an image recognition problem might involve hundreds or\n",
    "thousands of neurons. And it’s in part because we usually won’t be able to “reason out”\n",
    "what the neurons should be.\n",
    "Instead (as usual) we use data to train neural networks. One popular approach is an\n",
    "algorithm called backpropagation that has similarities to the gradient descent algorithm\n",
    "we looked at earlier.\n",
    "Imagine we have a training set that consists of input vectors and corresponding target\n",
    "output vectors. For example, in our previous xor_network example, the input vector [1,\n",
    "0] corresponded to the target output [1]. And imagine that our network has some set of\n",
    "weights. We then adjust the weights using the following algorithm:\n",
    "1. Run feed_forward on an input vector to produce the outputs of all the neurons in the\n",
    "network.\n",
    "2. This results in an error for each output neuron — the difference between its output\n",
    "and its target.\n",
    "3. Compute the gradient of this error as a function of the neuron’s weights, and adjust\n",
    "its weights in the direction that most decreases the error.\n",
    "4. “Propagate” these output errors backward to infer errors for the hidden layer.\n",
    "5. Compute the gradients of these errors and adjust the hidden layer’s weights in the\n",
    "same manner.\n",
    "Typically we run this algorithm many times for our entire training set until the network\n",
    "converges:\n",
    "def backpropagate(network, input_vector, targets):\n",
    "hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "# the output * (1 - output) is from the derivative of sigmoid\n",
    "output_deltas = [output * (1 - output) * (output - target)\n",
    "for output, target in zip(outputs, targets)]\n",
    "# adjust weights for output layer, one neuron at a time\n",
    "for i, output_neuron in enumerate(network[-1]):\n",
    "# focus on the ith output layer neuron\n",
    "for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "# adjust the jth weight based on both\n",
    "# this neuron's delta and its jth input\n",
    "output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "# back-propagate errors to hidden layer\n",
    "hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "dot(output_deltas, [n[i] for n in output_layer])\n",
    "for i, hidden_output in enumerate(hidden_outputs)]\n",
    "# adjust weights for hidden layer, one neuron at a time\n",
    "for i, hidden_neuron in enumerate(network[0]):\n",
    "for j, input in enumerate(input_vector + [1]):\n",
    "hidden_neuron[j] -= hidden_deltas[i] * input\n",
    "This is pretty much doing the same thing as if you explicitly wrote the squared error as a\n",
    "function of the weights and used the minimize_stochastic function we built in\n",
    "Chapter 8.\n",
    "In this case, explicitly writing out the gradient function turns out to be kind of a pain. If\n",
    "you know calculus and the chain rule, the mathematical details are relatively\n",
    "straightforward, but keeping the notation straight (“the partial derivative of the error\n",
    "function with respect to the weight that neuron i assigns to the input coming from neuron\n",
    "j”) is not much fun.\n",
    "Example: Defeating a CAPTCHA\n",
    "To make sure that people registering for your site are actually people, the VP of Product\n",
    "Management wants to implement a CAPTCHA as part of the registration process. In\n",
    "particular, he’d like to show users a picture of a digit and require them to input that digit to\n",
    "prove they’re human.\n",
    "He doesn’t believe you that computers can easily solve this problem, so you decide to\n",
    "convince him by creating a program that can easily solve the problem.\n",
    "We’ll represent each digit as a 5 × 5 image:\n",
    "@@@@@ ..@.. @@@@@ @@@@@ @...@ @@@@@ @@@@@ @@@@@ @@@@@ @@@@@\n",
    "@...@ ..@.. ....@ ....@ @...@ @.... @.... ....@ @...@ @...@\n",
    "@...@ ..@.. @@@@@ @@@@@ @@@@@ @@@@@ @@@@@ ....@ @@@@@ @@@@@\n",
    "@...@ ..@.. @.... ....@ ....@ ....@ @...@ ....@ @...@ ....@\n",
    "@@@@@ ..@.. @@@@@ @@@@@ ....@ @@@@@ @@@@@ ....@ @@@@@ @@@@@\n",
    "Our neural network wants an input to be a vector of numbers. So we’ll transform each\n",
    "image to a vector of length 25, whose elements are either 1 (“this pixel is in the image”) or\n",
    "0 (“this pixel is not in the image”).\n",
    "For instance, the zero digit would be represented as:\n",
    "zero_digit = [1,1,1,1,1,\n",
    "1,0,0,0,1,\n",
    "1,0,0,0,1,\n",
    "1,0,0,0,1,\n",
    "1,1,1,1,1]\n",
    "We’ll want our output to indicate which digit the neural network thinks it is, so we’ll need\n",
    "10 outputs. The correct output for digit 4, for instance, will be:\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "Then, assuming our inputs are correctly ordered from 0 to 9, our targets will be:\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "for j in range(10)]\n",
    "so that (for example) targets[4] is the correct output for digit 4.\n",
    "At which point we’re ready to build our neural network:\n",
    "random.seed(0) # to get repeatable results\n",
    "input_size = 25 # each input is a vector of length 25\n",
    "num_hidden = 5 # we'll have 5 neurons in the hidden layer\n",
    "output_size = 10 # we need 10 outputs for each input\n",
    "# each hidden neuron has one weight per input, plus a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "for __ in range(num_hidden)]\n",
    "# each output neuron has one weight per hidden neuron, plus a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "for __ in range(output_size)]\n",
    "# the network starts out with random weights\n",
    "network = [hidden_layer, output_layer]\n",
    "And we can train it using the backpropagation algorithm:\n",
    "# 10,000 iterations seems enough to converge\n",
    "for __ in range(10000):\n",
    "for input_vector, target_vector in zip(inputs, targets):\n",
    "backpropagate(network, input_vector, target_vector)\n",
    "It works well on the training set, obviously:\n",
    "def predict(input):\n",
    "return feed_forward(network, input)[-1]\n",
    "predict(inputs[7])\n",
    "# [0.026, 0.0, 0.0, 0.018, 0.001, 0.0, 0.0, 0.967, 0.0, 0.0]\n",
    "Which indicates that the digit 7 output neuron produces 0.97, while all the other output\n",
    "neurons produce very small numbers.\n",
    "But we can also apply it to differently drawn digits, like my stylized 3:\n",
    "predict([0,1,1,1,0, # .@@@.\n",
    "0,0,0,1,1, # ...@@\n",
    "0,0,1,1,0, # ..@@.\n",
    "0,0,0,1,1, # ...@@\n",
    "0,1,1,1,0]) # .@@@.\n",
    "# [0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.01, 0.0, 0.12]\n",
    "The network still thinks it looks like a 3, whereas my stylized 8 gets votes for being a 5, an\n",
    "8, and a 9:\n",
    "predict([0,1,1,1,0, # .@@@.\n",
    "1,0,0,1,1, # @..@@\n",
    "0,1,1,1,0, # .@@@.\n",
    "1,0,0,1,1, # @..@@\n",
    "0,1,1,1,0]) # .@@@.\n",
    "# [0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.0, 0.0, 0.93, 1.0]\n",
    "Having a larger training set would probably help.\n",
    "Although the network’s operation is not exactly transparent, we can inspect the weights of\n",
    "the hidden layer to get a sense of what they’re recognizing. In particular, we can plot the\n",
    "weights of each neuron as a 5 × 5 grid corresponding to the 5 × 5 inputs.\n",
    "In real life you’d probably want to plot zero weights as white, with larger positive weights\n",
    "more and more (say) green and larger negative weights more and more (say) red.\n",
    "Unfortunately, that’s hard to do in a black-and-white book.\n",
    "Instead, we’ll plot zero weights as white, with far-away-from-zero weights darker and\n",
    "darker. And we’ll use crosshatching to indicate negative weights.\n",
    "To do this we’ll use pyplot.imshow, which we haven’t seen before. With it we can plot\n",
    "images pixel by pixel. Normally this isn’t all that useful for data science, but here it’s a\n",
    "good choice:\n",
    "import matplotlib\n",
    "weights = network[0][0] # first neuron in hidden layer\n",
    "abs_weights = map(abs, weights) # darkness only depends on absolute value\n",
    "grid = [abs_weights[row:(row+5)] # turn the weights into a 5x5 grid\n",
    "for row in range(0,25,5)] # [weights[0:5], ..., weights[20:25]]\n",
    "ax = plt.gca() # to use hatching, we'll need the axis\n",
    "ax.imshow(grid, # here same as plt.imshow\n",
    "cmap=matplotlib.cm.binary, # use white-black color scale\n",
    "interpolation='none') # plot blocks as blocks\n",
    "def patch(x, y, hatch, color):\n",
    "\"\"\"return a matplotlib 'patch' object with the specified\n",
    "location, crosshatch pattern, and color\"\"\"\n",
    "return matplotlib.patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "hatch=hatch, fill=False, color=color)\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5): # row\n",
    "for j in range(5): # column\n",
    "if weights[5*i + j] < 0: # row i, column j = weights[5*i + j]\n",
    "# add black and white hatches, so visible whether dark or light\n",
    "ax.add_patch(patch(j, i, '/', \"white\"))\n",
    "ax.add_patch(patch(j, i, '\\\\', \"black\"))\n",
    "plt.show()\n",
    "Figure 18-4. Weights for the hidden layer\n",
    "In Figure 18-4 we see that the first hidden neuron has large positive weights in the left\n",
    "column and in the center of the middle row, while it has large negative weights in the right\n",
    "column. (And you can see that it has a pretty large negative bias, which means that it\n",
    "won’t fire strongly unless it gets precisely the positive inputs it’s “looking for.”)\n",
    "Indeed, on those inputs, it does what you’d expect:\n",
    "left_column_only = [1, 0, 0, 0, 0] * 5\n",
    "print feed_forward(network, left_column_only)[0][0] # 1.0\n",
    "center_middle_row = [0, 0, 0, 0, 0] * 2 + [0, 1, 1, 1, 0] + [0, 0, 0, 0, 0] * 2\n",
    "print feed_forward(network, center_middle_row)[0][0] # 0.95\n",
    "right_column_only = [0, 0, 0, 0, 1] * 5\n",
    "print feed_forward(network, right_column_only)[0][0] # 0.0\n",
    "Similarly, the middle hidden neuron seems to “like” horizontal lines but not side vertical\n",
    "lines, and the last hidden neuron seems to “like” the center row but not the right column.\n",
    "(The other two neurons are harder to interpret.)\n",
    "What happens when we run my stylized 3 through the network?\n",
    "my_three = [0,1,1,1,0, # .@@@.\n",
    "0,0,0,1,1, # ...@@\n",
    "0,0,1,1,0, # ..@@.\n",
    "0,0,0,1,1, # ...@@\n",
    "0,1,1,1,0] # .@@@.\n",
    "hidden, output = feed_forward(network, my_three)\n",
    "The hidden outputs are:\n",
    "0.121080 # from network[0][0], probably dinged by (1, 4)\n",
    "0.999979 # from network[0][1], big contributions from (0, 2) and (2, 2)\n",
    "0.999999 # from network[0][2], positive everywhere except (3, 4)\n",
    "0.999992 # from network[0][3], again big contributions from (0, 2) and (2, 2)\n",
    "0.000000 # from network[0][4], negative or zero everywhere except center row\n",
    "which enter into the “three” output neuron with weights network[-1][3]:\n",
    "-11.61 # weight for hidden[0]\n",
    "-2.17 # weight for hidden[1]\n",
    "9.31 # weight for hidden[2]\n",
    "-1.38 # weight for hidden[3]\n",
    "-11.47 # weight for hidden[4]\n",
    "- 1.92 # weight for bias input\n",
    "So that the neuron computes:\n",
    "sigmoid(.121 * -11.61 + 1 * -2.17 + 1 * 9.31 - 1.38 * 1 - 0 * 11.47 - 1.92)\n",
    "which is 0.92, as we saw. In essence, the hidden layer is computing five different\n",
    "partitions of 25-dimensional space, mapping each 25-dimensional input down to five\n",
    "numbers. And then each output neuron looks only at the results of those five partitions.\n",
    "As we saw, my_three falls slightly on the “low” side of partition 0 (i.e., only slightly\n",
    "activates hidden neuron 0), far on the “high” side of partitions 1, 2, and 3, (i.e., strongly\n",
    "activates those hidden neurons), and far on the low side of partition 4 (i.e., doesn’t active\n",
    "that neuron at all).\n",
    "And then each of the 10 output neurons uses only those five activations to decide whether\n",
    "my_three is their digit or not.\n",
    "For Further Exploration\n",
    "Coursera has a free course on Neural Networks for Machine Learning. As I write this it\n",
    "was last run in 2012, but the course materials are still available.\n",
    "Michael Nielsen is writing a free online book on Neural Networks and Deep Learning.\n",
    "By the time you read this it might be finished.\n",
    "PyBrain is a pretty simple Python neural network library.\n",
    "Pylearn2 is a much more advanced (and much harder to use) neural network library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
