{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 18. Neural Networks\n",
    "\n",
    "An **artificial neural network** (ANN or neural network for short) = a predictive model motivated by the way the brain operates. Brain = collection of neurons wired together, each neuron looking @ the outputs of other neurons that feed into it, then performing a calculation, + then either fires (if calculation exceeds some threshhold) or doesn’t (if it doesn’t).\n",
    "\n",
    "Accordingly, ANNs consist of artificial neurons, which perform similar\n",
    "calculations over their inputs. NNs can solve a wide variety of problems like handwriting recognition + face detection, + are used heavily in **deep learning**, a trendy subfields of data science. However, most NNs = “black boxes” — inspecting their details doesn’t give much understanding of *HOW* they’re solving a problem. Large NNs can be difficult to train. **For most problems, they’re probably not the right choice.**\n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "Simplest NN = the **perceptron** = approximates a single neuron w/ `n` binary inputs + computes a *weighted sum* of its inputs + “fires” if that weighted sum >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './../../../00_DataScience/DSFromScratch/code')\n",
    "\n",
    "from linear_algebra import dot\n",
    "\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron_output(weights,bias,x):\n",
    "    \"\"\"Returns 1 if perceptron fires, 0 if not\"\"\"\n",
    "    calculation = dot(weights,x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron = simply distinguishing between the half spaces separated by the **hyperplane** of points `x` for which `dot(weights,x) + bias == 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W/ properly chosen weights, perceptrons can solve a # of simple problems\n",
    "\n",
    "* EX: can create an **AND gate** = returns 1 if both inputs = 1 but returns 0 if 1 input = 0) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [2, 2]\n",
    "bias = -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If both inputs = 1, `calculation` = 2 + 2 - 3 = 1, so `output = 1`. If only 1 input = 1, `calculation` = 2 + 0 - 3 = -1, so `output = 0`. If *both* inputs = 0, `calculation` = -3, so output = 0. \n",
    "\n",
    "Similarly, we could build an **OR gate with:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [2, 2]\n",
    "bias = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could build a **NOT gate** (1 input + converts 1 to 0 and 0 to 1) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [-2]\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some problems simply can’t be solved by a single perceptron. \n",
    "* Ex: Cannot use a perceptron to build an **XOR gate** = outputs 1 if exactly 1 input = 1 and 0 otherwise. \n",
    "\n",
    "This needs more-complicated NNs. Of course, you don’t need to approximate a neuron in order to build a logic gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "and_gate = min\n",
    "or_gate = max\n",
    "xor_gate = lambda x, y: 0 if x == y else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like real neurons, artificial neurons = more interesting when connected together.\n",
    "\n",
    "### Feed-Forward Neural Networks\n",
    "\n",
    "Topology of brain = enormously complicated, so it’s common to *approximate* it w an idealized **feed-forward NN** that consists of discrete **layers** of neurons, each connected to the next, which typically entails an **input layer** (receives inputs + feeds them forward *unchanged*), 1+ **hidden layers** (each consists of neurons that take outputs of previous layer, perform some calculation, + passes\n",
    "result to next layer), + an **output layer** (produces final outputs).\n",
    "\n",
    "Just like a perceptron, each (non-input) neuron has a **weight** corresponding to each of its inputs + a **bias**. To make representation impler, add the bias to the end of the weights vector + give each neuron a bias input that always = 1.\n",
    "\n",
    "As w/ a perceptron, for each neuron --> sum up products of its inputs + its weights. But here, rather than outputting `step_function()` applied to that product, output a **smooth approximation** of a step function = **the sigmoid function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1/(1+math.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use sigmoid instead of the simpler `step_function()`? = **In order to train a NN, need to use calculus, + to use calculus, need smooth functions**, + the **step function isn’t even continuous, + sigmoid = a good smooth approximation of it.**\n",
    "\n",
    "***NOTE*** sigmoid shape = logistic function\n",
    "\n",
    "After smoothing, calculate the output as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_output(weights,inputs):\n",
    "    return sigmoid(dot(weights,inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this function, can represent a neuron simply as a **`list` of weights w/ length = 1 more than # of inputs it takes (b/c of bias weight)**. Then can represent a NN as a **`list` of (noninput) layers, where each layer = just a `list` of neurons in that layer**\n",
    "\n",
    "* ***NN = `list` (layers) of `lists` (neurons) of `lists` (weights).***\n",
    "\n",
    "Given such a representation, *using* a NN is quite simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(neural_network,input_vec):\n",
    "    \"\"\"Takes in a NN, represented as a list of lists of lists of weights\n",
    "    and returns the output from forward-propogating the input\"\"\"\n",
    "    outputs = []\n",
    "    \n",
    "    # process 1 layer at a time\n",
    "    for layer in neural_network:\n",
    "        # add bias input\n",
    "        input_with_bias = input_vec + [1]\n",
    "        # computer layer (neuron) output\n",
    "        output = [neuron_output(neuron,input_with_bias)\n",
    "                 for neuron in layer]\n",
    "        # remember the output\n",
    "        outputs.append(output)\n",
    "        \n",
    "        # then send output to next layer as an input\n",
    "        input_vec = output\n",
    "    \n",
    "    # return final outputs\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s easy to build the **XOR gate** we couldn’t build w/ a single perceptron. We just need to scale the weights up so the `neuron_outputs` are either really close to 0 or really close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [9.38314668300676e-14]\n",
      "0 1 [0.9999999999999059]\n",
      "1 0 [0.9999999999999059]\n",
      "1 1 [9.383146683006828e-14]\n"
     ]
    }
   ],
   "source": [
    "xor_network = [ # hidden layer\n",
    "                [[20,20,-30],  # 'and' neuron\n",
    "                 [20,20,-10]], # 'or' neuron\n",
    "                # output layer\n",
    "                [[-60,60,-30]]] # 2nd input (but not 1st input's neuron)\n",
    "\n",
    "for x in [0,1]:\n",
    "    for y in [0,1]:\n",
    "        # feed-forward produces the outputs of every neuron\n",
    "        # feedforward[-1] = outputs of output-layer neurons\n",
    "        print(x,y,feed_forward(xor_network,[x,y])[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By using a hidden layer, we're able to feed the output of an “and” neuron + output of an “or” neuron into a “2nd input but not 1st input” neuron. The result = a network that performs “or, but not and,” which is precisely XOR \n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Usually don’t build NNs by hand in part b/c we use them to solve much bigger problems (image recognition problem might involve hundreds or thousands of neurons) + in part b/c we usually won’t be able to “reason out” what the neurons should be.\n",
    "\n",
    "Instead (as usual), use data to train NN + 1 popular approach = **backpropagation algorithm** = similarities w/ gradient descent algorithm.\n",
    "\n",
    "Imagine we have a training set w/ input vectors + corresponding target output vectors.\n",
    "\n",
    "* Ex: xor_network --> input vector = `[1,0]` correspondeds to target output `[1]`.\n",
    "\n",
    "Imagine our network has some set of weights that we adjust using the following algorithm:\n",
    "\n",
    "* 1. Run `feed_forward()` on an input vector to produce outputs of all neurons in the network.\n",
    "* 2. Results in an error for each output neuron (difference between its output\n",
    "+ its target)\n",
    "3. Compute the gradient of this error as a function of the neuron’s weights + adjust its weights in the direction that most decreases the error.\n",
    "* 4. “Propagate” these output errors *backward* to **infer errors** for the hidden layer.\n",
    "* 5. Compute gradients of *these* errors + adjust the hidden layer’s weights in the same manner.\n",
    "\n",
    "Typically run this algorithm many times for an entire training set until the network **converges**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(network, input_vec, targets):\n",
    "    \n",
    "    hidden_outputs, outputs = feed_forward(network,input_vec)\n",
    "    \n",
    "    # output*(1-output) = derivative of sigmoid\n",
    "    output_deltas = [output*(1 - output)*(output - targets[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "    \n",
    "    # adjust weights for output later, 1 neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        # focus on ith output later neuron\n",
    "        for j, hidden_output in enumerate(hidden_outputs+[1]):\n",
    "            # adjust jth weight based on both ith neuron's delta\n",
    "            # + its jth input\n",
    "            output_neuron[j] -= output_deltas[i]*hidden_output\n",
    "    \n",
    "    # back-propogate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output*(1*hidden_ouput)*\n",
    "                     dot(output_deltas,[n[i] for n in output_layer])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    # adjust weights for hidden layer, 1 neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vec + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i]*input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate(network, input_vector, target):\n",
    "\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target[i])\n",
    "                     for i, output in enumerate(outputs)]\n",
    "\n",
    "    # adjust weights for output layer (network[-1])\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "\n",
    "    # back-propagate errors to hidden layer\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                      dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # adjust weights for hidden layer (network[0])\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much doing the same thing as if you explicitly wrote the squared error as a function of the weights + used `minimize_stochastic()`. In this case, explicitly writing out the gradient function turns out to be kind of a pain. If you know calc + the chain rule, the mathematical details are relatively straightforward, but keeping the notation straight (“the partial derivative of the error function w/ respect to the weight that neuron `i` assigns to the input coming from neuron `j`”) is not much fun.\n",
    "\n",
    "### Example: Defeating a CAPTCHA\n",
    "    \n",
    "To make sure people registering for a site are actually people, VP of Product\n",
    "Management wants to implement a CAPTCHA as part of the registration process. In particular, wants to show users a pic of a digit + require them to input that digit to prove they’re human. He doesn’t believe you that CPUs can easily solve this problem, so you decide to prove it.\n",
    "\n",
    "Represent each digit as a 5 × 5 image:\n",
    "\n",
    "@@@@@ ..@.. @@@@@ @@@@@ @...@ @@@@@ @@@@@ @@@@@ @@@@@ @@@@@\n",
    "@...@ ..@.. ....@ ....@ @...@ @.... @.... ....@ @...@ @...@\n",
    "@...@ ..@.. @@@@@ @@@@@ @@@@@ @@@@@ @@@@@ ....@ @@@@@ @@@@@\n",
    "@...@ ..@.. @.... ....@ ....@ ....@ @...@ ....@ @...@ ....@\n",
    "@@@@@ ..@.. @@@@@ @@@@@ ....@ @@@@@ @@@@@ ....@ @@@@@ @@@@@\n",
    "\n",
    "The NN wants an input to be a *vector of #'s* --> so transform each image to a vector of length 25 w/ elements = either 1 (“this pixel is in the image”) or\n",
    "0 (“this pixel is not in the image”).\n",
    "\n",
    "* Ex: '0' digit would be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_digit = [1,1,1,1,1,\n",
    "              1,0,0,0,1,\n",
    "              1,0,0,0,1,\n",
    "              1,0,0,0,1,\n",
    "              1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want output to indicate which digit the NN thinks it is, so we need 10 outputs, where (for ex) correct output for digit '4' = `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`. Then, assuming inputs are correctly ordered from 0 to 9, our targets will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = [[1 if i == j else 0 for i in range(10)] for j in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that (for example) `targets[4]` = the correct output for digit '4'.\n",
    "\n",
    "At which point we’re ready to build our NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DATA SETUP\n",
    "\n",
    "raw_digits = [\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             1...1\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\n",
    "             ..1..\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             1....\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"1...1\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1....\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\n",
    "             ....1\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             1...1\n",
    "             11111\"\"\",\n",
    "\n",
    "          \"\"\"11111\n",
    "             1...1\n",
    "             11111\n",
    "             ....1\n",
    "             11111\"\"\"]\n",
    "\n",
    "def make_digit(raw_digit):\n",
    "    return [1 if c == '1' else 0\n",
    "            for row in raw_digit.split(\"\\n\")\n",
    "            for c in row.strip()]\n",
    "\n",
    "inputs = list(map(make_digit, raw_digits))\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "           for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random.seed(0)   \n",
    "input_size = 25  # dimensions of img = lenght-25 vector\n",
    "num_hidden = 5   # 5 neurons in hidden layer\n",
    "output_size = 10 # 10 outputs for each input (binary 1/0 for each digit)\n",
    "\n",
    "### # the network starts out with random weights\n",
    "# each hidden neuron = 1 weight per input, + a bias weight\n",
    "hidden_layer = [[random.random() for __ in range(input_size + 1)]\n",
    "                for __ in range(num_hidden)]\n",
    "\n",
    "# each output neuron = 1 weight per hidden neuron, + a bias weight\n",
    "output_layer = [[random.random() for __ in range(num_hidden + 1)]\n",
    "                for __ in range(output_size)]\n",
    "\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "# seems to converge ~10k iterations\n",
    "for __ in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "\n",
    "def predict(input):\n",
    "    return feed_forward(network, input)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.024099252251227495, 6.535805275537946e-06, 7.82498285497602e-11, 0.017301919554017466, 0.0011793422713514314, 6.648285303812686e-10, 4.480803518696326e-08, 0.969007735053818, 1.0889968608877994e-08, 2.565215480544878e-08]\n"
     ]
    }
   ],
   "source": [
    "print(predict(inputs[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates the digit '7' output neuron produces `0.97`, while all other output\n",
    "neurons produce very small numbers. But we can also apply it to differently drawn digits, like stylized 3 or 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.089379748597191e-09, 0.0015111927756024143, 1.2269558364879473e-08, 0.9449497176995292, 6.320723731390414e-07, 4.772839810266144e-06, 2.566661329270676e-10, 0.009159247290970103, 6.425186805525349e-08, 0.1296193147755028]\n",
      "\n",
      "\n",
      "[2.8398960946147525e-06, 3.420730717233115e-13, 7.377641549372272e-09, 0.0003464790956137235, 1.3653801311139735e-10, 0.5915126275536223, 2.9656227284800537e-05, 1.7741535556168957e-07, 0.9492129655894995, 0.9961441866705154]\n"
     ]
    }
   ],
   "source": [
    "# classify a '3'\n",
    "print(predict([0,1,1,1,0, # .@@@.\n",
    "         0,0,0,1,1, # ...@@\n",
    "         0,0,1,1,0, # ..@@.\n",
    "         0,0,0,1,1, # ...@@\n",
    "         0,1,1,1,0])) # .@@@.\n",
    "print('\\n')\n",
    "# classify an '8'\n",
    "print(predict([0,1,1,1,0, # .@@@.\n",
    "               1,0,0,1,1, # @..@@\n",
    "               0,1,1,1,0, # .@@@.\n",
    "               1,0,0,1,1, # @..@@\n",
    "               0,1,1,1,0])) # .@@@."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN thinks stylized 3 looks like a 3, but stylized 8 gets votes for being a 5, an 8, + a 9. Having a larger training set would probably help.\n",
    "\n",
    "Although the NN’s operation is not exactly transparent, we can inspect weights of the hidden layer to get a sense of what they’re recognizing. In particular, can plot weights of each neuron in a 5x5 grid corresponding to the 5×5 inputs, w/ 0 weights = white + larger positive weights more green + larger negative weights more red. Or, just have far-away-from-0 weights get darker + darker, using **crosshatching** to indicate negative weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x858b048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "weights = network[0][0] # 1st neuron in hidden layer\n",
    "abs_weights = [abs(weight) for weight in weights] # darkness will only depend on absolute value\n",
    "\n",
    "grid = [abs_weights[row:(row+5)] # turn weights into 5x5 grid\n",
    "       for row in range(0,25,5)] # weights[0:5], ..., weights[20:25]\n",
    "\n",
    "ax = plt.gca() # need axis to plot hatching\n",
    "\n",
    "# cmap = white-black color scale, interpolation='none' for plotting blocks as blocks\n",
    "ax.imshow(grid, cmap=matplotlib.cm.binary, interpolation=\"none\")\n",
    "\n",
    "def patch(x,y,hatch,color):\n",
    "    \"\"\"Return a matplotlib 'patch' object with the specified location,\n",
    "    crosshatch pattern, and color\"\"\"\n",
    "    return matplotlib.patches.Rectangle((x-.5,y-.5),1,1,\n",
    "                                       hatch=hatch,fill=False,color=color)\n",
    "\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5): # row\n",
    "    for j in range(5): # col\n",
    "        if weights[5*i+j]<0: # row i, col j = weights[5*i+j]\n",
    "            # add B&W hatches visible whether dark or light\n",
    "            ax.add_patch(patch(j,i,'/','white'))\n",
    "            ax.add_patch(patch(j,i,'\\\\','black'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngQOFEThQGIEDhRE4UBiBA4X9FzIE/1OXJHf7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x858b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = network[0][1] # 2nd neuron in hidden layer\n",
    "abs_weights = [abs(weight) for weight in weights] # darkness will only depend on absolute value\n",
    "\n",
    "grid = [abs_weights[row:(row+5)] # turn weights into 5x5 grid\n",
    "       for row in range(0,25,5)] # weights[0:5], ..., weights[20:25]\n",
    "\n",
    "ax = plt.gca() # need axis to plot hatching\n",
    "\n",
    "# cmap = white-black color scale, interpolation='none' for plotting blocks as blocks\n",
    "ax.imshow(grid, cmap=matplotlib.cm.binary, interpolation=\"none\")\n",
    "\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5): # row\n",
    "    for j in range(5): # col\n",
    "        if weights[5*i+j]<0: # row i, col j = weights[5*i+j]\n",
    "            # add B&W hatches visible whether dark or light\n",
    "            ax.add_patch(patch(j,i,'/','white'))\n",
    "            ax.add_patch(patch(j,i,'\\\\','black'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x82bdda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoaCE5IwFJyQhKHghCQMBSckYf4PlJtm717aPeoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85c8ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = network[0][3] # 2nd neuron in hidden layer\n",
    "abs_weights = [abs(weight) for weight in weights] # darkness will only depend on absolute value\n",
    "\n",
    "grid = [abs_weights[row:(row+5)] # turn weights into 5x5 grid\n",
    "       for row in range(0,25,5)] # weights[0:5], ..., weights[20:25]\n",
    "\n",
    "ax = plt.gca() # need axis to plot hatching\n",
    "\n",
    "# cmap = white-black color scale, interpolation='none' for plotting blocks as blocks\n",
    "ax.imshow(grid, cmap=matplotlib.cm.binary, interpolation=\"none\")\n",
    "\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5): # row\n",
    "    for j in range(5): # col\n",
    "        if weights[5*i+j]<0: # row i, col j = weights[5*i+j]\n",
    "            # add B&W hatches visible whether dark or light\n",
    "            ax.add_patch(patch(j,i,'/','white'))\n",
    "            ax.add_patch(patch(j,i,'\\\\','black'))\n",
    "plt.show()\n",
    "\n",
    "weights = network[0][4] # 2nd neuron in hidden layer\n",
    "abs_weights = [abs(weight) for weight in weights] # darkness will only depend on absolute value\n",
    "\n",
    "grid = [abs_weights[row:(row+5)] # turn weights into 5x5 grid\n",
    "       for row in range(0,25,5)] # weights[0:5], ..., weights[20:25]\n",
    "\n",
    "ax = plt.gca() # need axis to plot hatching\n",
    "\n",
    "# cmap = white-black color scale, interpolation='none' for plotting blocks as blocks\n",
    "ax.imshow(grid, cmap=matplotlib.cm.binary, interpolation=\"none\")\n",
    "\n",
    "# cross-hatch the negative weights\n",
    "for i in range(5): # row\n",
    "    for j in range(5): # col\n",
    "        if weights[5*i+j]<0: # row i, col j = weights[5*i+j]\n",
    "            # add B&W hatches visible whether dark or light\n",
    "            ax.add_patch(patch(j,i,'/','white'))\n",
    "            ax.add_patch(patch(j,i,'\\\\','black'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the 1st hidden neuron has large positive weights in the left col + center of middle row, w/ large negative weights in  right column. (has a pretty large negative bias, which means it won’t fire strongly unless it gets precisely the positive inputs it’s “looking for.”)\n",
    "\n",
    "Indeed, on those inputs, it does what you’d expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999757061230374\n",
      "0.9454650524875013\n",
      "2.0967428729819267e-09\n"
     ]
    }
   ],
   "source": [
    "left_col_only = [1, 0, 0, 0, 0]*5\n",
    "print(feed_forward(network,left_col_only)[0][0])\n",
    "\n",
    "center_middle_row = [0, 0, 0, 0, 0]*2 + [0, 1, 1, 1, 0] + [0, 0, 0, 0, 0] * 2\n",
    "print(feed_forward(network,center_middle_row)[0][0])\n",
    "\n",
    "right_col_only = [0, 0, 0, 0, 1]*5\n",
    "print(feed_forward(network,right_col_only)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the middle hidden neuron seems to “like” horizontal lines but not side vertical lines, + the last hidden neuron seems to “like” the center row but not the right col. (The other two neurons are harder to interpret.)\n",
    "\n",
    "What happens when we run my stylized 3 through the network?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10087970544675262, 0.9999789525258017, 0.9999999995375104, 0.999992327786406, 2.5371920979367814e-07]\n",
      "\n",
      " [8.089379748597191e-09, 0.0015111927756024143, 1.2269558364879473e-08, 0.9449497176995292, 6.320723731390414e-07, 4.772839810266144e-06, 2.566661329270676e-10, 0.009159247290970103, 6.425186805525349e-08, 0.1296193147755028]\n"
     ]
    }
   ],
   "source": [
    "my_three = [0,1,1,1,0, # .@@@.\n",
    "            0,0,0,1,1, # ...@@\n",
    "            0,0,1,1,0, # ..@@.\n",
    "            0,0,0,1,1, # ...@@\n",
    "            0,1,1,1,0] # .@@@.\n",
    "\n",
    "hidden, output = feed_forward(network,my_three)\n",
    "\n",
    "print(hidden)\n",
    "print(\"\\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden outputs are:\n",
    "* 0.121080 = from network\\[0][0], probably dinged by (1, 4)\n",
    "* 0.999979 = from network\\[0][1], big contributions from (0, 2) and (2, 2)\n",
    "* 0.999999 = from network\\[0][2], positive everywhere except (3, 4)\n",
    "* 0.999992 = from network\\[0][3], again big contributions from (0, 2) and (2, 2)\n",
    "* 0.000000 = from network\\[0][4], negative or 0 everywhere except center row\n",
    "\n",
    "which enter into the “three” output neuron w/ weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.020905997913047, -2.105345568125583, 9.343087556164646, -1.3176301244196778, -11.652280680184836, -1.8646131418995124]\n"
     ]
    }
   ],
   "source": [
    "print(network[-1][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* -12.02 # weight for hidden[0]\n",
    "* -2.10 # weight for hidden[1]\n",
    "* 9.34 # weight for hidden[2]\n",
    "* -1.317 # weight for hidden[3]\n",
    "* -11.65 # weight for hidden[4]\n",
    "* -1.86 # weight for bias input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the neuron computes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9194716562747247"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(.121 * -11.61 + 1 * -2.17 + 1 * 9.31 - 1.38 * 1 - 0 * 11.47 - 1.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, the hidden layer is computing 5 different partitions of 25D space, mapping each 25D input down to 5 #'s, then each output neuron looks only at the results of those 5 partitions.\n",
    "\n",
    "`my_three` falls slightly on the “low” side of partition 0 (i.e., only slightly activates hidden neuron 0), far on the “high” side of partitions 1, 2, + 3, (i.e., strongly activates those hidden neurons), + far on the low side of partition 4 (i.e., doesn’t activate that neuron at all), + then each of the 10 output neurons uses only those 5 activations to decide whether `my_three` is their digit or not.\n",
    "\n",
    "### For Further Exploration\n",
    "* [Coursera](https://www.coursera.org/learn/neural-networks) has a free course on Neural Networks for Machine Learning. \n",
    "* Michael Nielsen is writing a free [online book](http://neuralnetworksanddeeplearning.com/) on Neural Networks and Deep Learning.\n",
    "* PyBrain is a pretty simple Python neural network library.\n",
    "* Pylearn2 is a much more advanced (and much harder to use) neural network library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
