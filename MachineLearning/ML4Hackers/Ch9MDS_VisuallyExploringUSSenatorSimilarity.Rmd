---
title: "Ch9MDS_VisuallyExploringUSSenatorSimilarity"
author: "Steve Newns"
date: "December 5, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
#library(lubridate) # ymd()
#library(reshape) # cast()
```

# Clustering Based on Similarity

There are many situations where we might want to know how similar members of a group of people are to one another, like a brand marketing company that had just completed a research survey on a potential new brand that showed a group of people several features of the brand + asked them to rank the brand on each of these features using a 5-pt scale. We also collected a bunch of socioeconomic data from the  (age, gender, race, zip, approximate annual income). From this survey, we want to understand how the brand appeals across all of these socioeconomic variables. Most importantly, we want to know whether the brand has broad appeal. 

An alternative way of thinking about this problem is we want to see whether individuals who like most of the brand features have diverse socioeconomic features. A useful means of doing this = visualize how survey respondents **cluster**. We could then use various visual cues to indicate their memberships in different socioeconomic categories (would want to see a large amount of mixing between gender, + among races + economic stratification). Likewise, we could use this knowledge to see how the close groups are clustered based on the brand’s appeal. We could also see how many people were in 1 cluster compared to others, or how far away other clusters were. This might tell us what features of the brand to target at different socioeconomic categories. 

When phrasing these questions, we use words such as “close” + “far,” which have an inherent notion of distance. To visualize distance among clusters, we need some spatial concept of how individuals congregate. We can use notions of distance among a set of observations to illustrate similarities + dissimilarities, which requires the definition of some metric for distance relevant to the type of data being analyzed. For brand marketing, we could use the ordinal nature of the survey’s scale to find the distances among respondents in a very straightforward way = simply calculate the absolute differences. It is not enough, however, to only calculate these distances. 

**Multidimensional scaling (MDS)** is for clustering observations based on a measure of distance among the observations. Applying MDS allows us to visualize data using *only a measure of the distance between all of the DPs*. 

# A Brief Introduction to Distance Metrics and Multidirectional Scaling

Suppose we have a very simple data set w/ 4 customers who have rated 6 products w/ each customer asked to give each product a thumbs up or down, but could skip a product if they had no opinion about it. There are many rating systems that work this way, like Pandora’s + YouTube’s. Using ratings data, we'd like to measure how similar customers are to one another. We will set up a 4×6 matrix where rows = customers + columns = product filled w/ simulated ratings by randomly selecting thumbs up (1), down (–1), or skip (0) for each customer/product pair. 
```{r}
set.seed(851982)
# sample from thumbs up/down or skip with replacement for all 24 cells in matrix
test_mtx <- matrix(sample(c(-1,0,1), size = 24, replace = T), nrow = 4, ncol = 6)

row.names(test_mtx) <- c('A','B','C','D')
colnames(test_mtx) <- c('P1','P2','P3','P4','P5','P6')
test_mtx
```

Now we need to use these differences to generate a distance metric among all customers first by summarizing the customer ratings for each product as they relate to all other customers (rather than only to the products in the current form). Another way to think about this is that we need to convert the N-by-M matrix into an N-by-N matrix, wherein elements of the new matrix provide summaries of the relation *among users based on product rating*. 1 way to do this = multiply our matrix by its **transpose** w/ the effect being a computation of the *correlation between every pair of columns in the original matrix*.

**Matrix transposition** *inverts* a matrix so rows become cols + cols become rows. Visually, it turns a matrix 90 degrees clockwise + then flips it vertically (over y-axis)
```{r}
# customer-by-review matrix
test_mtx
# review-by-customer matrix:
t(test_mtx)
```

**Matrix multiplication** is slightly more complicated, but involves only basic arithmetic. To multiply 2 matrices, loop over the rows of the 1st matrix + the columns of the 2nd matrix. For each row-column pair, multiply each pair of entries + sum them together. There are a few important things to keep in mind when performing matrix multiplication. 

B/c we're multiplying the row elements of the 1st matrix by the column elements of the 2ND, these dimensions *MUST* conform across the 2 matrices. In our case, we could not simply multiply `test.matrix` by a 2×2 matrix as the arithmetic would not work out. As such, the result of a matrix multiplication will *always* have the same # of rows as the 1st matrix + the same # of columns as the 2nd matrix. An immediate consequence of this is that order matters in matrix multiplication. In our case, we want a matrix that summarizes differences among customers, so we multiply our customer-by-review matrix by its transpose to get a **customer-by-customer matrix** as the product. If we did the inverse, we'd get the differences among the reviews, which might be interesting in another situation, but it is not useful right now.

Matrix multiplication is as simple as taking a row element of the 1st matrix + multiplying it by the corresponding column element from the 2nd matrix, taking all those products, + summing them.
```{r}
(test.mult <- test_mtx %*% t(test_mtx))
```

`%*%` performs matrix multiplication. The interpretation of the new matrix is fairly straightforward. B/c we've used the 1, –1, 0 coding scheme, the *off-diagonal values* summarize their overall agreement (positive value) or disagreement (negative value) on product reviews, given those products they have *both* reviewed (i.e. nonzero entries). The more positive the off-diagonal element, the more agreement, + the more negative, the less agreement. B/c the entries are random, there is very little divergence among the customers, w/ no value > 1 or < –1. The *diagonal values* simply represent the # of products each user reviewed.

We now have a somewhat useful summary of the differences among users. Customers A + D both gave product 4 a thumbs down, but customer D liked products 1 + 3, whereas customer A did not review them. So, for the products for which we have info, we might say they are similar, + thus have a 1 corresponding to their relationship. Unfortunately, this is very limiting b/c we can only say something about the *overlap* + we'd rather have a method for extrapolating these differences into a richer representation.

To do this, we will use the concept of **Euclidean distance** in a multidimensional space. In 1, 2, or 3D, Euclidean distance = a formalization of our intuitions about distance. Euclidean distance between 2 points in space = measure the shortest direct path between them. We want to calculate Euclidean distance among all customers based on the measures of overall similarities + differences defined by the matrix multiplication.

To do this, treat each customer’s ratings as a vector. To compare customer A w/ customer B, subtract the vectors, square the differences, sum them, + take the square root = Euclidean distance between customer A’s ratings + customer B’s ratings. Do this “by hand” in R using the base functions for Customers A + D
```{r}
sqrt(sum((test.mult[1,]-test.mult[4,])^2))
```

Thankfully, calculating all pairwise distances between rows of a matrix is done w/ the base function `dist` which returns a matrix of the distances = **a distance matrix** + where Euclidean distance is the default distance metric
```{r}
(test.dist <- dist(test.mult))
```

This matrix is actually only the lower triangle of the entire distance matrix. It's very common to show only the lower triangle for a distance matrix b/c the distance matrix must be symmetric, as the distance between row X + row Y = distance between row Y + row X. Showing the upper triangle of the distance matrix = redundant (can override this by setting `upper=TRUE` in `dist`)

Customers A + D are the closest, and customers D + B are the farthest. We now have a clearer sense of similarities among the users based on product reviews; however, it would be better if we could get a *visual* sense of these dissimilarities, which is where MDS can be used: to produce a spatial layout of customers based on these distances.

**MDS** = a set of statistical techniques used to visually depict similarities + differences from set of distances. For classical MDS: take a distance matrix that specifies the distance between every pair of points in a data set + return a set of coordinates for those 2 points that approximates those distances. We need to create an *approximation* b/c it may not be possible to find points in 2 dimensions that are all separated by a specific set of distances. For example, it isn’t possible to find 4 points in 2 dimensions that are all at a distance of 1 from each other. (To convince yourself of this, note that 3 points all at a distance of 1 from each other = tips of a triangle. Convince yourself there isn’t any way to find another point that’s at a distance of 1 from all 3 points on this triangle.)

Classical MDS uses a specific approximation to the distance matrix + is therefore another example of an optimization algorithm being used for ML. Of course, the approximation algorithm behind classical MDS can be used in 3 dimensions or 4, but our goal is to obtain a representation of our data that’s easy to visualize, so we'll be using MDS to scale data in 2 dimensions. This is the most common way of using MDS because it allows for very simple visualizations of the data on a coordinate plot. It is perfectly reasonable, however, to use MDS to scale data into higher-order dimensions. A 3D visualization might reveal different levels of clustering among observations as points move into the 3rd dimension.

The classical MDS procedure is part of R’s base functions as `cmdscale` w/ required input = a distance matrix. By default, `cmdscale` computes an MDS in 2 dimensions, but you can set this using the `k` parameter
```{r}
(test.mds <- cmdscale(test.dist))
plot(test.mds, type = "n")
text(test.mds, c('A','B','C','D'))
```

See customers A + D cluster together in the center-right of the plot while Customers B + C do not cluster at all. From our data, we can see that customers A + D have somewhat similar tastes, but we'd need to acquire much more data and/or customers before we could hope to understand how customers B + C cluster. It is also important to note that although we can see how A + D cluster, + likewise how B + C do not, we cannot say anything substantive about how to interpret these distances. We know A + D are more similar b/c of their proximity in the coordinate plane, but we cannot use the numeric distance between them to interpret how similar or dissimilar they are from B or C. The exact numeric distance produced by MDS is an artifact of the MDS algorithm + allows only a very limited substantive interpretation.

Case study = real data from roll call votes from the US Senate to show how members of the US Senate cluster over chronological Congresses using records of roll call votes in the Senate to generate our distance metric + using MDS to cluster senators in 2 dimensions.

# How Do US Senators Cluster?

"The current Congress —the 111th— is the most ideologically polarized in modern history. In both the House + the Senate, the most conservative Democrat is more liberal than the most liberal Republican. If one defines the congressional “center” as the overlap between the 2 parties, the center has disappeared." — William A. Galston, The Brookings Institute (2010)

Often hear claims that polarization in the US Congress is at an all-time high + it is easy to understand why. Such portraits are often made in the popular press + mainstream media outlets in the US often work to amplify these differences. If we think of legislative morass as a by-product of this polarization, we could look to legislative outcomes as a rough measure of polarization. In the 110th Congress, nearly 14k pieces of legislation were introduced, but only 449 bills, or 3.3%, actually became law + of those 449 bills, 144 simply changed the name of a federal building.

But is the US Congress actually more polarized now than ever before? We'd prefer a more principled means of answering this question w/ MDS to visualize clustering of senators across party lines to see whether any mixing exists between members of the 2 parties. Before we can do this, however, we need a metric for measuring distance among senators. Fortunately, the US Congress is 1 of the most open legislative bodies in the world + we can use the public records of legislators to construct a reasonable distance metric.

We will use legislator voting records + use roll call voting records as a means of measuring legislators’ approval/disapproval of a proposed bill. Legislators vote w/ Yeas (approve) or Nays (disapprove) for bills. A **roll call vote** is one of the most basic parliamentary procedures in either chamber of the US Congress + it is the process by which members of the House of Representatives + Senate vote for any proposal brought before the floor. Each house has a different mechanism by which a roll call vote can be initiated, but the results are basically equivalent. 

**A roll call vote** = the record of each legislator’s action on a given proposal which typically takes the form of a Yea or Nay, but the results are slightly more complicated than that. Roll call data is a great resource for measuring similarities + differences among legislators + is an invaluable resource for political scientists who study Congress. The data is so valuable that 2 political scientists created a unified resource for downloading it. Keith Poole (University of Georgia) and Howard Rosenthal (NYU) maintain http://www.voteview.com/, a repository for all US roll call data from the very 1st Congress to the most recent at the time of this writing (i.e. the 111th). We will examine the roll call voting data from the US Senate for the 101st through 111th Congresses. 

# Analyzing US Senator Roll Call Data (101st–111th Congresses)

Use `list.files` to generate a character vector `data.files` w/ all of the data filenames in it.
```{r}
library(foreign)
library(ggplot2)
data.dir <- "data/roll_call/"
data.files <- list.files(data.dir)
head(data.files)
```

See that the extension for the datafiles = **.dta** for a Stata datafile, a commercial statistical computing program that happens to be very popular among academics, particularly political scientists. B/c Poole + Rosenthal decided to disseminate the data in this format, we need a way of loading this data into R. `foreign` is designed to read a large number of exotic datafiles into R data frames, including S, SAS, SPSS, Systat, dBase, + many others. `read.dta` is designed to read Stata files. We want to analyze the data for 10 Congresses, from the 101st to the 111th in a single object that can be manipulated at once. These data sets are relatively small, so we do not have any concern about
memory in this case. 
```{r}
# read in each .dta file from data.files character vector
rollcall_data <- lapply(data.files, 
                        function(x) { read.dta(paste(data.dir, x, sep = ""),
                                               convert.factors = F) })
head(rollcall_data,3)
```

We now have all 10 data frames of roll call votes stored in `rollcall.data`

When we check the dims of the `st DF, the 101st Congress, see it has 103 rows + 647 columns. When we further check the head of this data frame, ther're 2 important things to notew
<ol>
<li> each row corresponds to a voter in the US Senate. </li>
<li> 1st 9 columns include identification info for those voters + remaining columns are the actual votes. </li>
</ol>

Before we can proceed, we need to make sense of this identification info
```{r}
#dim(rollcall_data[[1]])
head(rollcall_data[[1]])
```

Some cols are quite obvious, such as `lstate` +`name`, but what about `eh1` + `eh2`.? Poole + Rosenthal provide a codebook for all roll call data w/ the codebook for the 101st Congress located at http://www.voteview.com/senate101.html
<ol>
<li> Congress Number <li>
<li> ICPSR ID Number:  5 digit code assigned by the ICPSR as corrected by Howard Rosenthal + myself. <li>
<li> State Code:  2 digit ICPSR State Code.  <li>
<li> Congressional District Number (0 if Senate) <li>
<li> State Name <li>
<li> Party Code:  100 = Dem., 200 = Repub. (See PARTY3.DAT) <li>
<li> Occupancy:  ICPSR Occupancy Code -- 0=only occupant; 1=1st occupant; 2=2nd occupant; etc. <li>
<li> Last Means of Attaining Office:  ICPSR Attain-Office Code -- 1=general election; 2=special election; 3=elected by state legislature; 5=appointed <li>
<li> Name <li>
<li> to the number of roll calls + 10:  Roll Call Data -- 0=not a member, 1=Yea, 2=Paired Yea, 3=Announced Yea, 4=Announced Nay, 5=Paired Nay, 6=Nay, 7=Present (some Congresses, also not used some Congresses), 8=Present (some Congresses, also not used some Congresses), 9=Not Voting <li>
</ol>

We're only interested in the *names* of voters, their party affiliations, + their actual votes. Get the roll call vote data in a form from which to create our reasonable distance metric from the votes. We see roll call votes in the Senate are not simply Yeas or Nays;
there are Announced and Paired forms of Yea and Nay votes, as well as Present votes (votes in which a senator abstained from voting on a specific bill but was present at the time of the vote). There are also times when senators were simply not present to cast a vote or had not yet even been elected to the Senate. Given the variety of possible votes, 1 approach to take this data + transform it into something we can easily use to measure distance between the senators is to simplify data coding by aggregating like vote types. 

For example, "Paired voting"" = procedure whereby members of Congress know they will not be present for a given roll call + have their vote “paired” w/ another member who is going to vote the opposite. This, along with "Announced votes"", are Parliamentary means for the Senate/House to a establish a quorum for a vote to be held. We're less concerned w/ the mechanism by which the vote occurred, but rather the intended outcome of the vote (for or against). As such, 1 method for aggregating = group all Yea + Nay types together + group all non-vote-casting types together.
```{r}
simplify_rollcall <- function(df) {
  not.present <- subset(df, state < 99) # remove VP
  # loop through all voting cols
  for(i in 10:ncol(not.present)) {
    not.present[,i] <- ifelse(not.present[,i] > 6, 0, # non-votes
                          ifelse(not.present[,i] > 0 & not.present[,i] < 4, 1, # yeays
                            ifelse(not.present[,i] > 1, -1, not.present[,i]))) # nays
  }
  return(as.matrix(not.present[10:ncol(not.present)]))
}

# apply this simplification to each congress (data frame in rollcall_data)
rollcall_simplified <- lapply(rollcall_data, simplify_rollcall)
```

# Exploring senator MDS clustering by Congress

1st step = use senator-by-votes matrix to create a senator-by-senator distance matrix on which we will perform MDS. Uee `lapply` to perform the conversion steps for each Congress separately by 1st performing the matrix multiplication + storing the results in `rollcall_dist` + then performing MDS using `cmdscale` via another `lapply`.

2 things to note about the MDS operation:
<ul>
<li> by default `cmdscale` computes the MDS for 2 dimensions, so `k=2` is redundant, but it's useful to be explicit when performing this operation when sharing code </li>
<li> We are multiplying all points by –1 strictly for visualization so it flips the x-axis positioning of all points + puts Dems on the left side + Republicans on the right. In context this is a useful visual cue, as we typically think of D as being ideologically left + R as toward the right. We only noticed that D's would end up on the right + R's on the left of the x-axis for the MDS after we visualized it. Part of doing data analysis well = being flexible + thinking critically about how to improve either a method or presentation of results after having completed them. </li>
</ul>
```{r}
# get distances of each senator
rollcall_dist <- lapply(rollcall_simplified, function(m) { dist(m %*% t(m))})

# create data frame containing coordinate points of each senator we got from their distances
rollcall_mds <- lapply(rollcall_dist, function(x) { as.data.frame(cmdscale(x, k = 2)*-1)})
head(rollcall_mds,5)
```

Next, add back in the appropriate identification data to the coordinate points data frames in `rollcall.mds` so we can visualize them in the context of party affiliation. 
```{r}
congresses <- 101:111
for(i in 1:length(rollcall_mds)) {
  # set names of coordinate points + remove VP from original data
  names(rollcall_mds[[i]]) <- c("X","Y")
  congress <- subset(rollcall_data[[i]], state < 99)  
  # for each senators name, split on the comma where present to only keep last names
  congress_names <- sapply(as.character(congress$name),
                           function(s) { strsplit(s, "[, ]")[[1]][1]}) 
  # add ID cols
  rollcall_mds[[i]] <- transform(rollcall_mds[[i]], 
                                 name = congress_names, party = as.factor(congress$party),
                                 congress = congresses[i])
}
head(rollcall_mds[[1]])
```
Now we have the data frames for the visualization. 1st, just plot the data for the 110th Senate, but then loop over this list of data frames to create individual visualizations for all Congresses.
```{r}
congress_110 <- rollcall_mds[[10]]
base_plot_110 <- ggplot(congress_110, aes(X, Y)) + 
                    scale_size(range = c(2,2), guide = F) + 
                    scale_alpha(guide = F) + 
                    theme_bw() + 
                    theme(axis.ticks.x = element_blank(), axis.text.x = element_blank(), 
                         axis.text.y = element_blank(), panel.grid.major = element_blank()) + 
                    ggtitle("Roll Call Vote MDS Clustering for 110th US Senate") +
                    xlab("") + ylab("") + 
                    scale_shape(name = "Party", breaks = c("100","200","328"), 
                                labels = c("Dem","Rep","Ind"), solid = F) + 
                    scale_color_manual(name = "Party", 
                                       values = c("100"="blue","200"="red","328"="green"),
                                       breaks = c("100","200","328"), labels = c("Dem","Rep","Ind"))

print(base_plot_110 + geom_point(aes(shape = party, alpha = .75, size = 2)))
print(base_plot_110 + geom_text(aes(color = party, alpha = .75, size = 2, 
                                    label = congress_110$name), hjust=0, vjust=0))
```

Typical `ggplot` procedure = create a object + then add a `geom` or `stat` layer, but in this case, we create a base object containing all the formatting particularities for these plots (including size, alpha, shape, color, + theme layers) b/c we want to make 2 plots: points themselves where shape = party affiliation + senators’ names as points w/ text color corresponds = party affiliation. By adding all of these formatting layers first, we can simply add `geom` layers to the base to get the plot we want. 