---
title: "Model Comparison"
author: "Steve Newns"
date: "December 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
```

# SVMs: The Support Vector Machine

The **support vector machine (SVM)** allows you to use multiple different **kernels** to find *non-linear* decision boundaries to classify points from a data set. A non-linear decision boundary can’t be discovered using a simple classification algorithm like logistic regression 

df <- read.csv('data/df.csv')
logit.fit <- glm(Label ~ X + Y,
                 family = binomial(link = 'logit'),
                 data = df)
logit.predictions <- ifelse(predict(logit.fit) > 0, 1, 0)
mean(with(df, logit.predictions == Label))
#[1] 0.5156
275
As you can see, we’ve correctly predicted the class of only 52% of the data. But we
could do exactly this well by predicting that every data point belongs to Class 0: