---
title: "Ch10_kNN_RecommendationSystems"
author: "Steve Newns"
date: "December 8, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
```
# The k-Nearest Neighbors Algorithm

Can use those same sort of similarity metrics as last chapter to recommend items to a website’s users.
k-nearest neighbors = arguably one of the most intuitive of all ML algorithms. Simplest form of kNN = the sort of algorithm most people would spontaneously invent if asked to make recommendations using similarity data: recommend the song that’s closest to songs a user already likes, but not yet in that list. That intuition is essentially a 1-nearest neighbor algorithm. The full k-NN algorithm amounts to a generalization of this intuition where you draw on more than 1 DP before making a recommendation.

The full k-NN algorithm works much in the way some of us ask for recommendations from our friends. 1st, start w/ people whose taste we feel we share ==> ask a bunch of them to recommend something to us. If many recommend the same thing, we deduce we’ll like it as well. How can we take that intuition and transform it into something algorithmic? 

Something simpler = classifying points into 2 categories: can use logistic regression to split those points using a single line, the decision boundaryBut, there are problems like that can’t be solved using  a simple linear decision boundary but can be solved w/ a nonlinear method; **the kernel trick** (Ch 12). Another approach = use the points nearest the point you’re trying to classify to make a guess. You could, for example, draw a circle around the point you’re looking at + then decide on a classification based on the points w/in that circle = the **“Grassroots Democracy” algorithm**, which is pretty usable, but has 1 noticeable flaw: we have to pick a radius for the circle we’ll use to define “local” points. If all of your DPs have neighbors at roughly the same distance, this won’t be a problem. But if some points have many close neighbors while others have almost none, you’ll end up picking a circle that’s so big it extends well past the decision boundaries for some points. What can we do to work around this problem? 

The obvious solution = avoid using a circle to define a point’s neighbors + instead look at the k-closest points = the k-nearest neighbors. Once we have them, look at their classifications + use majority rule to decide on a class for our new point. 
```{r}
df <- read.csv("data/example_data.csv")
glimpse(df)
```

```{r}
# compute Euclidiean distance between the points + store in distance matrix
# distance matrix = 2D array with distance between i + j is at matrix[i,j]
distance_mtx <- function(df) {
  distance <- matrix(rep(NA, nrow(df)**2), ncol = nrow(df))
  
  for(i in 1:nrow(df)) {
    for(j in 1:nrow(df)) {
      distance[i,j] <- sqrt((df[i,'X']-df[j,'X'])**2 + (df[i,'Y']-df[j,'Y'])**2)
    }
  }
return(distance)
}
distance_mtx(df)[1:5,1:5]
```

We also need a function to return the k-nearest neighbors for a point `i` = simply look at the `i`th row of the distance matrix to find how far all other points are from your input point. Once you sort this row, you have a list of the neighbors ordered by distance from the input point `i`. Select the 1st k entries after ignoring the input point (very 1st point in the distance matrix) + you’ll have the k-nearest neighbors of input point `i`.
```{r}
#kNN algorithm with default k = 5 to look for distances of point i
k_nearest_neighbors <- function(i, distance, k = 5) {
  # get the k points occuring after input point i
  return(order(distance[i,])[2:(k+1)])
}
distance_mtx(df)[1,k_nearest_neighbors(1,distance_mtx(df),k=5)]
```

Once you’ve done that, build a `knn` function that takes a df + a value for k + then produces predictions for each point in the df. Our function isn’t very general, b/c we’re assuming the class labels are found in a column `Label`, but this is just a quick exercise to see how the algorithm works. Use majority rules voting to make predictions by taking the mean `Label` value + thresholding mean distance of point `i` at 0.5. This function returns a vector of predictions we can append to our data frame + then assess performance.
```{r}
knn <- function(df, k = 5) {
  # get distance + set up prediction array to store predictions
  distance <- distance_mtx(df)
  predictions <- rep(NA, nrow(df))
  
  for(i in 1:nrow(df)) {
    indices <- k_nearest_neighbors(i, distance ,k = k)
    predictions[i] <- ifelse(mean(df[indices,'Label']) > .5, 1, 0)
  }
  return(predictions)
}

df %<>% 
  mutate(kNNPredictions = knn(df))
head(df)

paste("Correct:",sum(with(df,Label != kNNPredictions)))
paste("Total:",nrow(df))
paste("Accuracy: ",(1 - sum(with(df,Label != kNNPredictions))/nrow(df))*100,"%", sep = "")
```

We’ve incorrectly predicted labels for 7 points out of 100, so we’re correct 93% of the time, not bad performance for such a simple algorithm. 

Now apply kNN to some real data about usage of R packages w/ a black-box implementation of kNN from `class` package, `knn`, which uses cross-validation.
```{r}
# remove hard-coded knn algorithm
rm(knn)

library('class')

n <- nrow(df)
set.seed(1)
# get a SORTED random sample of indices from df of size = 1/2 the # of rows in df
indices <- sort(sample(1:n, n*(1/2)))

# create training + test sets from random sample indices
train_x <- df[indices, 1:2]
train_y <- df[indices, 3]
test_x <- df[-indices, 1:2]
test_y <- df[-indices, 3]

# use black-box knn algorithm to predict neighbors where 
#   cl = train.y = factor of true classifications of training set
#predicted.y <- knn(train = train_x, test = test_x, cl = train_y, k = 5)

predicted_y <- knn(train_x, test_x, train_y, k = 5)

paste("Correct:",sum(predicted_y != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predicted_y != test_y)/length(test_y))*100,"%", sep = "")
```

Amazingly, we get exactly 7 labels wrong here as well, but the test set has only 50 examples, so we’re only achieving 86% accuracy. That’s still pretty good, though. Let’s look at how a logistic model would've worked to give a point of comparison:
```{r}
# train logistic regression model on training data with default error distribution + test on test data
logistic_model <- glm(Label ~ X + Y, data = df[indices,], family = "gaussian")
predictions <- as.numeric(predict(logistic_model, newdata = df[-indices,]) > 0)

paste("Correct:",sum(predictions != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predictions != test_y)/length(test_y))*100,"%", sep = "")
```

As you can see, the best logistic classifier mispredicts 16 DP's = an accuracy of 68%. When your problem is far from being linear, kNN will work out of the box much better than other methods.

# R Package Installation Data

Now that we have an understanding of how to use kNN, let’s go through the logic of making recommendations. 1 way to start to make recommendations = find items similar to items users already like + then apply kNN = the **item-based approach**. Another approach = find popular items w/ users who are similar to a user we’re trying to build recommendations for + then apply kNN = **user-based approach**. Both approaches are reasonable, but 1 is almost always better than the other one in practice. 

If you have many more users than items (Netflix users vs. movies), you’ll save on computation + storage costs by using the item-based approach b/c you only need to compute similarities between all pairs of items. If you have many more items than users (often when just starting to collect data), probably want to try user-based approach. We’ll go w/ the item-based approach w/ the data available to participants in the R package recommendation contest on Kaggle = complete package installation record for approximately 50 R programmers. This = quite a small data set, but was sufficient
to start learning things about the popularity of different packages + their degree of similarity.

Winning entries for the contest all used kNN as part of their recommendation algorithm, though it was generally only 1 part of many for the winning algorithms. The other important algorithm incorporated into most winning solutions = **matrix factorization**. Anyone building PROD-level recommendation system should consider combining recommendations from both kNN + matrix factorization models. The best systems often combine kNN, matrix factorization, + other classifiers together into a **super-model** via **ensemble methods** (Ch 12, briefly).

In the R contest, participants were supposed to predict whether a user would install a package for which we had withheld the installation data by exploiting the fact that you knew which other R packages a user had installed. When making predictions about a new package, could look at packages most similar to it + check if they were installed. In other words, it was natural to use an item-based kNN approach for the contest.

1st, load the data set + inspect it a bit, then construct a similarity measure between packages. Raw data for the contest is not ideally suited for this purpose, so we’ve prepared a simpler data set in which each row describes a user-item pair + a third column states whether that user has the package installed or not.
```{r}
installations <- read.csv('data/installations.csv')
head(installations)
```

User 1 installed `abind` package, but not `AcceptanceSampling`. This raw info will give us a measure of similarity
between packages after we transform it into a different form. We’re going to convert **long form data** into a **wide form** of data where each row = a user + each column = to a package using `cast()` from `reshape`
```{r}
library("reshape")
user.pckg.mtx <- cast(installations, User ~ Package, value = "Installed")

# check 1st + 2nd cols = installations of abind and AcceptanceStalling
user.pckg.mtx[, 1]
```

Unfortunately, this operation has stored the user ID's in the 1st col. Make these the row names and remove this col.
```{r}
# make user IDs the row names + remove the redundant col
row.names(user.pckg.mtx) <- user.pckg.mtx[, 1]
user.pckg.mtx <- user.pckg.mtx[, -1]
head(user.pckg.mtx)
```

Now we have a proper user-package matrix that makes it trivial to compute similarity measures. For simplicity, use the correlation between columns as our measure of similarity via `cor()`
```{r}
suppressWarnings(similarities <- cor(user.pckg.mtx))
dim(similarities)
```

```{r}
# similarities/correlation between 1st package and itself, and 1st package and the 2nd package
similarities[1, 1]
similarities[1, 2]
```

Package 1 is perfectly similar to package 1 + somewhat dissimilar from package 2. But kNN doesn’t use similarities; it uses distances = need to translate similarities into distances w/ some clever algebra so that a similarity of 1 = distance of 0 + similarity of –1 = an infinite distance. The code we’ve
written here does this. If it’s not intuitively clear why it works, try to spend some time
thinking about how the algebra moves points around.
```{r}
# create distances such that a similarity of 1 = -log(1) which is 0, and similiary of -1 = -log(0), which is Inf
distances <- -log((similarities / 2) + 0.5)
```

We now have our distance measure + can start implementing kNN w/ `k = 25` for example purposes, but you’d want to try multiple values of k in PROD to see which works best. To make recommendations, estimate how likely a package is to be installed by simply counting how many of its neighbors are installed, then sort packages by this count + suggest packages w/ the highest score to users.
```{r}
get_knn <- function(p, distances, k = 25) {
  # for given package p, sort the distances from p of every other package (cols after p) and get top 25
  return(order(distances[p,])[2:(k + 1)])
}
```

Using the nearest neighbors, we get back, predict probability of installation by simply counting how many of the neighbors are installed:
```{r}
get_installation_prob <- function(user, pckg, user.pckg.mtx, distances, k = 25) {
  # get sorted distances of all packages from given package and return top 25
  neighbors <- get_knn(pckg, distances, k = k)
  # for each neighbor, get the install/not install value from user.pckg.mtx and get the mean of this install sum 
  return(mean(sapply(neighbors, function(neighbor) { user.pckg.mtx[user, neighbor]})))
}
get_installation_prob(1, 1, user.pckg.mtx, distances)
```

For user 1, package 1 has probability 0.76 = 76% of being installed. What we’d like to do is find packages most likely to be installed + to recommend those. Do this by looping over ALL packages, finding their installation probabilities, + displaying the most probable:
```{r}
get_most_probably_pckgs <- function(user, user.pckg.mtx, distances, k = 25) {
  # for the total # of packages in the matrix, get the installation probability of the package
  # then order their indices them from greatest to least
  return(order(sapply(1:ncol(user.pckg.mtx), 
                      function(package) {
                        get_installation_prob(user, package, user.pckg.mtx, distances, k = k)
                      }), decreasing = T))
}

# get most probable packages for user 1 to install
user <- 1
listings <- get_most_probably_pckgs(user, user.pckg.mtx, distances)

# get top 10 most probable package installations for user 1
colnames(user.pckg.mtx)[listings[1:10]]
```

1 of the things that’s nice about this approach is we can easily justify our predictions to end users. We can say that we recommended package P to a user U b/c he had already installed packages X, Y, + Z. This transparency can be a real virtue in some applications. Now that we’ve built a recommendation system using only similarity measures, it’s time to move on to building an engine in which we use social info in the form of network structures to make recommendations.