---
title: "Ch3_Ranking_InboxPriority"
author: "Steve Newns"
date: "November 13, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(tm)
library(magrittr)
library(plyr) # ddply
```

# How Do You Sort Something When You Don’t Know the Order?
**binary classification** = placing items into 1 of 2 types or classes + In many cases, we will be satisfied w/ an approach that can make such a distinction. 

But what if the items in one class are *not* created equally + we want to **rank** the items w/in a class? (say 1 email = most spammy + another = second most spammy, or want to distinguish among them in some other meaningful way?)

we not only want to filter spam but also wanted to place “more important” messages at the top of the queue = **generating rules** = a very common problem + increasingly common task in ML

Recommendation systems implicitly produce a ranking of products.
Ex: Netflix + Amazon recommendations are based on 2 kinds of data
<ul>
<li> the data pertaining to the inventory itself (TV has type (e.g., plasma, LCD, LED), manufacturer, price, + so on while movies have genre, cast, director, etc.) </li>
<li> the data related to the behavior of the customers </li>
</ul> 

For both types of data, the *features are well-identified* (know the labels for categorical data such as product type or movie genre) + *user-generated data is well structured* (form of purchase/rental records + explicit ratings).

B/c we usually have explicit examples of the outputs of interest when doing ranking,
this is supervised learning. For spam classification, we knew the terms associated w/ spam + ham messages, + we trained a classifier using that "recipe", a very simple problem where we were able to obtain relatively good classification results using a feature set w/ only a single element: email message terms.

For **ranking**, need to assign a unique **weight** to each item to **stratify** them in a finer way.

To sort something when you don’t already know its order (context = ordering emails by importance), reword the question in terms of the features available in the (email) data + how those features relate to an item's (email’s) priority

# Ordering Email Messages by Priority

*What makes an email important?": step back + think about what email is = a transaction-based medium that people send + receive over time. As such, in order to determine importance of an email, we need to focus on the transactions themselves. 

Unlike spam classification where we could use static info from all emails to determine type, to rank emails by importance, must focus on the *dynamics of the in-and-outbound transactions*.

Specifically, want to determine the **likelihood** a person will interact w/ a new email once it has been received = *Given a set of features we, how likely is the reader to perform an action on this email in the immediate future?*

The critical new dimension this problem incorporates is **time**. In transaction-based context, in order to rank things by importance, we need some concept of time. A natural way to use time to determine importance of an email = measure how long it takes a user to perform some action on an email.

Shorter the average time it given its set of features = the more important emails of that type may be. The *implicit assumption* in this model = more important emails will be acted on sooner. Intuitively, this makes sense. 

Before we can begin, however, we must determine *which features* in email are good proxy measures for priority.

# Priority Features of Email
Gmail's “priority inbox” = 2010 + several months after the priority inbox feature was
released, they published a paper, “The Learning Behind Gmail Priority Inbox,” which describes their strategy for designing the supervised learning approach + how to implement it at scale.

As mentioned, measuring time is critical + Google had the luxury of a long + detailed history of users’ interactions w/ email. Google priority inbox attempts to predict probability a user will perform some action on an email w/in a fixed # of seconds from delivery. The set of actions is large: reading, replying, labeling, etc., + delivery is not explicitly the time an email is received by the server, but the time it is delivered to a user

This is a relatively simple problem to state "What is probability a user will perform some actions, w/in our set of possible actions, between some minimum + maximum numbers of seconds, given a set of features for that email + the knowledge that the user has recently checked his email?"

Google incorporated a very large # of possible email features. Everyone has a different way of ordering priority of email, so given this variability in how users may evaluate the feature set, Google needed to incorporate multiple features. 

Google engineers explored various different types of email features. Google's long history of user interactions w/ Gmail afforded them a rich perspective into what actions users perform on emails + when.

We will again use the **SpamAssassin public corpus**, + though this data was distributed as a means of testing spam classification algorithms, it also contains a convenient timeline of a single user’s email. 

Given this single **thread**, we can repurpose the data set to design + test a priority email ranking system. Also, we will focus only on ham emails, so we'd know all messages are those a user would want in her inbox.

Before we can proceed, consider how our data differs from that of a full-detail email log + how that affects the features we'll be able to use in our algorithm. There were 4 categories proposed by Google --> determine how they might fit into the data we are using.

Most critical difference between a full-detail email log + what we have = we can only see messages received =  “flying half-blind” b/c we have no data on when + how a user responded to emails, or if a user was the originator of a thread. This is a significant limitation, + therefore the methods + algorithms used in this chapter should be considered *exercises only* + NOT examples of how enterprise priority inbox systems should be implemented. 

Given email is a transaction-based medium, social features will be paramount in assessing importance of an email. In our case, we see only 1/2 of a transaction. For a full-detail case, we'd  want to measure volume of interactions between a user + various email senders in order to determine which senders receive more immediate actions from the user. W/ our data, however,
we can measure only incoming volume, so we assume this 1-way volume is a good proxy for the type of social features we're attempting to extract from the data.

Clearly this is not ideal. Recall that we are only using ham messages. If one receives a large volume of ham email messages from a certain address, it may be that the user has a strong social connection to the sender. Alternatively, it may be the case that the user is signed up to a mailing list w/ a high volume + would prefer these emails not receive a high priority. This is exactly the reason why we must incorporate other features to balance these types of info when developing our ranking system.

1 problem w/ looking only at volume of messages from a given address = the **temporal component** is protracted. B/c our data set is *static* compared to a fully detailed email log, we must partition the data into **temporal segments** + measure volume over these periods to get a better understanding of the temporal dynamics.

We will simply order all of messages chronologically, then split the set in half. The 1st half = used to train the ranking algorithm, + the 2nd half = used to test. As such, message volume from each email address over the entire time period covered by the training data will be used to train our ranker’s social feature.

Given the nature of our data, this may be a good start, but we will need to achieve a deeper understanding if we hope to rank messages more accurately. 1 way to partition data to gain a more granular view of these dynamics = identify conversation threads + measure intra-thread activity. (To ID threads, we can borrow techniques used by other email clients + match message subjects w/ key thread terms, such as “RE:”.) 

Although we don't know what actions a user is taking on a thread, the assumption here = if it is very active, it is likely to be more important. 

By compressing temporal partitions into these small pieces, we can get a much more accurate proxy for the thread features we need to model email priority.

Next, there are many content features we could extract from emails to add to a feature set. Keep things relatively simple by extending text-mining techniques to say if there are common terms in subjects + bodies of emails received by a user, future emails that contain these terms in the subject + body may be more important than those that do not (a common technique mentioned briefly in the description of Google’s priority inbox). 

By adding content features based on terms for both the email subject + body, we will encounter an interesting problem of **weighting**. Typically, there are considerably fewer terms in an email’s subject than the body + therefore, we should not weight the relative importance of common terms in these 2 features equally.

There are also many features used in enterprise distributed priority inbox implementations that are simply unavailable to us in this exercise (blind to much of the social feature set, + therefore must use proxies to measure these interactions, + there are many user actions we do not even have the ability to approximate, like labeling or moving emails)

Although this is a weakness to the approach described here when compared to those that use full-detail email logs, the fact that they are missing will not affect our results.

We now have a basic blueprint for the feature set we'll use to create our email ranking system, beginning by ordering messages chronologically (b/c in this case much of what we're interested in predicting is contained in the temporal dimension) + the 1st half = train, 2nd half = test sets of ranker. Next, we have 4 features to use during training:
<ul>
<li> a proxy for the social feature = measures volume of messages from a given user in the training data. </li>
<li> attempt to compress temporal measurements by looking for threads + ranking active threads higher than inactive ones. </li>
<li> 2 content features based on frequent terms in email subjects + message bodies. </li>
</ul>

# Writing a Priority Inbox

Before we can get to the sexy parts of ML, we need to get our hands dirty hacking at the data to split, pull, + parse it until it’s in a shape fit for analysis. Also, here, we are not
concerned w/ type of email, but rather w/ how each should be ranked in terms of priority. Therefore, we use ham = largest data set. B/c we may safely assume a user would not distinguish among emails in this way when determining which emails have a higher priority,
there is no reason to carry this info into our ranking system (i.e. assume users are not acting on emails that were harder to ID as ham than those that were easy). Instead, we
want to be able to *learn as much about our features sets from a single user’s emails*,

```{r}
easyham_path <- "./data/easy_ham/"
```

Next, create a series of functions to work together to parse each email into the feature set needed = need to extract 4 elements from each email message: sender’s address, date received,
subject, + message body.

# Functions for Extracting the Feature Set

Task of constructing the training data is one of **rectangularization** = shape the email data set to fit into a usable feature set. Features extracted = columns, + each row = unique values from a single email. 

```{r}
# function to parse email file for features needed for ranking
parse_email <- function(path) {
  full_msg <- msg_full(path)
  date <- get_date(full_msg)
  from <- get_sender(full_msg)
  subj <- get_subject(full_msg)
  msg <- get_msg(full_msg)
  
  return(c(date, from, subj, msg, path))
}
```

`parse.email()` calls a series of **helper functions** that extract the appropriate data from each message + then orders these elements into a single vector = row of data 

The process of turning each email message into these vectors requires some classic text hacking.
```{r}
# helper function to grab message text from email file
msg_full <- function(path) {
  con <- file(path, open="rt", encoding="latin1") # open connection
    msg <- readLines(con) # read file contents into vector
  close(con) # close connection
 
  return(msg) # return vector of file lines
}
```

We return the entire email as a character vector + will write separate functions to work on this vector to extract necessary data (fight way through the data to extract as much usable info as possible from the email messages) + organize them in a uniform way

1st = relatively easy task of extracting sender’s address by IDing the text patterns in the email messages that ID the data we're looking for. 

........................................................
X-Sender: fortean3@pop3.easynet.co.uk (Unverified)
Message-Id: <p05100300ba138e802c7d@[194.154.104.171]>
To: Yahoogroups Forteana <zzzzteana@yahoogroups.com>
From: Joe McNally <joe@flaneur.org.uk>
X-Yahoo-Profile: wolf_solent23
MIME-Version: 1.0
Mailing-List: list zzzzteana@yahoogroups.com; contact
 forteana-owner@yahoogroups.com
........................................................

We need to ID the line in each message that contains sender email address, which always has the term “From: ” by searching the character vector of each email to ID thevcorrect element. However, there is variation in how the address is written.

Sometimes the address is encapsulated in angled brackets (From: Joe McNally <joe@flaneur.org.uk>), whereas in others it is not (From: paul-bayes@svensson.org (Paul Svensson)), but it always contains the name. For that reason, write a `get_from()` function that uses RegEx's to extract the data for this feature.

```{r}
# function to ID and extract sender email address
get_sender <- function(msg_vec) {
  from <- msg_vec[grepl(pattern="From: ", x=msg_vec)] # search for pattern + return line that matches
  from <- strsplit(from, '[":<> ]')[[1]] # split on colons, <>'s, + spaces to return list of components
  from <- from[which(from !="" & from !=" ")] # remove the empty spaces
  return(from[grepl("@", from)][1]) # find list elements that contains "@: and return it
}
```
```{r,echo=F}
## test
#msg_vec <- c("X-Sender: fortean3@pop3.easynet.co.uk (Unverified)","Message-Id: <p05100300ba138e802c7d@[194.154.104.171]>","To: Yahoogroups Forteana <zzzzteana@yahoogroups.com>","From: Joe McNally <joe@flaneur.org.uk>","X-Yahoo-Profile: wolf_solent23","MIME-Version: 1.0","Subject: [Spambayes] Corpus Collection (Was: Re: Deployment","Mailing-List: list zzzzteana@yahoogroups.com; contact","forteana-owner@yahoogroups.com")
#(from <- msg_vec[grepl(pattern = "From: ", x = msg_vec)])
#(from <- strsplit(from, '[":<> ]')[[1]])
#(from <- from[which(from !="" & from !=" ")])
#(from[grepl("@", from)][1])
```

`grepl()`, rather than returning vector indices like `grep()`, returns a vector of the same length as the input w/ Boolean values indicating where a pattern was matched in the vector. After getting the correct line, in `strsplit()`  we split the character element into a list by colons, <>'s, and empty spaces. This pattern will always put the address as the 1st element in the list, so we can pull that from the list w/ [[1]]. 

Because of the variation in the pattern, however, it will also add empty elements to this vector. In order to return only the email address itself, we ignore those empty elements + look for the remaining element containing “@”. This parses 1/4 of the data needed to generate training data.
```{r}
get_msg <- function(msg_vec) {
  msg <- msg_vec[seq(which(msg_vec == "")[1] + 1, length(msg_vec), 1)] # start email body extraction after 1st line break to end of msg
  return(paste(msg, collapse="\n")) # collapse into single vector
}

get_subject <- function(msg_vec) {
  subj <- msg_vec[grepl("Subject: ", msg_vec)] # find line that starts w/ "Subject" and extract it
  
  if(length(subj) > 0) { # if we found a subject, split into 2 parts that returns as list
    return(strsplit(subj, "Subject: ")[[1]][2]) # get 1st list element from result + extract 2nd part (actual subject line) of that list element
    } else {
      return("") # return empty string b/c special values (integer(0),character(0)) have length 0 from grepl
    }
}
```
```{r}
##test 
#(subj <- msg_vec[grepl("Subject: ", msg_vec)])
#(strsplit(subj, "Subject: "))
```

The final element, the date + time a message was received, will cause us to suffer the most. This
field will be difficult to work w/ for 2 reasons. 
<ul>
<li> Dealing w/ dates is almost always painful, as different programming languages often have slightly different ways of thinking about time. Eventually we want to convert date strings into **POSIX** date objects in order to sort data chronologically. To do this, we need a common character representation of the dates, which leads directly to the second reason for our suffering. </li>
<li> There is considerable variation w/in the SpamAssassin public corpus in how receival dates + times of messages are represented. There are many things we need to be cognizant of when extracting date + time info from each email. </li>
  <li> Data we want to extract is always IDed by “Date: ”; however, there are many traps in using this pattern that we must be mindful of. Sometimes there will be multiple lines that match this pattern, some lines may be partial matches, + in either case the data on these lines can be conflicting.  
  <li> Dates + times are not stored in a uniform way across all emails (extraneous GMT offsets + other types of labeling info) 
  <li> Format for the date + time can be totally different </li>
</ul>

All of this info will be critical in getting the data into a uniform + workable form. For now, we need to focus only on extracting the date + time info *w/out the extraneous offset info* by defining a `get.date()` function.

Once we have all of the date/time strings, we will need to deal w/ converting conflicting date/time formats to a uniform POSIX object, but this will not be handled by `get.date`.
```{r}
get_date <- function(msg_vec) {
  date_grep <- grepl("^Date: ", msg_vec) # search for pattern of "Date" at the start of line (boolean)
  date <- msg_vec[which(date_grep == TRUE)[1]] # get 1st line w/ occurence of "Date" at start
  date <- strsplit(date, "\\+|\\-|: ")[[1]][2] # split on +, -, or : into a list + grab 2nd element
  date <- gsub("^\\s+|\\s+$", "", date) # sub in any leading/trailing whitespace with "\"
  return(strtrim(date, 25)) # remove extraneous info
  # standard data/time string = 25 characters = anything over this is extraneous.
}
```
```{r}
##test 
#msg_vec <- c("Email #1","........................................................","Date: Thu, 22 Aug 2002 18:26:25 +0700","Date: Wed, 21 Aug 2002 10:54:46 -0500","From: Chris Garrigues","lt;cwg-dated-1030377287.06fa6d@DeepEddy.Comgt;","Message-ID: lt;1029945287.4797.TMDA@deepeddy.vircio.comgt;")

#(date.grep <- grepl("^Date: ", msg_vec))
#(date <- msg_vec[which(date.grep == TRUE)[1]])
#(date <- strsplit(date, "\\+|\\-|: "))
#(date <- gsub("^\\s+|\\s+$", "", date))
#(strtrim(date, 25))
```

We've now transformed this amorphous set of emails into a structured rectangle suitable for training a ranking algorithm. Now we will create a vector w/ all of “easy ham” files, remove the extra “cmds” file from the vector, + then use `lapply` to apply `parse.email` with all its inner helper functions email file. 

Next, we need to convert the list of vectors returned by `lapply` into a matrix via `do.call` w/ `rbind`, then convert this to a data frame of character vectors, + then set column names. 

```{r, message=F,warning=F}
# get all file + dir names in input dir
easyham_docs <- dir(easyham_path)

# ignore cmds files (long list of UNIX base commands to move files in that dir)
easyham_docs <- easyham_docs[which(easyham_docs != "cmds")]

# parse text from each email + return in list
# use anonymous function to concatenate file name + path
easyham_parse <- lapply(easyham_docs, function(x) parse_email(paste(easyham_path, x, sep="")))

# transform returned list to matrix via do.call(function,list), then transform to dataframe
ehparse_mtrx <- do.call(rbind, easyham_parse)
allparse_df <- data.frame(ehparse_mtrx, stringsAsFactors=FALSE)
names(allparse_df) <- c("Date", "Sender_EMail", "Subject", "Message", "Path")
head(allparse_df)
```

Before we can proceed to creating a **weighting scheme** from this data, however, there is still some remaining housekeeping.

Our 1st trial w/ extracting dates was simply isolating the text, now we need to take that text + convert it into POSIX objects that can be compared logically b/c we need to sort emails chronologically. 

Temporal differences among observed features can be used to infer importance, so *character* representation of dates + times will not suffice. There are 2 variations on the date format (e.g. “Wed, 04 Dec 2002 11:36:32,” and “04 Dec 2002 11:49:23”). To convert these into POSIX, we need to use `strptime`, but pass it 2 different formats to make the conversion. 

Each element of these strings matches a specific POSIX format element, so we will need to specify conversion strings that match these variants.
```{r}
pattern1 <- "%a, %d %b %Y %H:%M:%S"
pattern2 <- "%d %b %Y %H:%M:%S"
```

R uses the standard POSIX date/time format strings to make these conversions (many options for these strings + `?strptime` has all of options). Here we will be using only a select few, but understanding them in greater depth will be very useful for working w/ dates + times in R.

We need to convert strings in `Date` column to the 2 different POSIX formats *separately*, then recombine them back into the data frame to complete the conversion via a `date.converter` function to take 2 different POSIX patterns + a character vector of date strings

When the pattern passed to `strptime` does not match the string passed to it, default behavior = return NA. We can use this to recombine the converted character vectors by replacing elements w/ NA from the 1st conversion w/ those from the 2nd + b/c we know there are only 2 patterns present in the data, the result will be a single vector w/ all date strings converted to POSIX objects.

```{r}
date_converter <- function(dates, pattern1, pattern2) {
  pattern1_convert <- as.POSIXct(strptime(dates, pattern1))
  pattern2_convert <- as.POSIXct(strptime(dates, pattern2))
  
  # replace NAs in pattern1_convert with values w/ same index from pattern2_convert
  pattern1_convert[is.na(pattern1_convert)] <- pattern2_convert[is.na(pattern1_convert)]
  
  return(pattern1_convert)
}

allparse_df$Date <- date_converter(allparse_df$Date, pattern1, pattern2)
head(allparse_df)
#class(allparse_df$Date)
```

The final bit of cleanup is to convert the character vectors in `Subject` + `From email` columns to all lowercase to ensure all data entries are as uniform as possible before moving into the training phase. 
```{r}
allparse_df %<>%
  mutate(Subject = tolower(Subject),
         Sender_EMail = tolower(Sender_EMail))
head(allparse_df)
```

Next, we sort the data chronologically using a combination of `with` + `order`, which returns a vector of the element *indices* in ascending chronological order. 
```{r}
priority_df <- allparse_df[with(allparse_df, order(Date)),]
```

Then, to order the data frame by these indices, we need to reference the elements of `allparse_df` in that order, + add the final comma before closing the square bracket so all columns are sorted this way.

Finally, store the 1st half of the chronologically sorted data frame as priority_train to use to train our ranker. Later, we will use the 2nd half to test the ranker. 
```{r}
priority_train <- priority_df[1:(round(nrow(priority_df) / 2)),]
head(priority_train)
tail(priority_train)
```

W/ the data fully formed, we are ready to begin designing our ranking scheme. Given our feature set, 1 way to proceed is to define **weights** for each observed feature in the training data.

# Creating a Weighting Scheme for Ranking

Most peoples' email activity crudely adheres to the 80/20 cliche (80% of activity conducted w/ 20% of the total # of people in your address book). We need to devise a scheme for weighting the observation of certain features in data, but b/c of potential differences in scale among these observations, we cannot compare their absolute values directly. 

1 of the features being added to our ranker = an approximation of social interaction based on volume of emails received from each address in our training data.

Functions in `plyr` = used to chop data into smaller squares/cubes so we can operate over pieces all at once (very similar to Map-Reduce paradigm in many large-scale data analysis environments). We find all columns w/ matching addresses in `Sender_EMail` column + count them any column for the creation of `Freq` (all columns matching our criteria will have the same length). 
```{r}
# in training data, 
sender_weight <- ddply(priority_train, .(Sender_EMail), summarise, Freq=length(Subject))
#sender_weight_rank <- allparse_df[with(sender_weight, order(Freq)),]
#sender_weight <- sender_weight_rank[1:(round(nrow(sender_weight_rank) / 2)),]
sender_weight <- sender_weight[order(sender_weight$Freq,decreasing = F),]
head(sender_weight[order(sender_weight$Freq,decreasing = T),])
```

To get a better sense of the scale of this data, plot the results w/ a bar chart of volume of emails from users who have sent more than 6 emails. 

```{r}
sender_weight_6Plus <- subset(sender_weight, Freq > 6)
from_scales <- ggplot(sender_weight_6Plus) +
  geom_rect(aes(xmin = 1:nrow(sender_weight_6Plus) - 0.5,
                xmax = 1:nrow(sender_weight_6Plus) + 0.5,
                ymin = 0,
                ymax = Freq,
                fill = "lightgrey",
                color = "darkblue")) +
  scale_x_continuous(breaks = 1:nrow(sender_weight_6Plus), labels = sender_weight_6Plus$Sender_EMail) +
  coord_flip() +
  scale_fill_manual(values = c("lightgrey" = "lightgrey"), guide = "none") +
  scale_color_manual(values = c("darkblue" = "darkblue"), guide = "none") +
  ylab("Number of Emails Received (truncated at 6)") +
  xlab("Sender Address") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 5, hjust = 1))
from_scales
```

See how quickly the data scales. The top emailer, tim.one@comcast.ent, has sent 45 messages, about 15 times more emails than the average person in our training data! Tim is pretty unique + there are only a few other senders near his level, + frequency drops off very quickly after them. 

We need to weight an observation from an average person in training data w/out skewing that value to account for outliers like our top emailers w/ **log**.

# A log-weighting scheme

Need to make the numerical relationship among units in our feature set less extreme. If we compare absolute frequency counts, an email from Tim will be weighted as 15X more important than email from an average sender = very problematic b/c we want to establish a threshold for being either a priority message or not based on the range of weight values produced by our ranker at the learning stage. 

W/ such extreme skewness, our threshold will be either far too low or far too high, so we need to rescale the units to account for the nature of our data.

A **logarithm** = function that returns the exponent value that would satisfy an equation where some base # being raised to that exponent = the # given to the logarithmic function. The **base value** in a log-transformation is critical. We are familiar w/ thinking of things in base 2/binary = solves an equation for the exponent value where the input value = 2 raised to that exponent. Ex: Transforming 16 by log base-two = 4 b/c base-TWO raised to the 4th power = 16. In essence, we are “reversing” an exponential, so these types of transformations work best when the data fits such a function. 

2 most common log-transformations = **natural log** + **log base-10** transformation. 

For ln, base = e, an irrational constant (like pi) = ~2.718. Rates of change by this constant are very often observed in nature + the derivation can be done geometrically as a function of the angles inside a circle. A natural log spiral can be observed in many naturally occurring phenomenon (interior structure of a nautilus shell, spiraling winds of a hurricane/tornado, scattering of interstellar particles in our galaxy, many professional camera settings’ apertures = set to vary by natural logs). Given the intimate relationship between this value + many naturally occurring phenomena, it is a great function for rescaling social data that is exponential. 

Alternatively, log base-10 transformations (**log10**) replaces e w/ a 10 + will reduce large values to much smaller ones than the natural log (log10 transformation of 1k = 3 b/c 10 raised to the 3rd = 1k, whereas ln = ~6.9). Therefore, it makes sense to use a log base-10 transformation when data scales by
a very large exponent.

The ways in which both of these options would transform our email volume data are illustrated here:
```{r}
# Log weight scheme, very simple but effective
sender_weight_log <- transform(sender_weight,
                         Weight = log(Freq + 1),
                         log10Weight = log10(Freq + 1))

(sender_rescaled <- ggplot(sender_weight_log, aes(x = 1:nrow(sender_weight_log))) +
  geom_line(aes(y = Weight, linetype = "ln")) +
  geom_line(aes(y = log10Weight, linetype = "log10")) +
  geom_line(aes(y = Freq, linetype = "Absolute")) +
  scale_linetype_manual(values = c("ln" = 1,
                                   "log10" = 2,
                                   "Absolute" = 3),
                        name = "Scaling") +
  xlab("") +
  ylab("Number of emails Receieved") +
  theme_bw() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank()))
```

In this figure, we can see that the volume of emails sent by the
users in the training data follows a fairly steep exponential. By transforming those values
by the natural log and log base-10, we significantly flatten out that line. As we know,
the log base-10 transforms the values substantially, whereas the natural log still provides
some variation that will allow us to pull out meaningful weights from this training
data. For this reason, we will use the natural log to define the weight for our email
volume feature.
110 | Chapter 4: Ranking: Priority Inbox
As we have done here and as we explained in detail in Chapter 2, it is
always a good idea to explore your data visually as you are working
through any machine learning problem. We want to know how all of
the observations in our feature set relate to one another in order to make
the best predictions. Often the best way to do this is through data visualization.
from.weight <- transform(from.weight, Weight=log(Freq + 1))
Finally, recall from grade school mathematics your rules for exponents. Anything raised
to zero always equals one. This is very important to keep in mind when using
log-transformation in a weighting scheme because any observation equal to one will be
Figure 4-3. A natural-log spiral, often observed in nature
Writing a Priority Inbox | 111
transformed to zero. This is problematic in a weighting scheme because multiplying
other weights with zero will zero out the entire value. To avoid this, we always add one
to all observations before taking logs.
There is actually a function in R called log1p that computes log(1 +
p), but for the purposes of learning and being explicit, we will do this
addition “by hand.”
Given the rescaling, this does not affect our results, and it keeps all weights greater than
zero. In this case, we are using the default base value for the log function, which is the
natural log.
For our purposes we will never have an observation in our feature set
that is equal to zero, because we are counting things. If there are no
observations of something, then it simply doesn’t enter our training
data. In some cases, however, this will not be true, and you may have
zero observations in your data. The log of zero is undefined, and if you
try to compute it in R, it will return the special value -Inf. Often, having
instances of -Inf in your data will cause things to blow up.
