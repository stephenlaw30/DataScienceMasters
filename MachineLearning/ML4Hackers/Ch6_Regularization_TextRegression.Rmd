---
title: "Ch6_Regularization_TextRegression"
author: "Steve Newns"
date: "November 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
library(caTools) #s plit data
library(glmnet) # regularization
library(tm)
library(Hmisc) # plotting final errors
```

# Nonlinear Relationships Between Columns: Beyond Straight

Can also use linear regression to capture relationships that aren’t well-described by a straight line.

We make systematic errors in predictions if we use a straight line for these data. This is easiest to see in a residuals plot that shows all the structure of the original data set, as none of the structure is captured by the default linear regression model.

Use `geom_smooth` without any `method` argument to fit a more complex statistical model called a **Generalized Additive Model (GAM)** that
provides a smooth, nonlinear representation of the structure in our data:
```{r}
set.seed(1)
x <- seq(-10, 10, by = 0.01)
y <- 1 - x**2 + rnorm(length(x), 0, 5)

ggplot(data.frame(X = x, Y = y), aes(X, Y)) + 
  geom_point() +
  geom_smooth(se = FALSE)
```

We want to fit a curved line instead of a straight line to this data set. The subtleties of linear regression come up when we fit some sort of curved line to our data?. Linear regression can only fit lines in the input, but you can create *new inputs* that are nonlinear functions of your original inputs. For example,
you can use the following code in R to 
```{r}
# produce a new input based on the raw inputx
x.squared <- x ^ 2
# plot y against new polynomial x to see a very different shape

ggplot(data.frame(XSquared = x.squared, Y = y), aes(XSquared, Y)) + 
  geom_point() +
  geom_smooth(method = 'lm', se = F)
```

This gives us a fit that looks exactly like a straight line b/c we’ve transformed our original nonlinear problem into a new problem in which the relationship between the 2 inputs satisfies the linearity assumption of linear regression. Replacing a complicated nonlinear problem w/ a simpler linear one using a transformation of the inputs comes up again + again in ML. This intuition is the essence of the **kernel trick**.

To see how much this simple squaring transformation improves prediction quality, compare R2 values for linear regressions using `x` + `x.squared`
```{r}
summary(lm(y ~ x))$r.squared
summary(lm(y ~ x.squared))$r.squared
```

We’ve gone from accounting for 0% of the variance to accounting for 97%, a pretty huge jump for such a simple change in our model. In general, we might wonder how much more predictive power we can get from using more expressive models w/ more complicated shapes than lines. 
It turns out you can capture essentially any type of relationship that might exist between 2 variables using more complex curved shapes, possibly via **polynomial regression**, but the flexibility polynomial regression provides is not a purely good thing, b/c it encourages us to *mimic the noise* in our data w/ our fitted model, rather than just the *true underlying pattern* we want to uncover.

# Introducing Polynomial Regression

Polynomial regression in R = `poly()`. Build up from a simple example + see what happens as we give our model more expressive power to mimic the structure in our data.

Use a sine wave to create a data set in which the relationship between x + y could never be described by a simple line.
```{r}
set.seed(1)
x <- seq(0, 1, by = 0.01) # 100 points from 0 to 1
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)

df <- data.frame(X = x, Y = y)
ggplot(df, aes(X,Y)) +
  geom_point()
```

Just looking at this data, it’s clear using a simple linear regression model won’t work. But let’s run a simple linear model and see how it performs.
```{r}
summary(lm(Y ~ X, data = df))
```
Surprisingly, we’re able to explain 60% of the variance in this data set using a linear model despite the fact we know the naive linear regression model is a bad model of wave data. We also know a good model should be able to explain more than 90% of the variance in this data set, but we’d still like to figure out what the linear model did to produce such a good fit to the data by plotting the results of fitting a linear regression by forcing `geom_smooth` to use a linear model w/ `method = 'lm'`
```{r}
ggplot(data.frame(X = x, Y = y), aes(x = X, y = Y)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

The lm finds a way to capture half of the sine wave’s structure using a downward-sloping line.This is not a great strategy b/c you’re systematically neglecting parts of the data not described by that downward-sloping line. If the sine wave were extended through another period, R2 for this model would suddenly drop closer and closer to 0%.

Conclude the default linear regression model overfits the quirks of our specific data set + fails to find its true underlying wave structure. Give the linear regression algorithm more inputs to work with to see if it find a structure that’s actually a wave.

```{r}
# add both 2nd + 3rd power of x to data for more wiggle room
df %<>%
  mutate(X2 = X**2,
         X3 = X**3)
summary(lm(Y ~ X + X2 + X3, df))
```

By adding 2 more inputs, we went from an R2 = 60% to an R2 = 97%. That’s a huge increase. In principle, there’s no reason why we can’t follow this logic out + keep adding more powers of X. But as we add more powers, we’ll eventually start to have more inputs than DPs. That’s usually worrisome b/c it means we could, in principle, fit our data perfectly. A more subtle problem w/ this strategy will present itself before then = new columns are so similar in value to the original columns `lm` will simply stop working =  a **singularity**

```{r}
df %<>%
  mutate(X4 = X**4,
         X5 = X**5,
         X6 = X**6,
         X7 = X**7,
         X8 = X**8,
         X9 = X**9,
         X10 = X**10,
         X11 = X**11,
         X12 = X**12,
         X13 = X**13,
         X14 = X**14,
         X15 = X**15)
summary(lm(Y ~ X + X2 + X3 + X4 + X5 + X6  +X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15, df))
```

The problem here = new columns are so correlated W/ the old columns that the linear regression algorithm breaks down + can’t find coefficients for all of the columns separately. Thankfully, there's a solution to this problem that can be found in the mathematical literature: instead of naively adding simple powers of X, add more complicated variants of x that work like successive powers of x *but aren’t correlated with each other in the way x + x^2 are* =  **orthogonal (uncorrelated) polynomials**

Run `lm` w/ the output from `poly` to get proper coefficients for all 14 powers of X
```{r}
# generate orthogonal polynomials
summary(lm(Y ~ poly(X, degree = 14), df))
```

In general, `poly` gives you a lot of expressive power + captures a huge variety of complicated shapes in data. That isn’t necessarily a good thing. Added power provided can be a source of trouble --> look at the shape of the models `poly` generates as you increase `degree`
```{r}
poly.fit <- lm(Y ~ poly(X, degree = 1), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
plot1 <- ggplot(df, aes(x = X, y = PredictedY)) +
  geom_point() +
  geom_line()

poly.fit <- lm(Y ~ poly(X, degree = 3), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
plot3 <- ggplot(df, aes(x = X, y = PredictedY)) +
  geom_point() +
  geom_line()
poly.fit <- lm(Y ~ poly(X, degree = 5), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
plot5 <- ggplot(df, aes(x = X, y = PredictedY)) +
  geom_point() +
  geom_line()
poly.fit <- lm(Y ~ poly(X, degree = 25), data = df)
df <- transform(df, PredictedY = predict(poly.fit))
plot25 <- ggplot(df, aes(x = X, y = PredictedY)) +
  geom_point() +
  geom_line()

gridExtra::grid.arrange(plot1,plot3,plot5,plot25, ncol = 2, nrow = 2)
```

We can continue this process indefinitely, but looking at the predicted values for our model makes it clear that eventually the shape we’re fitting doesn’t resemble a wave anymore + starts to become distorted by kinks + spikes = a model more powerful than the data can support = **overfitting.** 

As data grows in observations, we can use more powerful models. But for any specific data set, there are always some models that are too powerful. To stop this + get a better sense of what’s going to go wrong if we give ourselves enough rope to hang ourselves with, use a  mixture of **cross-validation** and **regularization**, 2 of the most important tools in the entire ML toolkit.

# Methods for Preventing Overfitting

**Overfit** = model matches part of the noise in a data set rather than the true underlying signal. If we don’t know the truth, how can we tell we’re getting further away from it rather than closer? ==> *Make clear what we mean by truth*. A predictive model is close to the truth when its predictions about future data are accurate, but we only have data from the past. We can simulate what it would be like to have access to future data by splitting up past data into 2 parts.

**Cross-validation** = ability to simulate testing a model on future data by ignoring part of historical data during model-fitting process + arguably, an instantiation of the scientific method: (1) formulate hypothesis, (2) gather data, (3) test it. There’s just a bit of sleight of hand b/c we don’t formulate a hypothesis based on existing data + then go out + gather more data + instea ignore part of our data while formulating our hypotheses so we can magically rediscover that missing data when it comes time to test our predictions.

Toy example to select a degree for polynomial regression for the sine wave data
```{r}
set.seed(1)
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
df <- data.frame(x = x, y = x)
```


Split the data exactly in half. For some applications, it’s better to use more data for the training set (80%) + less for the test set (20%) b/c the more data you have during model fitting, the better the fitted model will tend to be. As always, YMMV, so experiment when facing any real-world problem. 
```{r}
#spl <- sample.split(df$y, SplitRatio = 0.5)

#training <- df %>% subset(spl == TRUE)
#test <- df %>%  subset(spl == FALSE)

n <- length(x)
indices <- sort(sample(1:n, round(0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
```

Good idea to randomly split data when making a training set/test sets b/c you don’t want the training + tests set to differ systematically

Test various degrees for our polynomial regression to see which works best + use RMSE to measure performance. 

```{r}
# create a special rmse function:
rmse <- function(y, h)
{
  return(sqrt(mean((y - h)**2)))
}

# loop over a set of polynomial degrees from 1 - 12:
performance <- data.frame()
for (d in 1:12)
{
  # fit polynomial linear regression for each degree from 1-12 using training data
  poly.fit <- lm(Y ~ poly(X, degree = d), data = training.df)
  
  # get performance of fitted model on training data
  performance <- rbind(performance,
                        data.frame(Degree = d,
                                   Data = 'Training',
                                   RMSE = rmse(training.df$Y, predict(poly.fit))))
  # get performance of fitted model on test data
  performance <- rbind(performance,
                        data.frame(Degree = d,
                                   Data = 'Test',
                                   RMSE = rmse(test.df$Y, predict(poly.fit, newdata = test.df))))
}
```

There are often many ways to accomplish the same data manipulation task above + rarely is a loop of this nature the most efficient. We use it here b/c the data is quite small + it allows the process of the algorithm to be understood most clearly

```{r}
# plot performance of the polynomial regression models for all the degrees tried:
ggplot(performance, aes(x = Degree, y = RMSE, linetype = Data)) +
  geom_point() +
  geom_line()
```

It' clear using a degree in the middle of the range gives us best performance on test data, as it **underfits** w/ degrees as low as 1 or 2 where model doesn’t capture the real pattern in the data + very poor predictive performance on both the training + test data. For degrees 11 and 12, predictions for the model start to get worse again on the test data b/c the model is becoming too complex and noisy, + fits quirks in the training data that aren’t present in the test data. Notice our performance on training + test sets start to diverge as degrees increase + training set error rate keeps going down, but test set performance starts going up = model doesn’t generalize to any data beyond the specific points it was trained on = overfit.

Plot lets us know how to set the degree of our regression to get the best performance from our model. This intermediate point would be very hard to discover without using CV. Another approach to preventing overfitting = **regularization** = quite different from CV in spirit, even though we’ll end up using CV to provide a demonstration that regularization is able to prevent overfitting. We’ll also have to use CV to calibrate our regularization algorithm, so the 2 ideas will ultimately become intimately connected.

# Preventing Overfitting with Regularization

1 approach when working W/ polynomial regression  = saying models are more complex when the degree is larger, but that doesn’t help us for linear regression in general

An alternative measure of complexit = a model is complicated when coefficients are large 9 (1y ~ 5x + 21 = more complicated than 1y ~ 3x + 21). Fit a model using `lm` + measure its complexity by summing over the values returned by `coef`
```{r}
lm.fit <- lm(y ~ x)
(model.complexity <- sum(coef(lm.fit) ^ 2))
```
We’ve squared the coefficients before summing so they don’t cancel (**L2 norm**). An alternative approach to squaring the coefficients is to tak absolute value of the coefficients instead (**L1 norm**)

```{r}
lm.fit <- lm(y ~ x)
(l2.model.complexity <- sum(coef(lm.fit) ^ 2))
(l1.model.complexity <- sum(abs(coef(lm.fit))))
```

Use this measure of complexity to force a model to be simpler during model fitting (Chapter 7), but the big idea for the time being is we trade off making the model fit the training data as well as possible against a measure of the model’s complexity. B/c we make a trade-off between fitting the data + model complexity, we’ll end up selecting a simpler model that fits worse over a more complex model that fits better. This ultimately prevents overfitting by restraining our model from matching noise in the training data we use to fit it.

`glmnet` packag provides `glmnet()` that fits linear models using regularization.
```{r}
set.seed(1)
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)

# convert vector copy of x to a matrix
x <- as.matrix(cbind(x, x))

# call glmnet in reverse order than w/ lm to create GLM w/ lasso or elasticnet regularization
glmnet(x, y)
```

We get back an entire set of possible regularizations of the regression you’ve asked it to perform. At the top of the list = strongest regularization `glmnet` performed/calculated, + at the bottom = weakest. Each row contains 3 columns. The 1st, `Df` tells you how many coefficients in the model ended up being nonzero (does not include intercept term b/c we don’t penalize for its size). Knowing the # of nonzero coefficients is useful b/c many people would like to be able to assert that only **a few inputs really matter** + we can assert this more confidently if the model performs well even when assigning zero weight to many inputs. When the majority of the inputs to a statistical model are assigned "0"" coefficients, the model is **sparse**. Developing tools for promoting **sparsity** in statistical models is a major topic in contemporary ML research.

The 2nd column, `%Dev`, is essentially the R2 for each model. For the top row, it’s 0% b/c you have a 0 coefficient for the 1 predictor + therefore can’t get better performance than just using a constant intercept. For the bottom row, `Dev` = 59%, the value you’d get from using `lm` b/c `lm` doesn’t do any regularization. In between the 2 extremes of regularizing are values from 9% to 58%.

`Lambda` = most important piece of info for regularization = parameter of the regularization algorithm that controls how complex the model fit is allowed to be. B/c it controls the values you’ll eventually get for the main parameters of the model, **Lambda** = a **hyperparameter** (Ch 7). When Lambda is very large, you penalize your model very heavily for being complex + this penalization pushes all coefficients toward zero. When Lambda is very small, you do NOT penalize your model much at all. At the farthest end of this spectrum toward weaker regularization, set Lambda = 0 + get results like those from an unregularized linear regression like from `lm`.

To find optimal value for Lambda that gives the best possible model, employ CV as part of the process of working w/ regularization. Instead of playing w/ the degree of a polynomial regression, set the degree to a high value right at the start + then fit the model w/ different values for Lambda on a training set and see how it performs on a held-out/out-of-sample test set. After doing this for many values of Lambda, we would be able to see which value gives us best performance on the test data.

MUST assess quality of a regularization on held-out test data b/c increasing the strength of regularization can only worsen performance on the training data, so there is literally 0 info you can learn from looking at performance on the training data.

```{r}
# create training + test data
set.seed(1)
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
n <- length(x)
indices <- sort(sample(1:n, round(0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
df <- data.frame(X = x, Y = y)
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)

# function to calculate RMSE
rmse <- function(y, h)
{
  return(sqrt(mean((y - h) ^ 2)))
}

# loop over values of Lambda instead of degrees + calculate RMSE for 10th-degree polynomial with different lambdas
# don’t have to refit model each time b/c glmnet stores fitted model for many values of Lambda after a single fitting step.
glmnet.fit <- with(training.df, glmnet(poly(X, degree = 10), Y))
lambdas <- glmnet.fit$lambda

performance <- data.frame()
for (lambda in lambdas)
{
  performance <- rbind(performance, 
                       data.frame(Lambda = lambda, 
                                  RMSE = rmse(test.y, with(test.df, predict(glmnet.fit, poly(X, degree = 10), s = lambda)))))
}
```

Having computed model’s performance for different values of Lambda, plot to see where in range of testd lambdas we can get the best performance on new data:
```{r}
ggplot(performance, aes(x = Lambda, y = RMSE)) +
  geom_point() +
  geom_line()
```

Seems like we get best possible performance (lowest RMSE) w/ Lambda = ~.05. So to fit a model to the full data, we can select that calculated lambda + train our model on the entire data set.
```{r}
best_lambda <- with(performance, Lambda[which(RMSE == min(RMSE))])

# fitting final model to the whole data set
glmnet.fit <- with(df, glmnet(poly(X, degree = 10), Y))

# examine structure of our regularized model:
coef(glmnet.fit, s = best_lambda)
```

We end up with a best lamdba of ~.25 and using only 3 nonzero coefficients, even though the model has the ability to use 10. Selecting a simpler model like this one, even when more complicated models are possible, is the major strategy behind regularization. W/ regularization in our toolkit, we can employ polynomial regression w/ a large degree + still keep ourselves from overfitting the data.

# Text Regression

CV +  regularization = both powerful tools that allow us to use complex models that can mimic very intricate patterns in data w/out overfitting. Can employ regularization to use text to predict some continuous output (how volatile a stock will be based on IPO filings). When using text as an input for a regression problem, almost always have far more inputs (words or cols) than observations (documents or rows). If we have more observations than **1-grams** (single words), simply consider **2-grams** (pairs of words) or **3-grams** (triplets of words) until we have more **n-grams** than documents. 

B/c our data set has more columns than rows, unregularized linear regression will *always* produce an overfit model. Must use some form of regularization to get any meaningful results.

Predict relative popularity of top-100-selling books O’Reilly has ever published using only descriptions of those books from their back covers as input. To transform these text descriptions into a useful set of inputs, convert each book’s description into a vector of word counts to see how often words such as “the” and “Perl” occur in each description. The results of our analysis will be, in theory, a list of the words in a book’s description that predict high sales.

Of course, it’s always possible that a prediction task simply can’t be achieved. This may be the result of the model assigning high coefficients to words that are essentially arbitrary. For example, there may be very few common words in the descriptions of popular O’Reilly books but b/c the model does not know this, it will still attempt to fit the data + assign value to some words. In this case, however, the results would provide little to no new info about what makes these words useful. It is very important to keep this in mind when performing text regression.

To get started, load in raw data set and transform it into a document term matrix using the 
```{r}
ranks <- read.csv('./data/oreilly.csv', stringsAsFactors = FALSE)

# get book description + put as text of document
documents <- data.frame(doc_id = row.names(ranks), text = ranks$Long.Desc.)
#row.names(documents) <- 1:nrow(documents)

# end up with 100 rows (for each book) with 1 column of book description text
# convert to Corpus + to lowercase, remove whitespace + stopwords, convert to DTM
corpus <- Corpus(DataframeSource(documents))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
```

Now to manipulate our variables a little bit to make it easier to describe our regression problem to `glmnet`
```{r}
# convert dtm to numeric matrix b/c easier to work with
x <- as.matrix(dtm)
# reverse the encoding so highest ranked book has y = 100 and lowest has y = 1
y <- rev(1:100)
```

With a reversed ranking, the coefficients that predict the popularity of a book will be positive when they signal an increase in popularity. If we used raw ranks, coefficients for those same words would have to be negative, which is less intuitive, even though there’s no substantive difference between the two coding systems.

```{r}
set.seed(1)
# loop over several possible values for Lambda to see which gives best results on test data. 
# we don’t have a lot of data so do this split 50 times for each value of Lambda 
# this gets a better sense of accuracy from the different levels of regularization. 
performance <- data.frame()

# set a values for Lambda to loop through
for (lambda in c(0.1, 0.25, 0.5, 1, 2, 5)) {
  # split data into training + test sets 50 times 
  for (i in 1:50)
  {
    indices <- sample(1:100, 80)
    training.x <- x[indices, ]
    training.y <- y[indices]
    test.x <- x[-indices, ]
    test.y <- y[-indices]
    # fit model
    glm.fit <- glmnet(training.x, training.y)
    # make predictions
    predicted.y <- predict(glm.fit, test.x, s = lambda)
    # calculate error
    rmse <- sqrt(mean((predicted.y - test.y) ^ 2))
    # store error, lambda value, + iteration # in data frame
    performance <- rbind(performance,
                         data.frame(Lambda = lambda,
                                    Iteration = i,
                                    RMSE = rmse))
  }
}
```

After computing the performance of the model for these different values of Lambda, compare them to see where the model does best:
```{r}
ggplot(performance, aes(x = Lambda, y = RMSE)) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'point')
```

Unfortunately, this looks like a failed attempt to apply a statistical method to data. It’s clear the model gets better + better w/ higher values of Lambda, but that case occurs exactly when a model reduces to nothing more than an intercept. At that point, we’re not using any text data at all. 

In short, there’s no signal here our text regression model can find. Everything we see turns out to be noise when you test the model against held-out data. Although that means we don’t have a better idea of how to write a back cover description for a book to make sure it sells well, it’s a valuable lesson for anyone working in ML to absorb: sometimes there’s just no signal to be found in the data you’re working w/ 

"The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data." — John Tukey

# Logistic Regression to the Rescue

But we don’t have to give up on this data set completely yet. Although we haven’t succeeded at building a tool that predicts ranks from texts, we might try to do something simpler + see if we can predict whether a book appears in the top 50 or not = classification. Because this distinction is so much broader + simpler, it should be easier to extract a signal from our small data set. 

```{r}
# add class labels to data set for 1 = in top 50 and 0 = not in top 50
y <- rep(c(1, 0), each = 50)
```

Logistic regression is, deep down, essentially a form of regression to predict probability an item belongs to 1 of 2 categories. B/c probabilities are always between 0-1, we can threshold them at 0.5 to construct a classification algorithm. Other than the fact the outputs are between 0-1, logistic regression behaves essentially identically to linear regression + the only difference = need to threshold outputs to produce class predictions. 

```{r}
# fit logistic regression to the entire data set
x <- as.matrix(cbind(x, x))

regularized.fit <- glmnet(x, y, family = 'binomial')
```

Linear regression assumes the errors you see have a **Gaussian distribution**, whereas logistic regression assumes errors are **binomially distributed**, so we use `family = "binomial"`. The binomial distribution produces errors that are all 0s or 1s, which is what we need for 
```{r}
# linear regression
regularized.fit <- glmnet(x, y)
regularized.fit <- glmnet(x, y, family = 'gaussian')
# logistic regression
regularized.fit <- glmnet(x, y, family = 'binomial')

# see what the predictions from our model look like
y_pred <- predict(regularized.fit, newx = x, s = 0.001)
head(y_pred)
tail(y_pred)
```

Output contains both positive + negative values, even though we’re hoping to get predictions that are 0 or 1. There are 2 things we can do w/ raw predictions.

1) threshold them at 0 and make 0/1 predictions
```{r}
ifelse(head(y_pred) > 0, 1, 0)
ifelse(tail(y_pred) > 0, 1, 0)
```
2) Convert into probabilitie = more easily interpretalbe but we’d have to do the thresholding again at 0.5
```{r}
# convert raw logistic regression predictions into probabilities, we’
head(boot::inv.logit(y_pred))
tail(boot::inv.logit(y_pred))
```

Logistic regression expects its outputs to be transformed through the **inverse logit function** before you can produce probabilities as outputs. For that reason, logistic regression is often called the **logit model**.

Whichever way you decide to transform the raw logistic regression outputs, what really matters is logistic regression provides you with a tool that does classification just as readily as we performed linear regression in 

Code to use logistic regression to see how well we can classify books into the top 50 or below is essentially identical to the code to fit our rank prediction model using linear regression:
```{r}
set.seed(1)
performance <- data.frame()
# split data 250 times to get a better sense of mean error rate for each setting of lambda
for (i in 1:250) {
  indices <- sample(1:100, 80)
  training.x <- x[indices, ]
  training.y <- y[indices]
  test.x <- x[-indices, ]
  test.y <- y[-indices]
  # for each split, loop through different lambda values + calculate error + store it
  for (lambda in c(0.0001, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.5, 0.1)) {
    # fit model, make predictions, compute error/rmse, store in data frame 
    glm.fit <- glmnet(training.x, training.y, family = 'binomial')
    predicted.y <- ifelse(predict(glm.fit, test.x, s = lambda) > 0, 1, 0)
    error.rate <- mean(predicted.y != test.y)
    performance <- rbind(performance,
                         data.frame(Lambda = lambda,
                                    Iteration = i,
                                    ErrorRate = error.rate))
  }
}
```

We’ve split the data so many times b/c done the error rates end up being very close to 50% + we want to confirm we’re truly doing better than chance when making predictions. 

To make this increased splitting more efficient, reverse the order of the splitting + lambda loops so we don’t
redo the split for every single value of lambda = saves time + serves as a reminder that writing effective ML code requires you behave like a good programmer who cares about writing efficient code.

```{r}
# graph error rate as we sweep through values of lambda
ggplot(performance, aes(x = Lambda, y = ErrorRate)) +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
  stat_summary(fun.data = 'mean_cl_boot', geom = 'point') +
  scale_x_log10()
```

The results tell us that we’ve had real success replacing regression with classification. For low values of lambda, we’re able to get better than chance performance when predicting whether a book will be among the top 50, which is a relief. Although this data was not sufficient to fit a sophisticated rank prediction regression, it turns out to be large enough to fit a much simpler binary distinction that splits books into the top 50 and below.

Sometimes simpler things are better. Regularization forces us to use simpler models so we get higher performance on test data + switching from a regression model to a classification model can give you much better performance because the requirements for a binary distinction are generally much weaker than the requirements for predicting ranks directly.

# Code Breaking as Optimization

Moving beyond regression models, almost every algorithm in ML can be viewed as an optimization problem trying to minimize some measure of prediction error, but sometimes parameters aren’t simple #'s, so evaluating an error function at a single point doesn’t give enough info about nearby DP's to use `optim`. For these problems, we could use grid search, but there are other approaches that work better than grid search. 

1 approach that’s fairly intuitive + very powerful has a big idea behind it = **stochastic optimization** = to move through the range of possible parameters slightly randomly, but making sure to head in directions where the error function tends to go down rather than up. This approach is related to a lot of popular optimization algorithms, including **simulated annealing**, **genetic algorithms**, + **Markov chain Monte Carlo (MCMC)**. The specific algorithm we’ll use is called the
**Metropolis method**, as versions of the Metropolis method power a lot of modern ML algorithms.

# Case study: Breaking secret codes. 

The algorithm we’re going to define isn’t a very efficient decryption system + would never be seriously used for PROD systems, but it’s a very clear example of how to use the Metropolis method. Importantly, it’s also an example where most out-of-the-box optimization algorithms such as `optim` could never work.

Given a string of letters you know are encrypted using a **substitution cipher**, but how do we decide on a decryption rule that gives us the original text? Substitution ciphers = the simplest possible encryption system = replace every letter in an unencrypted message w/ a fixed letter in the encrypted message. **ROT13** is probably the most famous example (replace every letter w/ the letter 13 positions further in the alphabet, so “a” = “n,” “b’” = “o,” + so on), though the **Caesar cipher** might also be known to you = a very simple substitution cipher = replace every letter w/ its neighbor in the alphabet: “a” = “b,” “b” = “c,”, “z” = “a.”)

```{r}
# create the Caesar cipher
english.letters <- c('a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',
                     'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v',
                     'w', 'x', 'y', 'z')
caesar.cipher <- list()
inverse.caesar.cipher <- list()

# for each letter, 
for (index in 1:length(english.letters)) {
  # get letter 1 position further in alphabet (use modulus to get z = a) + place into original index
  caesar.cipher[[english.letters[index]]] <- english.letters[index %% 26 + 1]
  # get new index + replace with original index to get back original alphabet
  inverse.caesar.cipher[[english.letters[index %% 26 + 1]]] <- english.letters[index]
}
head(caesar.cipher)
tail(caesar.cipher)
```

W/ the cipher implemented, build some functions so we can translate a string using a cipher:
```{r}
# function to apply cipher to single string
apply.cipher.to.string <- function(string, cipher) {
  output <- ""
  for (i in 1:nchar(string)) {
    # get current letter index in string via substr, paste with other chars with no spaces
    output <- paste(output, cipher[[substr(string,i,i)]], sep = "")
  }
  return(output)
}
#apply.cipher.to.string("hi", caesar.cipher)

# cipher to apply to multiple strings in a text vector
apply.cipher.to.text <- function(text, cipher) {
  output <- c()
  for (string in text) {
    # apply single-string cipher to each string in the text
    output <- c(output, apply.cipher.to.string(string, cipher))
  }
  return(output)
}

apply.cipher.to.text(c('sample', 'text'), caesar.cipher)
```

Now we have some basic tools for working w/ ciphers, start thinking about the problem of breaking codes we might come across. Just as w/ linear regression, break the substitution ciphers in several parts:
<ol>
<li> Define a measure of the **quality** of a proposed decryption rule. </li>
<li> Define an algorithm for proposing **new potential decryption rules** that randomly modifies versions of our current best rule. </li>
<li> Define an algorithm for moving progressively toward better decryption rules. </li>
</ol>

To start thinking about how to measure quality of decryption rule, suppose you were given a piece of text you knew had been encrypted using a substitution cipher. Now imagine you only had a *piece* of encrypted text + a guarantee the original message was in standard English. How would you go about decoding it?

The approach to solving that problem = to say a rule is "good" if it turns the encrypted message into normal English. Given a proposed decryption rule, run it on the encrypted text + see whether the output is realistic English. Imagine we had 2 proposed decryption rules, A + B, whose results are:
<ul>
<li> decrypt(T, A) = xgpk xkfk xkek </li>
<li> decrypt(T, B) = veni vidi vici </li>
</ul>

It’s pretty clear rule B is better as it looks like real language rather than nonsense. To transform that human intuition into something automatic we can program a CPU to do, use a **lexical database** that provides the probability for any word we see. Real language will then be equivalent to text built out of words that have high probability, whereas fake language will be equivalent to text w/ words w/ low probability. The only complexity w/ this approach = dealing w/ words that don’t exist at all as their probability = 0 + we’re going to estimate the probability of a entire piece of text as a whole by multiplying the probability of individual words together. So we need to replace 0 w/ something really small = our machine’s smallest distinguishable floating-point difference, **epsilon**, to be able to use our algorithm. 

Once we’ve handled that **edge case**, we can use a lexical database to rank the quality of 2 pieces of translated text by finding the probability of each word + multiplying them together to find an estimate of the probability of the text as a whole. Using a lexical database to calculate probability of decrypted text gives us our error metric for evaluating a proposed decryption rule. 

Now that we have an error function, our code-breaking problem has turned entirely into a optimization problem, so we just need to find decryption rules that produce text w/ high probability. Unfortunately, the problem of finding the rule w/ the highest text probability isn’t close to being the sort of problem where `optim` would work. Decryption rules can’t be graphed + don’t have the smoothness `optim` needs when trying to figure how to head toward better rules. So we need a totally new optimization algorithm for solving our decryption problem = the Metropolis method, though it’s very, very slow for any reasonable length of text.

The basic idea for the Metropolis method = start w/ an arbitrary decryption rule + iteratively improve it many times so that it becomes a rule that could feasibly be the right one. This may seem like magic at first, but it often works in practice. Once we have a potential decryption rule in hand, we can use human intuition based on semantic coherence + grammar to decide whether we’ve correctly decrypted the text (slowness is considerably exacerbated by R’s slow text-processing tools)

To generate a good rule, start w/ a completely arbitrary rule + repeat a single operation that improves our rule a large # of times, say, 50k. B/c each step tends to head in the direction of better rules, repeating this operation over + over will get us somewhere reasonable in the end, though there’s no guarantee we won’t need 50M steps to get where we’d like to be. That’s the reason this algorithm won’t work well for a serious code-breaking system = you have no guarantee the algorithm will you a solution in a reasonable amount of time + it’s very hard to tell if you’re even moving in the right direction while waiting. This case study is just a toy example that shows off how to use optimization algorithms to solve complex problems that might otherwise seem impossible to solve.

Let’s be specific about how we’re going to propose a new decryption rule. We’ll do it by *randomly disturbing the current rule in just 1 place* = disturb our current rule by changing the rule’s effect on a *single* letter of the input alphabet. If “a” currently translates to “b” under our rule, we’ll propose a modification that has “a” translate to “q.” B/c of the way substitution ciphers works, this will actually require another modification to the part of the rule that originally sent another letter (like say "c") to “q.” To keep our cipher working, “c” now has to translate to “b.” So our algorithm for proposing new rules boils down to making 2 swaps in our existing rule, 1 randomly selected + another forced by the definition of a substitution cipher.

<ol>
<li> If probability of the text decrypted w/ rule B > probability of the text decrypted w/ rule A, replace A w/ B.
<li> If not, we’ll still replace A w/ B sometimes, just not every time. To be specific, if probability of the text decrypted w/ rule B = **P(T, B)** + probability of the text decrypted w/ rule A = **P(T, A)**, we’ll switch over to rule B P(T, B) / P(T, A) % of the time.

If this ratio seems to have come out of left field, don’t worry. For intuition’s sake, what really matters isn’t the specific ratio, but the fact that we accept rule B > 0% of the time. That’s what helps us to avoid the traps **greedy optimization**

Before we can use the Metropolis method to sort through different ciphers, we need some tools for creating the perturbed ciphers
```{r}
generate.random.cipher <- function() {
  cipher <- list()
  inputs <- english.letters
  # get 26 random samples of letter indices from the #'s 1-26 + get the letters w/ these indices
  outputs <- english.letters[sample(1:length(english.letters),length(english.letters))]
  
  for (index in 1:length(english.letters)) {
    # replace the letter of the current index in the original letters w/ new letter w/ same index
    cipher[[inputs[index]]] <- outputs[index]
  }
  # return list of letters that have been perturbed
  return(cipher)
}
#generate.random.cipher()

modify.cipher <- function(cipher, input, output) {
  # get new list of letters from given arg
  new.cipher <- cipher
  new.cipher[[input]] <- output
  old.output <- cipher[[input]]
}

  new.cipher <- cipher
  new.cipher[[input]] <- output
  old.output <- cipher[[input]]
  collateral.input <- names(which(sapply(names(cipher), 
                      function (key) {cipher[[key]]}) == output))
  new.cipher[[collateral.input]] <- old.output
  return(new.cipher)
}
propose.modified.cipher <- function(cipher)
{
  input <- sample(names(cipher), 1)
  output <- sample(english.letters, 1)
  return(modify.cipher(cipher, input, output))
}
```
Combining this tool for proposing new rules + the rule-swapping procedure specified softens the greediness of our optimization approach w/out making us waste too much time on obviously bad rules w/ much lower probability than our current rule. To do this softening algorithmically, just compute P(T, B) / P(T , A) + compare it with a random number between 0 and 1. If the
resulting random number is higher than 
probability(T, B) / probability(T , A)
, we
replace our current rule. If not, we stick with our current rule.
In order to compute the probabilities that we keep mentioning, we’ve created a lexical
database that tells you how often each of the words in 
/usr/share/dic/words
 occurs in
text on Wikipedia. To load that into R, you would do the following:
load('data/lexical_database.Rdata')
You can get a feel for the database by querying some simple words and seeing their
frequencies in our sample tex would have us fall in

