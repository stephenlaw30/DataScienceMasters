return(predictions)
}
df %<>%
mutate(kNNPredictions = knn(df))
head(df)
paste("Correct:",sum(with(df,Label != kNNPredictions)))
paste("Total:",nrow(df))
paste("Accuracy: ",(1 - sum(with(df,Label != kNNPredictions))/nrow(df))*100,"%", sep = "")
# remove hard-coded knn algorithm
rm(knn)
library('class')
n <- nrow(df)
set.seed(1)
# get a SORTED random sample of indices from df of size = 1/2 the # of rows in df
indices <- sort(sample(1:n, n*(1/2)))
# create training + test sets from random sample indices
train_x <- df[indices, 1:2]
train_y <- df[indices, 3]
test_x <- df[-indices, 1:2]
test_y <- df[-indices, 3]
# use black-box knn algorithm to predict neighbors where
#   cl = train.y = factor of true classifications of training set
#predicted.y <- knn(train = train_x, test = test_x, cl = train_y, k = 5)
predicted_y <- knn(train_x, test_x, train_y, k = 5)
paste("Correct:",sum(predicted_y != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predicted_y != test_y)/length(test_y))*100,"%", sep = "")
# train logistic regression model on training data with default error distribution + test on test data
logistic_model <- glm(Label ~ X + Y, data = df[indices,], family = "gaussian")
predictions <- as.numeric(predict(logistic_model, newdata = df[-indices,]) > 0)
paste("Correct:",sum(predictions != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predictions != test_y)/length(test_y))*100,"%", sep = "")
installations <- read.csv('data/installations.csv')
head(installations)
library("reshape")
user.pckg.mtx <- cast(installations, User ~ Package, value = "Installed")
# check 1st + 2nd cols = installations of abind and AcceptanceStalling
user.pckg.mtx[, 1]
# make user IDs the row names + remove the redundant col
row.names(user.pckg.mtx) <- user.pckg.mtx[, 1]
user.pckg.mtx <- user.pckg.mtx[, -1]
head(user.pckg.mtx)
suppressWarnings(similarities <- cor(user.pckg.mtx))
dim(similarities)
# similarities/correlation between 1st package and itself, and 1st package and the 2nd package
similarities[1, 1]
similarities[1, 2]
# create distances such that a similarity of 1 = -log(1) which is 0, and similiary of -1 = -log(0), which is Inf
distances <- -log((similarities / 2) + 0.5)
get_knn <- function(p, distances, k = 25) {
# for given package p, sort the distances from p of every other package (cols after p) and get top 25
return(order(distances[p,])[2:(k + 1)])
}
get_installation_prob <- function(user, pckg, user.pckg.mtx, distances, k = 25) {
# get sorted distances of all packages from given package and return top 25
neighbors <- get_knn(pckg, distances, k = k)
# for each neighbor, get the install/not install value from user.pckg.mtx and get the mean of this install sum
return(sapply(neighbors, function(neighbor) { user.pckg.mtx[user, neighbor]}))
}
get_installation_prob(1, 1, user.pckg.mtx, distances)
get_installation_prob <- function(user, pckg, user.pckg.mtx, distances, k = 25) {
# get sorted distances of all packages from given package and return top 25
neighbors <- get_knn(pckg, distances, k = k)
# for each neighbor, get the install/not install value from user.pckg.mtx and get the mean of this install sum
return(mean(sapply(neighbors, function(neighbor) { user.pckg.mtx[user, neighbor]})))
}
get_installation_prob(1, 1, user.pckg.mtx, distances)
get_most_probably_pckgs <- function(user, user.pckg.mtx, distances, k = 25) {
# for the total # of packages in the matrix, get the installation probability of the package
# then order their indices them from greatest to least
return(order(sapply(1:ncol(user.pckg.mtx),
function(package) {
get_installation_prob(user, package, user.pckg.mtx, distances, k = k)
}), decreasing = T))
}
# get most probabile packages for user 1 to install
user <- 1
listings <- get_most_probably_pckgs(user, user.pckg.mtx, distances)
get_most_probably_pckgs <- function(user, user.pckg.mtx, distances, k = 25) {
# for the total # of packages in the matrix, get the installation probability of the package
# then order their indices them from greatest to least
return(order(sapply(1:ncol(user.pckg.mtx),
function(package) {
get_installation_prob(user, package, user.pckg.mtx, distances, k = k)
}), decreasing = T))
}
# get most probabile packages for user 1 to install
user <- 1
listings <- get_most_probably_pckgs(user, user.pckg.mtx, distances)
# get top 10 most probable package installations for user 1
colnames(user.pckg.mtx)[listings[1:10]]
library(ggplot2)
library(jsonlite)
# jsonlite is already loaded
# Challenge 1
json1 <- '[1, 2, 3, 4, 5, 6]'
fromJSON(json1)
# Challenge 2
json2 <- '{"a": [1, 2, 3], "b": [4, 5, 6]}'
fromJSON(json2)
# Challenge 1
#json1 <- '[[1, 2], [3, 4], [5, 6]]'
json1 <- '[[1, 2], [3, 4]]'
fromJSON(json1)
# Challenge 2
#json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}]'
json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 5, "b": 6}]'
fromJSON(json2)
# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"
# Import the .csv file located at url_csv
water <- read.csv(url_csv,stringsAsFactors = F)
# Convert the data file according to the requirements
water_json <- toJSON(water)
# Print out water_json
water_json
# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"
# Import the .csv file located at url_csv
water <- read.csv(url_csv,stringsAsFactors = F)
# Convert the data file according to the requirements
water_json <- toJSON(water)
# Print out water_json
head(water_json)
# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"
# Import the .csv file located at url_csv
water <- read.csv(url_csv,stringsAsFactors = F)
# Convert the data file according to the requirements
water_json <- toJSON(water)
# Print out water_json
water_json[1:5]
# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"
# Import the .csv file located at url_csv
water <- read.csv(url_csv,stringsAsFactors = F)
# Convert the data file according to the requirements
water_json <- toJSON(water)
# Print out water_json
water_json
# jsonlite is already loaded
# Convert mtcars to a pretty JSON: pretty_json
pretty_json <- toJSON(mtcars)
# Print pretty_json
pretty_json
# Minify pretty_json: mini_json
mini_json <- minify(pretty_json)
# Print mini_json
mini_json
# jsonlite is already loaded
# Convert mtcars to a pretty JSON: pretty_json
pretty_json <- toJSON(mtcars, pretty = T)
# Print pretty_json
pretty_json
# Minify pretty_json: mini_json
mini_json <- minify(pretty_json)
# Print mini_json
mini_json
install.packages("haven")
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")
library(haven)
# Import sales.sas7bdat: sales
sales <- read_sas("sales.sas7bdat")
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")
plot(sugar$Import,sugar$Weight_I)
# Import SPSS data from the URL: work
work <- read_sav("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav")
# Display summary of work$GENDER
summary(work$GENDER) # doesn't give much info
# Convert work$GENDER to a factor
work$GENDER <- as_factor(work$GENDER)
# Display summary of work$GENDER again
summary(work$GENDER) # doesn't give much info
library(tidyverse)
library(ggplot2)
library(magrittr)
df <- read.csv('data/df.csv')
df <- read.csv('./data/df.csv')
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
mean(with(df, 0 == Label))
#[1] 0.5156
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions < ifelse(predict(svm_model) > 0, 1, 0)
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
df <- cbind(df, data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression
SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM
df
melt(df, id.vars = c('X', 'Y'))
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
melt(df, id.vars = c('X', 'Y'))
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
mean(with(df, 0 == Label))
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
df <- cbind(df, data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression
SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM
head(df)
# create new df where cols melt into single col's values
predictions <- melt(df, id.vars = c('X', 'Y'))
head(predictions)
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
# reset df
df <- df[, c('X', 'Y', 'Label')]
linear_svm <- svm(Label ~ X + Y, df, kernel = 'linear')
with(df, mean(Label == ifelse(predict(linear_svm) > 0, 1, 0)))
polynomial_svm <- svm(Label ~ X + Y, df, kernel = 'polynomial')
with(df, mean(Label == ifelse(predict(polynomial_svm) > 0, 1, 0)))
radial_svm <- svm(Label ~ X + Y, df, kernel = 'radial')
with(df, mean(Label == ifelse(predict(radial_svm) > 0, 1, 0)))
sigmoid_svm <- svm(Label ~ X + Y, df, kernel = 'sigmoid')
with(df, mean(Label == ifelse(predict(sigmoid_svm) > 0, 1, 0)))
df <- cbind(df,
data.frame(LinearSVM = ifelse(predict(linear_svm) > 0, 1, 0),
PolynomialSVM = ifelse(predict(polynomial_svm) > 0, 1, 0),
RadialSVM = ifelse(predict(radial_svm) > 0, 1, 0),
SigmoidSVM = ifelse(predict(sigmoid_svm) > 0, 1, 0)))
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
mean(with(df, 0 == Label))
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
mean(with(df, 0 == Label))
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
df <- cbind(df, data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression
SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM
head(df)
# create new df where cols melt into single col's values
predictions <- melt(df, id.vars = c('X', 'Y'))
head(predictions)
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
# reset df
df <- df[, c('X', 'Y', 'Label')]
linear_svm <- svm(Label ~ X + Y, df, kernel = 'linear')
with(df, mean(Label == ifelse(predict(linear_svm) > 0, 1, 0)))
polynomial_svm <- svm(Label ~ X + Y, df, kernel = 'polynomial')
with(df, mean(Label == ifelse(predict(polynomial_svm) > 0, 1, 0)))
radial_svm <- svm(Label ~ X + Y, df, kernel = 'radial')
with(df, mean(Label == ifelse(predict(radial_svm) > 0, 1, 0)))
sigmoid_svm <- svm(Label ~ X + Y, df, kernel = 'sigmoid')
with(df, mean(Label == ifelse(predict(sigmoid_svm) > 0, 1, 0)))
df <- cbind(df,
data.frame(LinearSVM = ifelse(predict(linear_svm) > 0, 1, 0),
PolynomialSVM = ifelse(predict(polynomial_svm) > 0, 1, 0),
RadialSVM = ifelse(predict(radial_svm) > 0, 1, 0),
SigmoidSVM = ifelse(predict(sigmoid_svm) > 0, 1, 0)))
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_model_polynomialD3 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 3)
paste("3rd degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))))
svm_model_polynomialD5 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 5)
paste("3rd degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))))
svm_model_polynomialD10 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 10)
paste("3rd degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))))
svm_model_polynomialD12 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 12)
paste("3rd degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0))))
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))),
Degree5SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))),
Degree10SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))),
Degree12SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
svm_predictions
df
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))),
Degree5SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))),
Degree10SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))),
Degree12SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))),
Degree5SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))),
Degree10SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))),
Degree12SVM = with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_predictions
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = ifelse(predict(svm_model_polynomialD3) > 0, 1, 0),
Degree5SVM = ifelse(predict(svm_model_polynomialD5) > 0, 1, 0),
Degree10SVM = ifelse(predict(svm_model_polynomialD10) > 0, 1, 0),
Degree12SVM = ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_predictions
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = ifelse(predict(svm_model_polynomialD3) > 0, 1, 0),
Degree5SVM = ifelse(predict(svm_model_polynomialD5) > 0, 1, 0),
Degree10SVM = ifelse(predict(svm_model_polynomialD10) > 0, 1, 0),
Degree12SVM = ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
mean(with(df, 0 == Label))
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
df <- cbind(df,
data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression prediction
SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM prediction
head(df)
# create new df where cols melt into single col's values
predictions <- melt(df, id.vars = c('X', 'Y'))
head(predictions)
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
# reset df
df <- df[, c('X', 'Y', 'Label')]
linear_svm <- svm(Label ~ X + Y, df, kernel = 'linear')
with(df, mean(Label == ifelse(predict(linear_svm) > 0, 1, 0)))
polynomial_svm <- svm(Label ~ X + Y, df, kernel = 'polynomial')
with(df, mean(Label == ifelse(predict(polynomial_svm) > 0, 1, 0)))
radial_svm <- svm(Label ~ X + Y, df, kernel = 'radial')
with(df, mean(Label == ifelse(predict(radial_svm) > 0, 1, 0)))
sigmoid_svm <- svm(Label ~ X + Y, df, kernel = 'sigmoid')
with(df, mean(Label == ifelse(predict(sigmoid_svm) > 0, 1, 0)))
df <- cbind(df, data.frame(LinearSVM = ifelse(predict(linear_svm) > 0, 1, 0),
PolynomialSVM = ifelse(predict(polynomial_svm) > 0, 1, 0),
RadialSVM = ifelse(predict(radial_svm) > 0, 1, 0),
SigmoidSVM = ifelse(predict(sigmoid_svm) > 0, 1, 0)))
# melt each SVM col into single col
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_model_polynomialD3 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 3)
paste("3rd degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))))
svm_model_polynomialD5 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 5)
paste("5th degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))))
svm_model_polynomialD10 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 10)
paste("10th degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))))
svm_model_polynomialD12 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 12)
paste("12th degree polynomial accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0))))
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = ifelse(predict(svm_model_polynomialD3) > 0, 1, 0),
Degree5SVM = ifelse(predict(svm_model_polynomialD5) > 0, 1, 0),
Degree10SVM = ifelse(predict(svm_model_polynomialD10) > 0, 1, 0),
Degree12SVM = ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_model_radial_cost1 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 1)
paste("Radial Cost 1 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))))
svm_model_radial_cost2 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 2)
paste("Radial Cost 2 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))))
svm_model_radial_cost3 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 3)
paste("Radial Cost 3 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))))
svm_model_radial_cost4 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 4)
paste("Radial Cost 4 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0))))
svm_model_radial_cost1 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 1)
paste("Radial Cost 1 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_radial_cost1) > 0, 1, 0))))
svm_model_radial_cost2 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 2)
paste("Radial Cost 2 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_radial_cost2) > 0, 1, 0))))
svm_model_radial_cost3 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 3)
paste("Radial Cost 3 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_radial_cost3) > 0, 1, 0))))
svm_model_radial_cost4 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 4)
paste("Radial Cost 4 accuracy:",
with(df, mean(Label != ifelse(predict(svm_model_radial_cost4) > 0, 1, 0))))
svm_model_radial_cost1 <- svm(Label ~ X + Y,
data = df,
kernel = 'radial',
cost = 1)
with(df, mean(Label == ifelse(predict(radial.cost1.svm.fit) > 0, 1, 0)))
svm_model_radial_cost1 <- svm(Label ~ X + Y,
data = df,
kernel = 'radial',
cost = 1)
with(df, mean(Label == ifelse(predict(svm_model_radial_cost1) > 0, 1, 0)))
svm_model_radial_cost1 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 1)
paste("Radial Cost 1 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_radial_cost1) > 0, 1, 0))))
svm_model_radial_cost2 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 2)
paste("Radial Cost 2 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_radial_cost2) > 0, 1, 0))))
svm_model_radial_cost3 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 3)
paste("Radial Cost 3 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_radial_cost3) > 0, 1, 0))))
svm_model_radial_cost4 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 4)
paste("Radial Cost 4 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_radial_cost4) > 0, 1, 0))))
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(Degree3SVM = ifelse(predict(svm_model_radial_cost1) > 0, 1, 0),
Degree5SVM = ifelse(predict(svm_model_radial_cost2) > 0, 1, 0),
Degree10SVM = ifelse(predict(svm_model_radial_cost3) > 0, 1, 0),
Degree12SVM = ifelse(predict(svm_model_radial_cost4) > 0, 1, 0)))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
svm_model_sigmoid_gamma1 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 1)
paste("sigmoid gamma 1 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma1) > 0, 1, 0))))
svm_model_sigmoid_gamma2 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 2)
paste("sigmoid gamma 2 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma2) > 0, 1, 0))))
svm_model_sigmoid_gamma3 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 3)
paste("sigmoid gamma 3 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma3) > 0, 1, 0))))
svm_model_sigmoid_gamma4 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 4)
paste("sigmoid gamma 4 accuracy:",
with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma4) > 0, 1, 0))))
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
data.frame(gamma1SVM = ifelse(predict(svm_model_sigmoid_gamma1) > 0, 1, 0),
gamma2SVM = ifelse(predict(svm_model_sigmoid_gamma2) > 0, 1, 0),
gamma3SVM = ifelse(predict(svm_model_sigmoid_gamma3) > 0, 1, 0),
gamma4SVM = ifelse(predict(svm_model_sigmoid_gamma4) > 0, 1, 0)))
# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))
ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
geom_point() +
facet_grid(variable ~ .)
sales <- read.csv("sales.csv", stringsAsFactors = F)
