# create training + test data
set.seed(1)
x <- seq(0, 1, by = 0.01)
y <- sin(2 * pi * x) + rnorm(length(x), 0, 0.1)
n <- length(x)
indices <- sort(sample(1:n, round(0.5 * n)))
training.x <- x[indices]
training.y <- y[indices]
test.x <- x[-indices]
test.y <- y[-indices]
df <- data.frame(X = x, Y = y)
training.df <- data.frame(X = training.x, Y = training.y)
test.df <- data.frame(X = test.x, Y = test.y)
# function to calculate RMSE
rmse <- function(y, h)
{
return(sqrt(mean((y - h) ^ 2)))
}
# loop over values of Lambda instead of degrees + calculate RMSE for 10th-degree polynomial with different lambdas
# don’t have to refit model each time b/c glmnet stores fitted model for many values of Lambda after a single fitting step.
glmnet.fit <- with(training.df, glmnet(poly(X, degree = 10), Y))
lambdas <- glmnet.fit$lambda
performance <- data.frame()
for (lambda in lambdas)
{
performance <- rbind(performance,
data.frame(Lambda = lambda,
RMSE = rmse(test.y, with(test.df, predict(glmnet.fit, poly(X, degree = 10), s = lambda)))))
}
ggplot(performance, aes(x = Lambda, y = RMSE)) +
geom_point() +
geom_line()
best_lambda <- with(performance, Lambda[which(RMSE == min(RMSE))])
# fitting final model to the whole data set
glmnet.fit <- with(df, glmnet(poly(X, degree = 10), Y))
# examine structure of our regularized model:
coef(glmnet.fit, s = best_lambda)
ranks <- read.csv('./data/oreilly.csv', stringsAsFactors = FALSE)
# get book description + put as text of document
documents <- data.frame(doc_id = row.names(ranks), text = ranks$Long.Desc.)
#row.names(documents) <- 1:nrow(documents)
# end up with 100 rows (for each book) with 1 column of book description text
# convert to Corpus + to lowercase, remove whitespace + stopwords, convert to DTM
corpus <- Corpus(DataframeSource(documents))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
dtm <- DocumentTermMatrix(corpus)
# convert dtm to numeric matrix b/c easier to work with
x <- as.matrix(dtm)
# reverse the encoding so highest ranked book has y = 100 and lowest has y = 1
y <- rev(1:100)
set.seed(1)
# loop over several possible values for Lambda to see which gives best results on test data.
# we don’t have a lot of data so do this split 50 times for each value of Lambda
# this gets a better sense of accuracy from the different levels of regularization.
performance <- data.frame()
# set a values for Lambda to loop through
for (lambda in c(0.1, 0.25, 0.5, 1, 2, 5)) {
# split data into training + test sets 50 times
for (i in 1:50)
{
indices <- sample(1:100, 80)
training.x <- x[indices, ]
training.y <- y[indices]
test.x <- x[-indices, ]
test.y <- y[-indices]
# fit model
glm.fit <- glmnet(training.x, training.y)
# make predictions
predicted.y <- predict(glm.fit, test.x, s = lambda)
# calculate error
rmse <- sqrt(mean((predicted.y - test.y) ^ 2))
# store error, lambda value, + iteration # in data frame
performance <- rbind(performance,
data.frame(Lambda = lambda,
Iteration = i,
RMSE = rmse))
}
}
ggplot(performance, aes(x = Lambda, y = RMSE)) +
stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
stat_summary(fun.data = 'mean_cl_boot', geom = 'point')
# add class labels to data set for 1 = in top 50 and 0 = not in top 50
y <- rep(c(1, 0), each = 50)
# fit logistic regression to the entire data set
x <- as.matrix(cbind(x, x))
regularized.fit <- glmnet(x, y, family = 'binomial')
# linear regression
regularized.fit <- glmnet(x, y)
regularized.fit <- glmnet(x, y, family = 'gaussian')
# logistic regression
regularized.fit <- glmnet(x, y, family = 'binomial')
# see what the predictions from our model look like
y_pred <- predict(regularized.fit, newx = x, s = 0.001)
head(y_pred)
tail(y_pred)
ifelse(head(y_pred) > 0, 1, 0)
ifelse(tail(y_pred) > 0, 1, 0)
# convert raw logistic regression predictions into probabilities, we’
head(boot::inv.logit(y_pred))
tail(boot::inv.logit(y_pred))
set.seed(1)
performance <- data.frame()
# split data 250 times to get a better sense of mean error rate for each setting of lambda
for (i in 1:250) {
indices <- sample(1:100, 80)
training.x <- x[indices, ]
training.y <- y[indices]
test.x <- x[-indices, ]
test.y <- y[-indices]
# for each split, loop through different lambda values + calculate error + store it
for (lambda in c(0.0001, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.5, 0.1)) {
# fit model, make predictions, compute error/rmse, store in data frame
glm.fit <- glmnet(training.x, training.y, family = 'binomial')
predicted.y <- ifelse(predict(glm.fit, test.x, s = lambda) > 0, 1, 0)
error.rate <- mean(predicted.y != test.y)
performance <- rbind(performance,
data.frame(Lambda = lambda,
Iteration = i,
ErrorRate = error.rate))
}
}
# graph error rate as we sweep through values of lambda
ggplot(performance, aes(x = Lambda, y = ErrorRate)) +
stat_summary(fun.data = 'mean_cl_boot', geom = 'errorbar') +
stat_summary(fun.data = 'mean_cl_boot', geom = 'point') +
scale_x_log10()
library(tidyverse)
library(ggplot2)
library(magrittr)
df <- read.csv("data/example_data.csv")
glimpse(df)
# compute Euclidiean distance between the points + store in distance matrix
# distance matrix = 2D array with distance between i + j is at matrix[i,j]
distance_mtx <- function(df) {
distance <- matrix(rep(NA, nrow(df)**2), ncol = nrow(df))
for(i in 1:nrow(df)) {
for(j in 1:nrow(df)) {
distance[i,j] <- sqrt((df[i,'X']-df[j,'X'])**2 + (df[i,'Y']-df[j,'Y'])**2)
}
}
return(distance)
}
distance_mtx(df)[1:5,1:5]
#kNN algorithm with default k = 5 to look for distances of point i
k_nearest_neighbors <- function(i, distance, k = 5) {
# get the k points occuring after input point i
return(order(distance[i,])[2:(k+1)])
}
distance_mtx(df)[1,k_nearest_neighbors(1,distance_mtx(df),k=5)]
knn <- function(df, k = 5) {
# get distance + set up prediction array to store predictions
distance <- distance_mtx(df)
predictions <- rep(NA, nrow(df))
for(i in 1:nrow(df)) {
indices <- k_nearest_neighbors(i, distance ,k = k)
predictions[i] <- ifelse(mean(df[indices,'Label']) > .5, 1, 0)
}
return(predictions)
}
df %<>%
mutate(kNNPredictions = knn(df))
head(df)
paste("Correct:",sum(with(df,Label != kNNPredictions)))
paste("Total:",nrow(df))
paste("Accuracy: ",(1 - sum(with(df,Label != kNNPredictions))/nrow(df))*100,"%", sep = "")
# remove hard-coded knn algorithm
rm(knn)
library('class')
n <- nrow(df)
set.seed(1)
# get a SORTED random sample of indices from df of size = 1/2 the # of rows in df
indices <- sort(sample(1:n, n*(1/2)))
# create training + test sets from random sample indices
train_x <- df[indices, 1:2]
train_y <- df[indices, 3]
test_x <- df[-indices, 1:2]
test_y <- df[-indices, 3]
# use black-box knn algorithm to predict neighbors where
#   cl = train.y = factor of true classifications of training set
#predicted.y <- knn(train = train_x, test = test_x, cl = train_y, k = 5)
predicted_y <- knn(train_x, test_x, train_y, k = 5)
paste("Correct:",sum(predicted_y != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predicted_y != test_y)/length(test_y))*100,"%", sep = "")
# train logistic regression model on training data with default error distribution + test on test data
logistic_model <- glm(Label ~ X + Y, data = df[indices,], family = "gaussian")
predictions <- as.numeric(predict(logistic_model, newdata = df[-indices,]) > 0)
paste("Correct:",sum(predictions != test_y))
paste("Total:",length(test_y))
paste("Accuracy: ",(1 - sum(predictions != test_y)/length(test_y))*100,"%", sep = "")
installations <- read.csv('data/installations.csv')
head(installations)
library("reshape")
user.pckg.mtx <- cast(installations, User ~ Package, value = "Installed")
# check 1st + 2nd cols = installations of abind and AcceptanceStalling
user.pckg.mtx[, 1]
# make user IDs the row names + remove the redundant col
row.names(user.pckg.mtx) <- user.pckg.mtx[, 1]
user.pckg.mtx <- user.pckg.mtx[, -1]
head(user.pckg.mtx)
suppressWarnings(similarities <- cor(user.pckg.mtx))
dim(similarities)
# similarities/correlation between 1st package and itself, and 1st package and the 2nd package
similarities[1, 1]
similarities[1, 2]
# create distances such that a similarity of 1 = -log(1) which is 0, and similiary of -1 = -log(0), which is Inf
distances <- -log((similarities / 2) + 0.5)
get_knn <- function(p, distances, k = 25) {
# for given package p, sort the distances from p of every other package (cols after p) and get top 25
return(order(distances[p,])[2:(k + 1)])
}
get_installation_prob <- function(user, pckg, user.pckg.mtx, distances, k = 25) {
# get sorted distances of all packages from given package and return top 25
neighbors <- get_knn(pckg, distances, k = k)
# for each neighbor, get the install/not install value from user.pckg.mtx and get the mean of this install sum
return(mean(sapply(neighbors, function(neighbor) { user.pckg.mtx[user, neighbor]})))
}
get_installation_prob(1, 1, user.pckg.mtx, distances)
get_most_probably_pckgs <- function(user, user.pckg.mtx, distances, k = 25) {
# for the total # of packages in the matrix, get the installation probability of the package
# then order their indices them from greatest to least
return(order(sapply(1:ncol(user.pckg.mtx),
function(package) {
get_installation_prob(user, package, user.pckg.mtx, distances, k = k)
}), decreasing = T))
}
# get most probable packages for user 1 to install
user <- 1
listings <- get_most_probably_pckgs(user, user.pckg.mtx, distances)
# get top 10 most probable package installations for user 1
colnames(user.pckg.mtx)[listings[1:10]]
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogRegression <- glmnet(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
?glmnet
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
## import data
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "", stringsAsFactors = F)
#sep = "\t",
glimpse(restaraunts)
head(restaraunts)
corpus <- VCorpus(VectorSource(restaraunts$Review))
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])
# clean data + reduce # of cols
corpus %<>%
tm_map(content_transformer(tolower)) %>% # need content_transformer() to keep corpus as a VCorpus, not a list
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords()) %>% # remove English stopwords
tm_map(stemDocument) %>% # get the ROOTs of each word via stemming
tm_map(stripWhitespace) # get rid of extra spaces leftover
# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
# set.seed(123)
# split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
# training_set = subset(dtm_df, split == TRUE)
# test_set = subset(dtm_df, split == FALSE)
#
# classifier = randomForest(x = training[-692],
#                           y = training$Liked,
#                           ntree = 10)
#
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test[-692])
#
# # Making the Confusion Matrix
# (cm = table(test[, 692], y_pred))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(training)
restaraunts_rf <- randomForest(x = training[-cols], y = training[,cols], ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-cols])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
# Predict using model on test set labels
temp_y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
y_pred_logreg <- if_else(temp_y_pred_logreg > .5, 1, 0)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_logreg))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
library(class)
## fit to training data
y_pred_knn <- knn(training[,-cols], test[,-cols], cl = training[,cols], k = 5)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_knn))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
library(e1071) # most popular SVM libary, other is `kernlab`
# Fitting Kernel SVM to the Training set
# `type` = *C-classification* to make sure we're doing a classification machine
# `kernel` = *linear*, as we are starting w/ the most basic kernel, the **linear kernel**.
restaraunts.svm <- svm(Liked ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred_svm <- predict(restaraunts.svm, test[-cols])
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_svm))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
library(e1071) # most popular SVM libary, other is `kernlab`
# Fitting Kernel SVM to the Training set
# `type` = *C-classification* to make sure we're doing a classification machine
# `kernel` = *linear*, as we are starting w/ the most basic kernel, the **linear kernel**.
restaraunts.svm <- svm(Liked ~ ., training, type = 'C-classification', kernel = 'radial')
# Predict on test set results
y_pred_svm <- predict(restaraunts.svm, test[-cols])
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_svm))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
restaraunts_nb <- naiveBayes(Liked ~ ., training)
# Predict on test set results
y_pred_nb <- predict(restaraunts_nb, test[-cols])
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_nb))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
library(rpart)
restaraunts_dtc <- rpart(Liked ~ ., training)
# Predict on test set results
y_pred_dtc <- predict(restaraunts_dtc, test[-cols], type = "class")
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_dtc))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
head(training)
training[cols]
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogReg <- glmnet(training[-cols], training[cols], family = "binomial")
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogReg <- glmnet(training, training[cols], family = "binomial")
(training[-cols]
type(training[-cols])
class(training[-cols])
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogReg <- glmnet(as.matrix(training[-cols]), training[cols], family = "binomial")
library('glmnet')
#regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
restaraunts_regularizedlogReg <- glmnet(as.matrix(training[-cols]), as.matrix(training[cols]), family = "binomial")
# binomial regression (1/0, yes/no, etc.)
(lambdas <- restaraunts_regularizedlogReg$lambda)
(lambdas <- restaraunts_regularizedlogReg$lambda)
((lambdas <- restaraunts_regularizedlogReg$lambda))
lambdas <- restaraunts_regularizedlogReg$lambda
performance <- data.frame()
for (lambda in lambdas) {
# test model on new test data with each lambda
temp_predictions <- predict(regularized_logistic_regression, test_x, s = lambda)
predictions <- as.numeric(temp_predictions > 0)
# calculate MSE
mse <- mean(predictions != test_y)
# add row back to DF
performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}
lambdas <- restaraunts_regularizedlogReg$lambda
performance <- data.frame()
for (lambda in lambdas) {
# test model on new test data with each lambda
temp_predictions <- predict(regularized_logistic_regression,
as.matrix(test[-cols]),
s = lambda)
predictions <- as.numeric(temp_predictions > 0)
# calculate MSE
mse <- mean(predictions != as.matrix(training[cols]))
# add row back to DF
performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}
temp_predictions <- predict(regularized_logistic_regression,
as.matrix(test[-cols]),
s = lambda)
lambdas <- restaraunts_regularizedlogReg$lambda
performance <- data.frame()
for (lambda in lambdas) {
# test model on new test data with each lambda
temp_predictions <- predict(restaraunts_regularizedlogReg,
as.matrix(test[-cols]),
s = lambda)
predictions <- as.numeric(temp_predictions > 0)
# calculate MSE
mse <- mean(predictions != as.matrix(training[cols]))
# add row back to DF
performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}
ggplot(performance, aes(Lambda, MSE)) +
geom_point() +
scale_x_log10()
lambdas <- restaraunts_regularizedlogReg$lambda
performance <- data.frame()
for (lambda in lambdas) {
# test model on new test data with each lambda
temp_predictions <- predict(restaraunts_regularizedlogReg,
as.matrix(test[-cols]),
s = lambda)
predictions <- as.numeric(temp_predictions > 0)
# calculate MSE
mse <- mean(predictions != as.matrix(training[cols]))
# add row back to DF
performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}
ggplot(performance, aes(Lambda, MSE)) +
geom_point() +
scale_x_log10()
(final_lambda <- with(performance, max(Lambda[which(MSE == min(MSE))])))
lambdas <- restaraunts_regularizedlogReg$lambda
performance <- data.frame()
for (lambda in lambdas) {
# test model on new test data with each lambda
temp_predictions <- predict(restaraunts_regularizedlogReg,
as.matrix(test[-cols]),
s = lambda)
predictions <- as.numeric(temp_predictions > 0)
# calculate MSE
mse <- mean(predictions != as.matrix(training[cols]))
# add row back to DF
performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}
ggplot(performance, aes(Lambda, MSE)) +
geom_point() +
scale_x_log10()
(final_lambda <- with(performance, max(Lambda[which(MSE == min(MSE))])))
(logreg_mse <- with(subset(performance, Lambda == final_lambda), MSE))
