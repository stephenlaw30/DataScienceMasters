classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Making the Confusion Matrix
(cm = table(test_set[, 692], y_pred > .5))
set.seed(123)
split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
training_set = subset(dtm_df, split == TRUE)
test_set = subset(dtm_df, split == FALSE)
# Fitting Random Forest Classification to the Training set
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Making the Confusion Matrix
(cm = table(test_set[, 692], y_pred))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, nTree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf > 0.5))
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, nTree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
set.seed(123)
split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
training_set = subset(dtm_df, split == TRUE)
test_set = subset(dtm_df, split == FALSE)
classifier = randomForest(x = training[-692],
y = training$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test[-692])
# Making the Confusion Matrix
(cm = table(test[, 692], y_pred))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692],
y = training$Liked,
ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, nTree = 10)
#randomForest(x = training[-692], y = training$Liked, ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, ntree = 10)
#randomForest(x = training[-692], y = training$Liked, nTree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or Recall): ",specificity)
cat("\nPrecision (Positive Predictive Value): ",precision)
cat("\nAccuracy: ",accuracy)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
## import data
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "", stringsAsFactors = F)
#sep = "\t",
glimpse(restaraunts)
head(restaraunts)
corpus <- VCorpus(VectorSource(restaraunts$Review))
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])
# clean data + reduce # of cols
corpus %<>%
tm_map(content_transformer(tolower)) %>% # need content_transformer() to keep corpus as a VCorpus, not a list
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords()) %>% # remove English stopwords
tm_map(stemDocument) %>% # get the ROOTs of each word via stemming
tm_map(stripWhitespace) # get rid of extra spaces leftover
# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
set.seed(123)
split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
training_set = subset(dtm_df, split == TRUE)
test_set = subset(dtm_df, split == FALSE)
classifier = randomForest(x = training[-692],
y = training$Liked,
ntree = 10)
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-(ncol(dtm_df))], y = training[(ncol(dtm_df))], ntree = 10)
training[(ncol(dtm_df))]
training[-(ncol(dtm_df))]
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-(ncol(dtm_df))], y = training[(ncol(dtm_df))], ntree = 10)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-as.numeric(ncol(dtm_df))],
y = training[as.numeric(ncol(dtm_df))],
ntree = 10)
ncol(dtm_df)
-as.numeric(ncol(dtm_df))
as.numeric(ncol(dtm_df))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(dtm_df)
restaraunts_rf <- randomForest(x = training[-cols],
y = training[cols],
ntree = 10)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(dtm_df)
restaraunts_rf <- randomForest(x = training[-692],
y = training[692],
ntree = 10)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-(ncol(dtm_df))], y = training[(ncol(dtm_df))], ntree = 10)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training[692], ntree = 10)
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
## import data
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "", stringsAsFactors = F)
#sep = "\t",
glimpse(restaraunts)
head(restaraunts)
corpus <- VCorpus(VectorSource(restaraunts$Review))
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])
# clean data + reduce # of cols
corpus %<>%
tm_map(content_transformer(tolower)) %>% # need content_transformer() to keep corpus as a VCorpus, not a list
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords()) %>% # remove English stopwords
tm_map(stemDocument) %>% # get the ROOTs of each word via stemming
tm_map(stripWhitespace) # get rid of extra spaces leftover
# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training[692], ntree = 10)
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
## import data
restaraunts <- read.delim("../data/Restaurant_Reviews.tsv", quote = "", stringsAsFactors = F)
#sep = "\t",
glimpse(restaraunts)
head(restaraunts)
corpus <- VCorpus(VectorSource(restaraunts$Review))
# check original text
as.character(corpus[[1]])
as.character(corpus[[841]])
# clean data + reduce # of cols
corpus %<>%
tm_map(content_transformer(tolower)) %>% # need content_transformer() to keep corpus as a VCorpus, not a list
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords()) %>% # remove English stopwords
tm_map(stemDocument) %>% # get the ROOTs of each word via stemming
tm_map(stripWhitespace) # get rid of extra spaces leftover
# check reviews to make sure functions worked
as.character(corpus[[1]])
as.character(corpus[[841]])
# create document term matrix
(dtm <- DocumentTermMatrix(corpus))
# check # of words
ncol(dtm)
# keep 99% of cols with the most `1` values = most frequent words
(dtm <- removeSparseTerms(dtm, sparse = .999))
# check # of words
ncol(dtm)
library(randomForest)
library(caTools) # split data
# transform matrix (contains the predictors) into a data frame
dtm_df <- as.data.frame(as.matrix(dtm))# as.data.frame(cbind(, Liked = as.factor(restaraunts$Liked)))
dtm_df$Liked <- factor(restaraunts$Liked, levels = c(0,1))
str(dtm_df[,1:5])
str(dtm_df[,(ncol(dtm_df)-1):ncol(dtm_df)])
# set.seed(123)
# split = sample.split(dtm_df$Liked, SplitRatio = 0.8)
# training_set = subset(dtm_df, split == TRUE)
# test_set = subset(dtm_df, split == FALSE)
#
# classifier = randomForest(x = training[-692],
#                           y = training$Liked,
#                           ntree = 10)
#
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test[-692])
#
# # Making the Confusion Matrix
# (cm = table(test[, 692], y_pred))
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
restaraunts_rf <- randomForest(x = training[-692], y = training$Liked, ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncols(training)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(training)
restaraunts_rf <- randomForest(x = training[-cols], y = training$Liked, ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or Recall): ",specificity)
cat("\nPrecision (Positive Predictive Value): ",precision)
cat("\nAccuracy: ",accuracy)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(training)
restaraunts_rf <- randomForest(x = training[-cols], y = training[cols], ntree = 10)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(training)
restaraunts_rf <- randomForest(x = training[-cols], y = training[,cols], ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-ncol(dtm_df)])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or Recall): ",specificity)
cat("\nPrecision (Positive Predictive Value): ",precision)
cat("\nAccuracy: ",accuracy)
set.seed(123)
spl <- sample.split(dtm_df$Liked, SplitRatio = .8)
training <- dtm_df %>% subset(spl == TRUE)
test <- dtm_df %>% subset(spl == FALSE)
cols <- ncol(training)
restaraunts_rf <- randomForest(x = training[-cols], y = training[,cols], ntree = 10)
# Predict using model on test set labels
y_pred_rf <- predict(restaraunts_rf, test[-cols])
head(y_pred_rf)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or Recall): ",specificity)
cat("\nPrecision (Positive Predictive Value): ",precision)
cat("\nAccuracy: ",accuracy)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_rf))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1 or 0, yes or no, etc.)
summary(restaraunts_logRegression)
tidy(restaraunts_logRegression)
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
# Predict using model on test set labels
y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_logreg))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
# Predict using model on test set labels
temp_y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
temp_y_pred_logreg <- if_else(temp_y_pred_logreg > .5, 1, 0)
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
# Predict using model on test set labels
temp_y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
y_pred_logreg <- if_else(temp_y_pred_logreg > .5, 1, 0)
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
suppressMessages(library(tidyverse))
suppressMessages(library(ggplot2))
suppressMessages(library(magrittr))
suppressMessages(library(tm)) # text mining
suppressMessages(library(SnowballC)) # stopwords
## fit to training data
restaraunts_logRegression <- glm(Liked ~ ., training, family = "binomial") # binomial regression (1/0, yes/no, etc.)
# Predict using model on test set labels
temp_y_pred_logreg <- predict(restaraunts_logRegression, test[-cols], type= "response")
y_pred_logreg <- if_else(temp_y_pred_logreg > .5, 1, 0)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_logreg))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
## fit to training data
y_pred_knn <- knn(training[,-cols], test[,-cols], cl = training[cols], k = 5)
library(class)
## fit to training data
y_pred_knn <- knn(training[,-cols], test[,-cols], cl = training[cols], k = 5)
training[cols]
library(class)
## fit to training data
y_pred_knn <- knn(training[,-cols], test[,-cols], cl = training[,cols], k = 5)
## Making the Confusion Matrix
(cm <- table(test$Liked, y_pred_knn))
sensitivity <- cm[4] / (cm[4] + cm[2]) # true positive rate = TP / (TP + FN)
specificity <- cm[1] / (cm[1] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[2]) # TP / predicted Pos
f1 <- (2*precision*specificity)/(precision + specificity)
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n
cat("\nSensitivity (TP rate): ",sensitivity)
cat("\nSpecificity (TN rate or \"Recall\", or \"measuring completeness\"): ",specificity)
cat("\nPrecision (Positive Predictive Value, or \"measuring exactness\"): ",precision)
cat("\nF1 Score (compromise between Precision and Recall): ",f1)
cat("\nAccuracy: ",accuracy)
# Load readxl
library(readxl)
# Import mbta.xlsx and skip first row: mbta
mbta <- read_excel("mbta.xlsx", skip = 1)
str(mbta)
# View the first 6 rows of mbta
head(mbta)
# View a summary of mbta
summary(mbta)
# Remove rows 1, 7, and 11 of mbta: mbta2
mbta2 <- mbta[-c(1,7,11),]
# Remove the first column of mbta2: mbta3
mbta3 <- mbta2[,-c(1)]
## mbta3 is pre-loaded
# Load tidyr
library(tidyr)
# Gather columns of mbta3: mbta4, not selecting mode
mbta4 <- gather(mbta3, key = month, value = thou_riders, -mode)
# View the head of mbta4
head(mbta4)
## mbta3 is pre-loaded
# Load tidyr
library(tidyr)
# Gather columns of mbta3: mbta4, not selecting mode
mbta4 <- gather(mbta3, key = month, value = thou_riders, -mode)
# View the head of mbta4
head(mbta4)
mbta5 <- spread(mode,  month, thou_riders)
mbta5 <- spread(mode,  month)
mbta5 <- spread(mbta4, mode)
mbta5 <- spread(mbta4, mode, month)
mbta5 <- spread(mbta4, mode, thou_rideres)
mbta5 <- spread(mbta4, mode, thou_riders)
head(mbta5)
