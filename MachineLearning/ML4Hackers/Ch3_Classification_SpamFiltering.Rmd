---
title: "Ch3 - Classification: Spam Filtering"
author: "Steve Newns"
date: "November 8, 2017"
output: html_document
---

# This or That: Binary Classification

A separating hyperplane **decision boundary** b/c typically working w/ data that can’t be classified
properly using only straight lines.

Producing general-purpose tools that handle problems where a decision boundary isn’t a single straight line has been 1 of the great achievements of ML

1 approach in particular = the **kernel trick** = has remarkable property of allowing us to work w/ much more sophisticated decision boundaries at almost no additional computational cost (coming later)

Assume we have a set of labeled examples of categories we want to learn how to ID that consist of a label/class + a series of measured variables that *describe* each example = **features/predictors** 

Examples of classifications can be found anywhere you look for them:
<ul>
<li> Given the results of a mammogram, does a patient have breast cancer? </li>
<li> Do blood pressure measurements suggest that a patient has hypertension? </li>
<li> Does a political candidate’s platform suggest she is a Republican or Democratic candidate? </li>
<li> Does a picture uploaded to a social network contain a face in it? </li>
<li> Was The Tempest written by William Shakespeare or Francis Bacon? </li>
</ul>

# Build a system for deciding whether an email is spam or ham.

Raw data comes from `SpamAssassin` public **corpus**, available for free download @ http://spamassassin.apache.org/old/publiccorpus/. 

At the unprocessed stage, features = simply the contents of the raw email as plain text which provides us w/ our 1st problem = need to transform our *raw text* data into a set of **features that describe qualitative concepts in a quantitative way**. 

In our case, that will be a 0/1 coding strategy: `spam` or `ham` 

May want to determine if containing HTML tags make an email more likely to be spam. To answer this, we need a strategy for turning the text in email into numbers. 

Fortunately, general-purpose text-mining packages available in R will do much of this work for us so we can focus on building up intuition for types of features people have used in the past when working w/ text data. 

**Feature generation/engineering** = a major topic in current ML research + is still very far from being automated in a general-purpose way. At present, it’s best to think of the features being used as part of a vocabulary of ML that you become more familiar w/ as you perform more ML tasks.

**Just as learning words of a new language builds up an intuition for what could realistically be a word, learning about the features people have used in the past builds up an intuition for what features could reasonably be helpful in the future.**

When working w/ text, historically the most important type of feature used = word count. If we think text of HTML tags are strong indicators of if email is spam, we might pick terms like “html” + “table” + count
how often they occur in 1 type of document vs. the other. 

A plot of points might not be very informative if too many DPs overlap (comes up quite often w/ data that contains only a few unique values for 1+ variables). As this is a recurring problem, there is a standard graphical solution: simply add random noise to plotted values which will “separate out” the points to reduce amount of over-plotting = **jittering**

In practice, we can do a much better job by using many more than just these 2 very obvious terms like "html" and "table". The final analysis uses several thousand terms + even though we’ll only use word-count data, we’ll still get a relatively accurate classification.

In the real world, you’d want to use other features beyond word counts, like falsified headers, IP or email black lists, etc., but here we only want to introduce the basics of text classification.

# Moving Gently into Conditional Probability

At its core, text classification = 20th-century application of the 18th-century concept of **conditional probability** = likelihood of observing 1 thing *given some other thing already known* like probability a college student is female given we already know the student’s major is CS. 

According to a National Science Foundation survey in 2005, only 22% of undergrad CS majors were female but 51% of undergrad science majors overall were female, so conditioning on being a CS major lowers
chances of being a woman from 51% to 22%.

**Naive Bayes classifier** = looks for differences of this sort by searching through text for words that are either:
<ul>
<li> noticeably more likely to occur in spam messages </li>
<li> noticeably more likely to occur in ham messages.  </li>
</ul>

When a word is noticeably more likely to occur in 1 context rather than the other, its occurrence can be diagnostic of whether a new message is spam or ham. The logic is simple: if you see a single word more likely to occur in spam than ham = evidence the email as a whole is spam. If you see *many* words more likely to occur in spam than ham + very few words more likely to occur in ham than spam = strong evidence the email as a whole is spam.

Ultimately, our text classifier formalizes this intuition by computing
<ul>
<li> probability of seeing exact contents of an email conditioned on the email being assumed to be
spam
<li> probability of seeing the same email’s contents conditioned on the email being assumed to be ham. 
</ul>

If it’s much more likely that we'd see the email in question if it were spam, declare it to be spam.

How much more likely a message needs to be to merit being labeled spam depends upon an additional piece of info: the **base rate** of seeing spam messages = the **prior** 

W/ email, prior comes into play b/c the majority of email sent is spam, which means even weak evidence an email is spam can be sufficient to justify labeling it as spam.

To compute probability of an email, assume the occurrence counts for every word can be estimated in isolation from all of the other words = **statistical independence**.

When we make this assumption w/out being certain it’s correct, we say our model is **naive.** B/c we also make use of base rate info about spam, the model is also a **Bayes model**. Ttogether, these 2 traits make a **Naive Bayes classifier**.

# Writing Our First Bayesian Spam Classifier

`SpamAssassin` public corpus consists of labeled emails from 3 categories: “spam,” “easy ham,” + “hard ham.” (more difficult to distinguish from spam than easy stuff + often includes HTML tags, which is 1 way to easily ID spam)

To more accurately classify hard ham, we must to include more info from many more text features + extracting these features requires some **text mining** of the email files + constitutes our initial step in creating a classifier.

All raw email files include headers + message text. Header contains a lot of info about where an email comes from. Despite the fact there is a lot of useful info contained in the headers, we will not be using any of this info in our classifier. 

Rather than focus on features contained in the *transmission* of the message, we're interested in how the *contents* of messages themselves can help predict an email’s type (not to say one should always ignore the header or any other info as all sophisticated modern spam filters utilize info contained in email message headers, such as whether portions of it appear to have been forged, whether the message
is from a known spammer, or whether there are bits missing)

B/c we're focusing on only the email message/body, we need to extract this text from the message files, which always begin after the 1st full line break in the email file. 

To begin building our classifier, 1st create R functions that can access the files + extract the message text by taking advantage of this text convention.

```{r}
library(tidyverse)
library(tm) # text mining
library(ggplot2)
library(magrittr)

spam_path <- "./data/spam/"
spam2_path <- "./data/spam_2/"
easyham_path <- "./data/easy_ham/"
easyham2_path <- "./data/easy_ham_2/"
hardham_path <- "./data/hard_ham/"
hardham2_path <- "./data/hard_ham_2/"
```

Now to create text corpuses from both sets of files via function that opens each file, finds the 1st line break, + returns the text below that break as a character vector w/ a single text element.

```{r}
get.msg <- function(path) {
 con <- file(path, open="rt", encoding="latin1")
 text <- readLines(con)
 # The message always begins after the first full line break
 msg <- text[seq(which(text=="")[1]+1,length(text), by = 1)]
 close(con)
 return(paste(msg, collapse="\n"))
}

spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which(spam.docs!="cmds")]
#all.spam <- sapply(spam.docs, function(p) get.msg(paste(spam.path,p ,sep="")))
```
get.tdm <- function(doc.vec) {
 doc.corpus <- Corpus(VectorSource(doc.vec))
 control <- list(stopwords=TRUE, removePunctuation=TRUE, removeNumbers=TRUE,
 minDocFreq=2)
 doc.dtm <- TermDocumentMatrix(doc.corpus, control)
 return(doc.dtm)
}
spam.tdm <- get.tdm(all.spam)

spam.matrix <- as.matrix(spam.tdm)
spam.counts <- rowSums(spam.matrix)
spam.df <- data.frame(cbind(names(spam.counts),
 as.numeric(spam.counts)), stringsAsFactors=FALSE)
names(spam.df) <- c("term","frequency")
spam.df$frequency <- as.numeric(spam.df$frequency)
spam.occurrence <- sapply(1:nrow(spam.matrix),
 function(i) {length(which(spam.matrix[i,] > 0))/ncol(spam.matrix)})
spam.density <- spam.df$frequency/sum(spam.df$frequency)
spam.df <- transform(spam.df, density=spam.density,
 occurrence=spam.occurrence)


```{r}
get_msg <- function(path) {
  # Function to grab all text after 
  # 1st line break in a text file 
  # and returns it in a character vector
  
  #open connection and read in the lines, each as seperate element of character vector
  connection <- file(path, open = "rt", encoding = "latin1")
  text <- readLines(path)
  
  # find 1st line break (1st empty element) and get all text after this position
  # get index of 1st empty element, add 1, + generate all indexes from this to the end
  msg <- text[seq(which(text == "")[1] + 1, length(text), 1)]
  close(connection)
  
  # get vector into a single character vector
  return(paste(msg, collapse = "\n"))
}
```

R performs file I/O in a very similar way to many other programming languages. This function takes a file path as a string + opens that file in **rt** = "read as text". Also notice `encoding = latin1` b/c many of the email messages contain non-ASCII characters + this encoding allows us to use these files. 

To train our classifier, need to get the email messages from all spam + ham emails. 1 approach = create a vector containing all messages such that each element of the vector = a single email. The most straightforward way to accomplish this in R = use an `apply` w/ our newly created `get.msg`
```{r}
# get all file + dir names in input dir
spam_docs <- dir(spam_path)

# ignore cmds files (long list of UNIX base commands to move files in that dir)
spam_docs <- spam_docs[which(spam_docs != "cmds")]

# get text from each email + return in single character vector
# use anonymous function to concatenate file name + path
all_spam <- sapply(spam_docs, function(x) get_msg(paste(spam_path, x, sep="")))

head(all_spam)
```
Now to create a **text corpus** from our vector of emails using the `tm` functions. Once we have the text represented as a corpus, we can manipulate the terms in the messages to begin building our **feature set** for the spam classifier. 

Huge advantage of `tm` package = much of the heavy lifting needed to clean + normalize text is hidden from view. What we will accomplish in a few lines of code would take many lines of string processing if we had to perform these operations ourselves in a lower-level language.

1 way of quantifying frequency of terms = construct a **term document matrix (TDM)** = N×M matrix in which terms found among all documents in a given corpus define rows + all documents in the corpus define columns.  The [i, j] cell of this matrix = # of times term i was found in document j.

As before, we will define a simple function, get.tdm, that will take a vector of email
messages and return a TDM:
```{r}
get_tdm <- function(vec) {
  ## Function to take a vector of email messages 
  ## and return a TDM
  
  # create a VectorSource of input vector and then create Corpus from this source
  # see other sources with ?getSource
  doc_corpus <- Corpus(VectorSource(vec))
  
  # create control = special list of options specifying how to distill text
  # remove 488 common English stopwords, punctuation, + numbers to reduce noise associated 
  #   with many characters (many emails contain HTML tags)
  # only get terms that are returned more than once w/ minDocFreq
  ctrl <- list(stopwords = TRUE, removePunctuation = TRUE, removeNumbers = TRUE, minDocFreq = 2)
  
  # build training set for classifier
  doc_dtm <- TermDocumentMatrix(doc_corpus, ctrl)
  return(doc_dtm)
}

spam_tdm <- get_tdm(all.spam)
```

Good approach to create training data = construct a data frame w/ all observed probabilities for each term, given we know it is spam.

```{r}

## Train classifier to know the probability an email is spam, given the observation of some term.

# turn the TDM intro a matrix and get a vector w/ total frequency counts for each term across all docs
spam_matrix <- as.matrix(spam_tdm)
spam_counts <- rowSums(spam_matrix)

# create a data frame from these counts and make sure strings aren't factors
spam_df <- data.frame(cbind(names(spam_counts), as.numeric(spam_counts)), stringsAsFactors = F)

# rename the columns and convert frequency back to numbers
names(spam_df) <- c("term","frequency")
spam_df %<>%
   mutate(frequency = as.numeric(frequency))

# calculate % of docs in which a given term in this matrix occurs
spam_occurrence <- sapply(1:nrow(spam_matrix),
  # pass through each term in the doc matrix and find cells w/ a positive element (a count > 0)
  # divide by total # of cols in the TDM/# of docs in the spam corpus
  function(i) {length(which(spam_matrix[i,] > 0))/ncol(spam_matrix)})

# calculate frequency of each word w/in entire corpus
spam_density <- spam_df$frequency/sum(spam_df$frequency)

# add new cols occurence + density vectors to data frame
spam_df <- spam_df %>%
  mutate(density = spam_density, 
         occurrence = spam_occurrence)

head(spam_df)
```

We have now generated the training data for spam classification. 

```{r}
# check which terms are the strongest indicators of spam given our training data
# by sorting terms by occurence in a copy of the data frame in with()
head(spam_df[with(spam_df, order(-occurrence)),])
```
HTML tags appear to be the strongest text features associated w/ `spam` Over 80% of the messages in the spam training data contain the term "html"", as well as other common HTML-related terms, such as body, table, font,
and head. Note, however, that these terms are not the most frequent by raw count. You
can see this for yourself by replacing -occurrence with -frequency in the preceding
statement. This is very important in terms of how we define our classifier. If we used
raw count data and the subsequent densities as our training data, we might be over
weighting certain kinds of spam—specifically, spam that contains HTML tables. However,
we know that not all spam messages are constructed this way. As such, a better
approach is to define the conditional probability of a message being spam based on
how many messages contain the term.
Writing Our First Bayesian Spam Classifier | 83
www.it-ebooks.info
Now that we have the spam training data, we need to balance it with the ham training
data. As part of the exercise, we will build this training data using only the easy ham
messages. Of course, it would be possible to incorporate the hard ham messages into
the training set; in fact, that would be advisable if we were building a production system.
But within the context of this exercise, it’s helpful to see how well a text classifier will
work if trained using only a small corpus of easily classified documents.
We will construct the ham training data in exactly the same way we did the spam, and
therefore we will not reprint those commands here. The only way this step differs from
generating the spam training data is that we use only the first 500 email messages in
the data/easy_ham folder.
You may note that there are actually 2,500 ham emails in this directory. So why are we
ignoring four-fifths of the data? When we construct our first classifier, we will assume
that each message has an equal probability of being ham or spam. As such, it is good
practice to ensure that our training data reflects our assumptions. We only have 500
spam messages, so we will limit or ham training set to 500 messages as well.
To see how we limited the ham training data in this way, see line 102 in
the email_classify.R file for this chapter.
Once the ham training data has been constructed, we can inspect it just as we did the
spam for comparison:
```{r}
ham_docs <- dir(easyham_path)
ham_docs <- ham_docs[which(ham_docs != "cmds")]
all_ham <- sapply(ham_docs, function(x) get_msg(paste(easyham_path, x, sep="")))
ham_tdm <- get_tdm(all_ham)
ham_matrix <- as.matrix(ham_tdm)
ham_counts <- rowSums(ham_matrix)

# create a data frame from these counts and make sure strings aren't factors
ham_df <- data.frame(cbind(names(ham_counts), as.numeric(ham_counts)), stringsAsFactors = F)

# rename the columns and convert frequency back to numbers
names(ham_df) <- c("term","frequency")
ham_df %<>%
   mutate(frequency = as.numeric(frequency))

# calculate % of docs in which a given term in this matrix occurs
ham_occurrence <- sapply(1:nrow(ham_matrix),
  # pass through each term in the doc matrix and find cells w/ a positive element (a count > 0)
  # divide by total # of cols in the TDM/# of docs in the ham corpus
  function(i) {length(which(ham_matrix[i,] > 0))/ncol(ham_matrix)})

# calculate frequency of each word w/in entire corpus
ham_density <- ham_df$frequency/sum(ham_df$frequency)

# add new cols occurence + density vectors to data frame
ham_df <- ham_df %>%
  mutate(density = ham_density, 
         occurrence = ham_occurrence)
```
```{r}
head(ham_df[with(ham_df, order(-occurrence)),])
```
head(spam_df)
head(all_spam)
head(easyham_df[with(easyham.df, order(-occurrence)),])
 term frequency density occurrence
3553 yahoo 185 0.008712853 0.180
966 dont 141 0.006640607 0.090
2343 people 183 0.008618660 0.086
1871 linux 159 0.007488344 0.084
1876 list 103 0.004850940 0.078
3240 time 91 0.004285782 0.064

```{r}
classify.email <- function(path, training.df, prior=0.5, c=1e-6) {
 msg <- get.msg(path)
 msg.tdm <- get.tdm(msg)
 msg.freq <- rowSums(as.matrix(msg.tdm))
 # Find intersections of words
 msg.match <- intersect(names(msg.freq), training.df$term)
 if(length(msg.match) < 1) {
 return(prior*c^(length(msg.freq)))
 }
 else {
 match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
 return(prior * prod(match.probs) * c^(length(msg.freq)-length(msg.match)))
 }
}

hardham.docs <- dir(hardham_path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
hardham.spamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham_path, p, sep=""),
 training.df=spam.df))
hardham.hamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham_path, p, sep=""),
 training.df=ham_df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest, TRUE, FALSE)
summary(hardham.res)
```
```{r}
spam.classifier <- function(path) {
 pr.spam <- classify.email(path, spam.df)
 pr.ham <- classify.email(path, easyham.df)
 return(c(pr.spam, pr.ham, ifelse(pr.spam > pr.ham, 1, 0)))
}
```

The first thing you will notice in the ham training data is that the terms are much more
sparsely distributed among the emails. The term that occurs in the most documents,
“yahoo,” does so in only 18% of them. The other terms all occur in less than 10% of
the documents. Compare this to the top spam terms, which all occur in over 24% of
the spam emails. Already we can begin to see how this variation will allow us to separate
spam from ham. If a message contains just one or two terms strongly associated with
spam, it will take a lot of nonspam words for the message to be classified as ham. With
both training sets defined, we are now ready to complete our classifier and test it!
84 | Chapter 3: Classification: Spam Filtering
www.it-ebooks.info
Defining the Classifier and Testing It with Hard Ham
We want to define a classifier that will take an email message file and calculate the
probability that it is spam or ham. Fortunately, we have already created most of the
functions and generated the data needed to perform this calculation. Before we can
proceed, however, there is one critical complication that we must consider.
We need to decide how to handle terms in new emails that match terms in our training
set and how to handle terms that do not match terms in our training set (see
Figure 3-3). To calculate the probability that an email message is spam or ham, we will
need to find the terms that are common between the training data and the message in
question. We can then use the probabilities associated with these terms to calculate
the conditional probability that a message is of the training data’s type. This is fairly
straightforward, but what do we do with the terms from the email being classified that
are not in our training data?
To calculate the conditional probability of a message, we combine the probabilities of
each term in the training data by taking their product. For example, if the frequency of
seeing html in a spam message is 0.30 and the frequency of seeing table in a spam
message is 0.10, then we’ll say that the probability of seeing both in a spam message is
0.30 × 0.10 = 0.03. But for those terms in the email that are not in our training data,
we have no information about their frequency in either spam or ham messages. One
possible solution would be to assume that because we have not seen a term yet, its
probability of occurring in a certain class is zero. This, however, is very misguided.
First, it is foolish to assume that we will never see a term in the entire universe of spam
and ham simply because we have not yet seen it. Moreover, because we calculate conditional
probabilities using products, if we assigned a zero probability to terms not in
our training data, elementary arithmetic tells us that we would calculate zero as the
probability of most messages, because we would be multiplying all the other probabilities
by zero every time we encountered an unknown term. This would cause
catastrophic results for our classifier because many, or even all, messages would be
incorrectly assigned a zero probability of being either spam or ham.
Researchers have come up with many clever ways of trying to get around this problem,
such as drawing a random probability from some distribution or using natural language
processing (NLP) techniques to estimate the “spamminess” of a term given its context.
For our purposes, we will use a very simple rule: assign a very small probability to terms
that are not in the training set. This is, in fact, a common way of dealing with missing
terms in simple text classifiers, and for our purposes it will serve just fine. In this exercise,
by default we will set this probability to 0.0001%, or one-ten-thousandth of a
percent, which is sufficiently small for this data set. Finally, because we are assuming
that all emails are equally likely to be ham or spam, we set our default prior belief that
an email is of some type to 50%. In order to return to this problem later, however, we
construct the classify.email function such that the prior can be varied.
Writing Our First Bayesian Spam Classifier | 85
www.it-ebooks.info
Figure 3-3. Jittered frequency of the terms “html” and “table” by email type
Be wary of always using 0.0001% for terms not in a training set. We are
using it in this example, but in others it may be too large or too small,
in which case the system you build will not work at all!
86 | Chapter 3: Classification: Spam Filtering
www.it-ebooks.info
classify.email <- function(path, training.df, prior=0.5, c=1e-6) {
 msg <- get.msg(path)
 msg.tdm <- get.tdm(msg)
 msg.freq <- rowSums(as.matrix(msg.tdm))
 # Find intersections of words
 msg.match <- intersect(names(msg.freq), training.df$term)
 if(length(msg.match) < 1) {
 return(prior*c^(length(msg.freq)))
 }
 else {
 match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
 return(prior * prod(match.probs) * c^(length(msg.freq)-length(msg.match)))
 }
}
You will notice that the first three steps of the classify.email function proceed just as
our training phase did. We must extract the message text with get.msg, turn it into a
TDM with get.tdm, and finally calculate the frequency of terms with rowSums. Next, we
need to find how the terms in the email message intersect with the terms in our training
data, as depicted in Figure 3-3. To do so, we use the intersect command, passing the
terms found in the email message and those in the training data. What will be returned
are those terms in the gray shaded area of Figure 3-3.
The final step of the classification is to determine whether any of the words in the email
message are present in the training set, and if so, we use them to calculate the probability
that this message is of the class in question.
Assume for now that we are attempting to determine if this email message is spam.
msg.match will contain all of the terms from the email message in our spam training
data, spam.df. If that intersection is empty, then the length of msg.match will be less than
zero, and we can update our prior only by multiplying it with the product of the number
of terms in the email with our tiny probability value: c. The result will be a tiny probability
of assigning the spam label to the email.
Conversely, if this intersection is not empty, we need to find those terms from the email
in our training data and look up their occurrence probabilities. We use the match function
to do the lookup, which will return the term’s element position in the term column
of our training data. We use these element positions to return the corresponding probabilities
from the occurrence column, and return those values to match.probs. We then
calculate the product of these values and combine it with our prior belief about the
email being spam with the term probabilities and the probabilities of any missing terms.
The result is our Bayesian estimate for the probability that a message is spam given the
matching terms in our training data.
As an initial test, we will use our training data from the spam and easy ham messages
to classify hard ham emails. We know that all of these emails are ham, so ideally our
classifier will assign a higher probability of being ham to all of these messages. We also
know, however, that hard ham messages are “hard” to classify because they contain
Writing Our First Bayesian Spam Classifier | 87
www.it-ebooks.info
terms that are also associated with spam. Now let’s see how our simple
classifier does!
hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
hardham.spamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham.path, p, sep=""),
 training.df=spam.df))
hardham.hamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham.path, p, sep=""),
 training.df=easyham.df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest, TRUE, FALSE)
summary(hardham.res)
Just as before, we need to get all of the file paths in order, and then we can test the
classifier for all hard ham messages by wrapping both a spam test and ham test in
sapply calls. The vectors hardham.spamtest and hardham.hamtest contain the
conditional probability calculations for each hard ham email of being either spam or
ham given the appropriate training data. We then use the ifelse command to compare
the probabilities in each vector. If the value in hardham.spamtest is greater than that in
hardham.hamtest, then the classifier has classified the message as spam; otherwise, it is
ham. Finally, we use the summary command to inspect the results, listed in Table 3-2.
Table 3-2. Testing our classifier against “hard ham”
Email type Number classified as ham Number classified as spam
Hard ham 184 65
Congratulations! You’ve written your first classifier, and it did fairly well at identifying
hard ham as nonspam. In this case, we have approximately a 26% false-positive rate.
That is, about one-quarter of the hard ham emails are incorrectly identified as spam.
You may think this is poor performance, and in production we would not want to offer
an email platform with these results, but considering how simple our classifier is, it is
doing quite well. Of course, a better test is to see how the classifier performs against
not only hard ham, but also easy ham and spam.
