---
title: "Ch3 - Classification: Spam Filtering"
author: "Steve Newns"
date: "November 8, 2017"
output: html_document
---

# This or That: Binary Classification

A separating hyperplane **decision boundary** b/c typically working w/ data that can’t be classified
properly using only straight lines.

Producing general-purpose tools that handle problems where a decision boundary isn’t a single straight line has been 1 of the great achievements of ML

1 approach in particular = the **kernel trick** = has remarkable property of allowing us to work w/ much more sophisticated decision boundaries at almost no additional computational cost (coming later)

Assume we have a set of labeled examples of categories we want to learn how to ID that consist of a label/class + a series of measured variables that *describe* each example = **features/predictors** 

Examples of classifications can be found anywhere you look for them:
<ul>
<li> Given the results of a mammogram, does a patient have breast cancer? </li>
<li> Do blood pressure measurements suggest that a patient has hypertension? </li>
<li> Does a political candidate’s platform suggest she is a Republican or Democratic candidate? </li>
<li> Does a picture uploaded to a social network contain a face in it? </li>
<li> Was The Tempest written by William Shakespeare or Francis Bacon? </li>
</ul>

# Build a system for deciding whether an email is spam or ham.

Raw data comes from `SpamAssassin` public **corpus**, available for free download @ http://spamassassin.apache.org/old/publiccorpus/. 

At the unprocessed stage, features = simply the contents of the raw email as plain text which provides us w/ our 1st problem = need to transform our *raw text* data into a set of **features that describe qualitative concepts in a quantitative way**. 

In our case, that will be a 0/1 coding strategy: `spam` or `ham` 

May want to determine if containing HTML tags make an email more likely to be spam. To answer this, we need a strategy for turning the text in email into numbers. 

Fortunately, general-purpose text-mining packages available in R will do much of this work for us so we can focus on building up intuition for types of features people have used in the past when working w/ text data. 

**Feature generation/engineering** = a major topic in current ML research + is still very far from being automated in a general-purpose way. At present, it’s best to think of the features being used as part of a vocabulary of ML that you become more familiar w/ as you perform more ML tasks.

**Just as learning words of a new language builds up an intuition for what could realistically be a word, learning about the features people have used in the past builds up an intuition for what features could reasonably be helpful in the future.**

When working w/ text, historically the most important type of feature used = word count. If we think text of HTML tags are strong indicators of if email is spam, we might pick terms like “html” + “table” + count
how often they occur in 1 type of document vs. the other. 

A plot of points might not be very informative if too many DPs overlap (comes up quite often w/ data that contains only a few unique values for 1+ variables). As this is a recurring problem, there is a standard graphical solution: simply add random noise to plotted values which will “separate out” the points to reduce amount of over-plotting = **jittering**

In practice, we can do a much better job by using many more than just these 2 very obvious terms like "html" and "table". The final analysis uses several thousand terms + even though we’ll only use word-count data, we’ll still get a relatively accurate classification.

In the real world, you’d want to use other features beyond word counts, like falsified headers, IP or email black lists, etc., but here we only want to introduce the basics of text classification.

# Moving Gently into Conditional Probability

At its core, text classification = 20th-century application of the 18th-century concept of **conditional probability** = likelihood of observing 1 thing *given some other thing already known* like probability a college student is female given we already know the student’s major is CS. 

According to a National Science Foundation survey in 2005, only 22% of undergrad CS majors were female but 51% of undergrad science majors overall were female, so conditioning on being a CS major lowers
chances of being a woman from 51% to 22%.

**Naive Bayes classifier** = looks for differences of this sort by searching through text for words that are either:
<ul>
<li> noticeably more likely to occur in spam messages </li>
<li> noticeably more likely to occur in ham messages.  </li>
</ul>

When a word is noticeably more likely to occur in 1 context rather than the other, its occurrence can be diagnostic of whether a new message is spam or ham. The logic is simple: if you see a single word more likely to occur in spam than ham = evidence the email as a whole is spam. If you see *many* words more likely to occur in spam than ham + very few words more likely to occur in ham than spam = strong evidence the email as a whole is spam.

Ultimately, our text classifier formalizes this intuition by computing
<ul>
<li> probability of seeing exact contents of an email conditioned on the email being assumed to be
spam
<li> probability of seeing the same email’s contents conditioned on the email being assumed to be ham. 
</ul>

If it’s much more likely that we'd see the email in question if it were spam, declare it to be spam.

How much more likely a message needs to be to merit being labeled spam depends upon an additional piece of info: the **base rate** of seeing spam messages = the **prior** 

W/ email, prior comes into play b/c the majority of email sent is spam, which means even weak evidence an email is spam can be sufficient to justify labeling it as spam.

To compute probability of an email, assume the occurrence counts for every word can be estimated in isolation from all of the other words = **statistical independence**.

When we make this assumption w/out being certain it’s correct, we say our model is **naive.** B/c we also make use of base rate info about spam, the model is also a **Bayes model**. Ttogether, these 2 traits make a **Naive Bayes classifier**.

# Writing Our First Bayesian Spam Classifier

`SpamAssassin` public corpus consists of labeled emails from 3 categories: “spam,” “easy ham,” + “hard ham.” (more difficult to distinguish from spam than easy stuff + often includes HTML tags, which is 1 way to easily ID spam)

To more accurately classify hard ham, we must to include more info from many more text features + extracting these features requires some **text mining** of the email files + constitutes our initial step in creating a classifier.

All raw email files include headers + message text. Header contains a lot of info about where an email comes from. Despite the fact there is a lot of useful info contained in the headers, we will not be using any of this info in our classifier. 

Rather than focus on features contained in the *transmission* of the message, we're interested in how the *contents* of messages themselves can help predict an email’s type (not to say one should always ignore the header or any other info as all sophisticated modern spam filters utilize info contained in email message headers, such as whether portions of it appear to have been forged, whether the message
is from a known spammer, or whether there are bits missing)

B/c we're focusing on only the email message/body, we need to extract this text from the message files, which always begin after the 1st full line break in the email file. 

To begin building our classifier, 1st create R functions that can access the files + extract the message text by taking advantage of this text convention.

```{r}
library(tidyverse)
library(tm) # text mining
library(ggplot2)
library(magrittr)

spam_path <- "./data/spam/"
spam2_path <- "./data/spam_2/"
easyham_path <- "./data/easy_ham/"
easyham2_path <- "./data/easy_ham_2/"
hardham_path <- "./data/hard_ham/"
hardham2_path <- "./data/hard_ham_2/"
```

Now to create text corpuses from both sets of files via function that opens each file, finds the 1st line break, + returns the text below that break as a character vector w/ a single text element.

```{r}
get_msg <- function(path) {
  # Function to grab all text after 
  # 1st line break in a text file 
  # and returns it in a character vector
  
  #open connection and read in the lines, each as seperate element of character vector
  connection <- file(path, open = "rt")#, encoding = "latin1")
  text <- readLines(path)
  
  # find 1st line break (1st empty element) and get all text after this position
  # get index of 1st empty element, add 1, + generate all indexes from this to the end
  from <- which(text == "")[1] + 1
  to <- length(text)
  msg <- text[seq(from, to, 1)]
  close(connection)
  
  # get vector into a single character vector
  return(paste(msg, collapse = "\n"))
}
```

R performs file I/O in a very similar way to many other programming languages. This function takes a file path as a string + opens that file in **rt** = "read as text". Also notice `encoding = latin1` b/c many of the email messages contain non-ASCII characters + this encoding allows us to use these files. 

To train our classifier, need to get the email messages from all spam + ham emails. 1 approach = create a vector containing all messages such that each element of the vector = a single email. The most straightforward way to accomplish this in R = use an `apply` w/ our newly created `get.msg`
```{r}
# get all file + dir names in input dir
spam_docs <- dir(spam_path)

# ignore cmds files (long list of UNIX base commands to move files in that dir)
spam_docs <- spam_docs[which(spam_docs != "cmds")]

# get text from each email + return in single character vector
# use anonymous function to concatenate file name + path
all_spam <- sapply(spam_docs, function(x) get_msg(paste(spam_path, x, sep="")))

head(all_spam)
```
Now to create a **text corpus** from our vector of emails using the `tm` functions. Once we have the text represented as a corpus, we can manipulate the terms in the messages to begin building our **feature set** for the spam classifier. 

Huge advantage of `tm` package = much of the heavy lifting needed to clean + normalize text is hidden from view. What we will accomplish in a few lines of code would take many lines of string processing if we had to perform these operations ourselves in a lower-level language.

1 way of quantifying frequency of terms = construct a **term document matrix (TDM)** = N×M matrix in which terms found among all documents in a given corpus define rows + all documents in the corpus define columns.  The [i, j] cell of this matrix = # of times term i was found in document j.

As before, we will define a simple function, get.tdm, that will take a vector of email
messages and return a TDM:
```{r}
get_tdm <- function(vec) {
  ## Function to take a vector of email messages 
  ## and return a TDM
  
  # create a VectorSource of input vector and then create Corpus from this source
  # see other sources with ?getSource
  doc_corpus <- Corpus(VectorSource(vec))
  
  # create control = special list of options specifying how to distill text
  # remove 488 common English stopwords, punctuation, + numbers to reduce noise associated 
  #   with many characters (many emails contain HTML tags)
  # only get terms that are returned more than once w/ minDocFreq
  ctrl <- list(stopwords = TRUE, removePunctuation = TRUE, removeNumbers = TRUE, minDocFreq = 2)
  
  # build training set for classifier
  doc_dtm <- TermDocumentMatrix(doc_corpus, ctrl)
  return(doc_dtm)
}

spam_tdm <- get_tdm(all_spam)
```

Good approach to create training data = construct a data frame w/ all observed probabilities for each term, given we know it is spam.

```{r}

## Train classifier to know the probability an email is spam, given the observation of some term.

# turn the TDM intro a matrix and get a vector w/ total frequency counts for each term across all docs
spam_matrix <- as.matrix(spam_tdm)
spam_counts <- rowSums(spam_matrix)

# create a data frame from these counts and make sure strings aren't factors
spam_df <- data.frame(cbind(names(spam_counts), as.numeric(spam_counts)), stringsAsFactors = F)

# rename the columns and convert frequency back to numbers
names(spam_df) <- c("term","frequency")
spam_df %<>%
   mutate(frequency = as.numeric(frequency))

# calculate % of docs in which a given term in this matrix occurs
spam_occurrence <- sapply(1:nrow(spam_matrix),
  # pass through each term in the doc matrix and find cells w/ a positive element (a count > 0)
  # divide by total # of cols in the TDM/# of docs in the spam corpus
  function(i) {length(which(spam_matrix[i,] > 0))/ncol(spam_matrix)})

# calculate frequency of each word w/in entire corpus
spam_density <- spam_df$frequency/sum(spam_df$frequency)

# add new cols occurence + density vectors to data frame
spam_df <- spam_df %>%
  mutate(density = spam_density, 
         occurrence = spam_occurrence)

head(spam_df)
```

We have now generated the training data for spam classification. 

```{r}
# check which terms are the strongest indicators of spam given our training data
# by sorting terms by occurence in a copy of the data frame in with()
head(spam_df[with(spam_df, order(-occurrence)),])
```
HTML tags appear to be the strongest text features associated w/ `spam` (Over 80% of the messages in the spam training data contain the term "html"), as well as other common HTML-related terms, such as body, table, font, and head. Note, however, that these terms are not the most frequent by raw count, seen by  replacing `occurrence` w/ `frequency` in the preceding statement. 

```{r}
head(spam_df[with(spam_df, order(-frequency)),])
```

This is very important in terms of how we define our classifier. If we used raw count data + the subsequent densities as our training data, we might be over-weighting certain kinds of spam that contains HTML tables. However, we know not all spam messages are constructed this way. As such, a better
approach is to define the **conditional probability** of a message being spam based on how many messages contain the term.


Now that we have the spam training data, balance it w/ the `easy ham` (possible to incorporate `hard ham` messages into the training set (advisable if building a production system), but w/in the context of this exercise, it’s helpful to see how well a text classifier will work if trained using only a small corpus of easily classified documents)

Construct the ham training data in exactly the same way we did the spam, + only difference = use only the 1st 500 email messages in `easy_ham`. There are actually 25k ham emails, but when constructing our 1st classifier, assume each message has an equal probability of being ham or spam. As such, it is good
practice to ensure our training data reflects our assumptions. 

We only have 500 spam messages, so limit ham training set to 500 messages as well.

```{r}
ham_docs <- dir(easyham_path)
ham_docs <- ham_docs[which(ham_docs != "cmds")]

# limit from 26k to 500
ham_docs <- ham_docs[1:500]

all_ham <- sapply(ham_docs, function(x) get_msg(paste(easyham_path, x, sep="")))
ham_tdm <- get_tdm(all_ham)
ham_matrix <- as.matrix(ham_tdm)
ham_counts <- rowSums(ham_matrix)

# create a data frame from these counts and make sure strings aren't factors
ham_df <- data.frame(cbind(names(ham_counts), as.numeric(ham_counts)), stringsAsFactors = F)

# rename the columns and convert frequency back to numbers
names(ham_df) <- c("term","frequency")
ham_df %<>%
   mutate(frequency = as.numeric(frequency))

# calculate % of docs in which a given term in this matrix occurs
ham_occurrence <- sapply(1:nrow(ham_matrix),
  # pass through each term in the doc matrix and find cells w/ a positive element (a count > 0)
  # divide by total # of cols in the TDM/# of docs in the ham corpus
  function(i) {length(which(ham_matrix[i,] > 0))/ncol(ham_matrix)})

# calculate frequency of each word w/in entire corpus
ham_density <- ham_df$frequency/sum(ham_df$frequency)

# add new cols occurence + density vectors to data frame
ham_df <- ham_df %>%
  mutate(density = ham_density, 
         occurrence = ham_occurrence)
```
```{r}
head(ham_df[with(ham_df, order(-occurrence)),])
```

The 1st thing you will notice in `ham` training data is terms are much more sparsely distributed among emails. The term that occurs in the most documents, “yahoo,” does so in only 18% of them. The other terms all occur in less than 10% of the documents. Compare this to the top spam terms, which all occur in over 24% of spam emails. Already we can begin to see how this variation will allow us to separate
spam from ham. 

If a message contains just 1 or 2 terms strongly associated w/ spam, + it will take a lot of nonspam words for the message to be classified as ham. W/ both training sets defined, we are now ready to complete our classifier and test it!


# Defining the Classifier and Testing It with Hard Ham

We want to define a classifier that will take an email message file + calculate the probability it is spam or ham. Fortunately, we have already created most of the functions + generated the data needed to perform this calculation. 

Before proceedING, there is 1 critical complication that we must consider = need to decide how to handle terms in new emails that do and do not match terms in our training set (see Figure 3-3). 

To calculate probability an email is spam or ham, we need to find terms common between the training data + the message in question + then use the probabilities associated w/ these terms to calculate the conditional probability a message is of the training data’s type. 

This is fairly straightforward, but what do we do w/ the terms not in our training data? To calculate the conditional probability of a message, we combine probabilities of each term in the training data by taking their product (ex: if frequency of "html" in a spam message = 0.30 + frequency of "table" in a spam message = 0.10, we’ll say the probability of seeing both in a spam message = 0.30 × 0.10 = 0.03).

But terms in the email *not* in our training data, we have no info about their frequency in either spam or ham messages. 1 possible solution = assume that b/c we have not seen a term yet, its probability of occurring in a certain class = 0. This, however, is very misguided.

1st = foolish to assume we will never see a term in the entire universe of spam + ham simply b/c we have not yet seen it. Moreover, b/c we calculate conditional probabilities using products, if we assigned a 0 probability to terms not in our training data, we'd calculate 0 as the probability of most messages, b/c we'd be multiplying all other probabilities by 0 every time we encountered an unknown term. 

This would cause catastrophic results for our classifier b/c many/even all messages would be incorrectly assigned a 0 probability of being either spam or ham.

Researchers have come up w/ many clever ways of trying to get around this problem, such as drawing a random probability from some distribution or using NLP techniques to estimate “spamminess” of a term given its context.

For our purposes, use a very simple rule: *assign a very small probability to terms not in the training set* = a common way of dealing w/ missing terms in simple text classifiers + for our purposes serves just fine. In this exercise, by default set this probability to 0.0001%, sufficiently small for this data set. 

Finally, b/c we're assuming all emails are equally likely to be ham or spam, we set our default prior belief an email of each to 50%. In order to return to this problem later, however, we construct `classify.email` function such that the prior can be varied.

Be wary of always using 0.0001% for terms not in a training set. We are using it in this example, but in others it may be too large or too small, in which case the system you build will not work at all!
```{r}
classify_email <- function(path, training_df, prior=0.5, c=1e-6) {
 msg <- get_msg(path)
 msg_tdm <- get_tdm(msg)
 msg_freq <- rowSums(as_matrix(msg_tdm))
 # Find intersections of words
 msg_match <- intersect(names(msg_freq), training_df$term)
 if(length(msg_match) < 1) {
 return(prior*c^(length(msg_freq)))
 }
 else {
 match_probs <- training_df$occurrence[match(msg_match, training_df$term)]
 return(prior * prod(match_probs) * c^(length(msg_freq)-length(msg_match)))
 }
}
```
You will notice that the first three steps of the classify.email function proceed just as
our training phase did. We must extract the message text w/ get.msg, turn it into a
TDM w/ get.tdm, + finally calculate the frequency of terms w/ rowSums. Next, we
need to find how the terms in the email message intersect w/ the terms in our training
data, as depicted in Figure 3-3. To do so, we use the intersect command, passing the
terms found in the email message + those in the training data. What will be returned
are those terms in the gray shaded area of Figure 3-3.
The final step of the classification is to determine whether any of the words in the email
message are present in the training set, + if so, we use them to calculate the probability
that this message is of the class in question.
Assume for now that we are attempting to determine if this email message is spam.
msg.match will contain all of the terms from the email message in our spam training
data, spam.df. If that intersection is empty, then the length of msg.match will be less than
zero, + we can update our prior only by multiplying it w/ the product of the number
of terms in the email w/ our tiny probability value: c. The result will be a tiny probability
of assigning the spam label to the email.
Conversely, if this intersection is not empty, we need to find those terms from the email
in our training data + look up their occurrence probabilities. We use the match function
to do the lookup, which will return the term’s element position in the term column
of our training data. We use these element positions to return the corresponding probabilities
from the occurrence column, + return those values to match.probs. We then
calculate the product of these values + combine it w/ our prior belief about the
email being spam w/ the term probabilities + the probabilities of any missing terms.
The result is our Bayesian estimate for the probability that a message is spam given the
matching terms in our training data.
As an initial test, we will use our training data from the spam + easy ham messages
to classify hard ham emails. We know that all of these emails are ham, so ideally our
classifier will assign a higher probability of being ham to all of these messages. We also
know, however, that hard ham messages are “hard” to classify because they contain
Writing Our First Bayesian Spam Classifier | 87
www.it-ebooks.info
terms that are also associated w/ spam. Now let’s see how our simple
classifier does!
```{r}
hardham.docs <- dir(hardham_path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
hardham.spamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham_path, p, sep=""),
 training.df=spam.df))
hardham.hamtest <- sapply(hardham.docs,
 function(p) classify.email(paste(hardham_path, p, sep=""),
 training.df=ham_df))
hardham.res <- ifelse(hardham.spamtest > hardham.hamtest, TRUE, FALSE)
summary(hardham.res)
```
Just as before, we need to get all of the file paths in order, + then we can test the
classifier for all hard ham messages by wrapping both a spam test + ham test in
sapply calls. The vectors hardham.spamtest + hardham.hamtest contain the
conditional probability calculations for each hard ham email of being either spam or
ham given the appropriate training data. We then use the ifelse command to compare
the probabilities in each vector. If the value in hardham.spamtest is greater than that in
hardham.hamtest, then the classifier has classified the message as spam; otherwise, it is
ham. Finally, we use the summary command to inspect the results, listed in Table 3-2.
Table 3-2. Testing our classifier against “hard ham”
Email type Number classified as ham Number classified as spam
Hard ham 184 65
Congratulations! You’ve written your first classifier, + it did fairly well at identifying
hard ham as nonspam. In this case, we have approximately a 26% false-positive rate.
That is, about one-quarter of the hard ham emails are incorrectly identified as spam.
You may think this is poor performance, + in production we would not want to offer
an email platform w/ these results, but considering how simple our classifier is, it is
doing quite well. Of course, a better test is to see how the classifier performs against
not only hard ham, but also easy ham + spam.
