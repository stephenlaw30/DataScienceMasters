---
title: "Model Comparison"
author: "Steve Newns"
date: "December 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
```

# SVMs: The Support Vector Machine

The **support vector machine (SVM)** allows you to use multiple different **kernels** to find *non-linear* decision boundaries to classify points from a data set. A non-linear decision boundary can’t be discovered using a simple classification algorithm like logistic regression 
```{r}
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
```

We’ve correctly predicted the class of only 52% of the data. But we could do exactly this well by predicting every DP = Class 0:
```{r}
mean(with(df, 0 == Label))
```

In short, the logistic regression model (+ the linear decision boundary it finds) is completely useless. It makes the same predictions as a model w/ no info beyond the fact that Class 0 occurs more often than Class 1 (**the baseline model** = should be used as a prediction in the absence of all other info). SVM provides a trivial way to outperform logistic regression. Before

`e1071` package provides `svm()` that is just as easy to use as `glm()`
```{r}
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
```

We’ve clearly done better than logistic regression just by switching to SVMs. Plot its predictions vs. those from a logistic regression:
```{r}
df <- cbind(df, data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression
                       SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM 
head(df)
```

```{r}
# create new df where cols melt into single col's values
predictions <- melt(df, id.vars = c('X', 'Y'))
head(predictions)
```
```{r}
ggplot(predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```
Here we’ve added the logistic regression + SVM predictions to the raw data set + plotted the ground truth labels along w/ the logit and SVM predictions in a faceted plot. It's obvious that the logistic regression is useless b/c it puts the decision boundary *outside* of the data set while the ground truth data in the top row clearly has a band of entries for Class 1 in the center. The SVM is able to capture this band structure, even though it makes odd predictions near the furthest boundaries of the data set.

The SVM uses the **kernel trick**, which using a mathematical transformation (depending only on a simple computation involving “kernels”) moves the original data set into a new mathematical space in which the decision boundary is easy to describe. Describing the mathematics of the kernel trick is nontrivial, but it’s easy to gain some intuition by testing out different kernels w/ the `kernel` parameter to 1 of 4 values: `linear`, `polynomial`, `radial`, `sigmoid`. To get a feel for how these kernels work, try using all of them to generate predictions + plot them:
```{r}
# reset df
df <- df[, c('X', 'Y', 'Label')]

linear_svm <- svm(Label ~ X + Y, df, kernel = 'linear')
with(df, mean(Label == ifelse(predict(linear_svm) > 0, 1, 0)))

polynomial_svm <- svm(Label ~ X + Y, df, kernel = 'polynomial')
with(df, mean(Label == ifelse(predict(polynomial_svm) > 0, 1, 0)))

radial_svm <- svm(Label ~ X + Y, df, kernel = 'radial')
with(df, mean(Label == ifelse(predict(radial_svm) > 0, 1, 0)))

sigmoid_svm <- svm(Label ~ X + Y, df, kernel = 'sigmoid')
with(df, mean(Label == ifelse(predict(sigmoid_svm) > 0, 1, 0)))

df <- cbind(df,
            data.frame(LinearSVM = ifelse(predict(linear_svm) > 0, 1, 0),
                       PolynomialSVM = ifelse(predict(polynomial_svm) > 0, 1, 0),
                       RadialSVM = ifelse(predict(radial_svm) > 0, 1, 0),
                       SigmoidSVM = ifelse(predict(sigmoid_svm) > 0, 1, 0)))
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

The linear + polynomial kernels look more or less like logistic regression. In contrast, the radial kernel gives us a decision boundary somewhat like the ground truth boundary, + the sigmoid kernel gives us a very complex + strange decision boundary. SVM could make much better predictions than it seems to do out of the box via a set of hyperparameters that are not set to useful values by default. Getting the best predictions from the model requires tuning these hyperparameters.

The first hyperparameter = the `degree` of the polynomial used by the polynomial kernel.
```{r}
polynomial.degree3.svm.fit <- svm(Label ~ X + Y,
                                  data = df,
                                  kernel = 'polynomial',
                                  degree = 3)
with(df, mean(Label != ifelse(predict(polynomial.degree3.svm.fit) > 0, 1, 0)))
#[1] 0.5156
polynomial.degree5.svm.fit <- svm(Label ~ X + Y,
                                  data = df,
                                  kernel = 'polynomial',
278
|
Chapter 12:
Model Comparison
                                  degree = 5)
with(df, mean(Label != ifelse(predict(polynomial.degree5.svm.fit) > 0, 1, 0)))
#[1] 0.5156
polynomial.degree10.svm.fit <- svm(Label ~ X + Y,
                                   data = df,
                                   kernel = 'polynomial',
                                   degree = 10)
with(df, mean(Label != ifelse(predict(polynomial.degree10.svm.fit) > 0, 1, 0)))
#[1] 0.4388
polynomial.degree12.svm.fit <- svm(Label ~ X + Y,
                                   data = df,
                                   kernel = 'polynomial',
                                   degree = 12)
with(df, mean(Label != ifelse(predict(polynomial.degree12.svm.fit) > 0, 1, 0)))
#[1] 0.4464
Here we can see that setting 
degree
 to 3 or 5 doesn’t have any effect on the quality of
the model’s predictions. (It’s worth noting that the default value of 
degree
 is 3.) But
setting 
degree
 to 10 or 12 does have an effect. To see what’s happening, let’s plot out
the decision boundaries again:
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
            data.frame(Degree3SVM = ifelse(predict(polynomial.degree3.svm.fit) > 0,
                                           1,
                                           0),
                       Degree5SVM = ifelse(predict(polynomial.degree5.svm.fit) > 0,
                                           1,
                                           0),
                       Degree10SVM = ifelse(predict(polynomial.degree10.svm.fit) > 0,
                                            1,
                                            0),
                       Degree12SVM = ifelse(predict(polynomial.degree12.svm.fit) > 0,
                                            1,
                                            0)))
predictions <- melt(df, id.vars = c('X', 'Y'))
ggplot(predictions, aes(x = X, y = Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
SVMs: The Support Vect