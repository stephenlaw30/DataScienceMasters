---
title: "Model Comparison"
author: "Steve Newns"
date: "December 9, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
library(e1071)
library(reshape)
```

# SVMs: The Support Vector Machine

The **support vector machine (SVM)** allows you to use multiple different **kernels** to find *non-linear* decision boundaries to classify points from a data set. A non-linear decision boundary can’t be discovered using a simple classification algorithm like logistic regression 
```{r}
df <- read.csv('./data/df.csv')
logistic_model <- glm(Label ~ X + Y, family = binomial(link = 'logit'), df)
logistic_predictions <- ifelse(predict(logistic_model) > 0, 1, 0)
# get accuracy compared to real data labels
mean(with(df, logistic_predictions == Label))
```

We’ve correctly predicted the class of only 52% of the data. But we could do exactly this well by predicting every DP = Class 0:
```{r}
mean(with(df, 0 == Label))
```

In short, the logistic regression model (+ the linear decision boundary it finds) is completely useless. It makes the same predictions as a model w/ no info beyond the fact that Class 0 occurs more often than Class 1 (**the baseline model** = should be used as a prediction in the absence of all other info). SVM provides a trivial way to outperform logistic regression. Before

`e1071` package provides `svm()` which is just as easy to use as `glm()`
```{r}
library('e1071')
svm_model <- svm(Label ~ X + Y, df)
svm_predictions <- ifelse(predict(svm_model) > 0, 1, 0)
mean(with(df, svm_predictions == Label))
```

We’ve clearly done better than logistic regression just by switching to SVMs. Plot its predictions vs. those from a logistic regression:
```{r}
df <- cbind(df, 
            data.frame(Logit = ifelse(predict(logistic_model) > 0, 1, 0), # logistic regression prediction
                       SVM = ifelse(predict(svm_model) > 0, 1, 0))) # SVM prediction
head(df)
```

```{r}
# create new df where cols melt into single col's values
predictions <- melt(df, id.vars = c('X', 'Y'))
head(predictions)
```
```{r}
ggplot(predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

Here we’ve added the logistic regression + SVM predictions to the raw data set + plotted the ground truth labels along w/ the logit + SVM predictions in a faceted plot. It's obvious that the logistic regression is useless b/c it puts the decision boundary *outside* of the data set while the ground truth data in the top row clearly has a band of entries for Class 1 in the center. The SVM is able to capture this band structure, even though it makes odd predictions near the furthest boundaries of the data set.

The SVM uses the **kernel trick**, which, using a mathematical transformation (depending only on a simple computation involving “kernels”), moves the original data set into a new mathematical space in which the decision boundary is easy to describe. Describing the mathematics of the kernel trick is nontrivial, but it’s easy to gain some intuition by testing out different kernels w/ the `kernel` parameter w/ 1 of 4 values: `linear`, `polynomial`, `radial`, `sigmoid`. To get a feel for how these kernels work, try using all of them to generate predictions + plot them:
```{r}
# reset df
df <- df[, c('X', 'Y', 'Label')]

linear_svm <- svm(Label ~ X + Y, df, kernel = 'linear')
with(df, mean(Label == ifelse(predict(linear_svm) > 0, 1, 0)))

polynomial_svm <- svm(Label ~ X + Y, df, kernel = 'polynomial')
with(df, mean(Label == ifelse(predict(polynomial_svm) > 0, 1, 0)))

radial_svm <- svm(Label ~ X + Y, df, kernel = 'radial')
with(df, mean(Label == ifelse(predict(radial_svm) > 0, 1, 0)))

sigmoid_svm <- svm(Label ~ X + Y, df, kernel = 'sigmoid')
with(df, mean(Label == ifelse(predict(sigmoid_svm) > 0, 1, 0)))

df <- cbind(df, data.frame(LinearSVM = ifelse(predict(linear_svm) > 0, 1, 0),
                       PolynomialSVM = ifelse(predict(polynomial_svm) > 0, 1, 0),
                       RadialSVM = ifelse(predict(radial_svm) > 0, 1, 0),
                       SigmoidSVM = ifelse(predict(sigmoid_svm) > 0, 1, 0)))

# melt each SVM col into single col
predictions <- melt(df, id.vars = c('X', 'Y'))

ggplot(predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

The linear + polynomial kernels look more or less like logistic regression. In contrast, the radial kernel gives us a decision boundary somewhat like the ground truth boundary, + the sigmoid kernel gives us a very complex + strange decision boundary. SVM could make much better predictions than it seems to do out-of-box via a set of hyperparameters that are not set to useful values by default. Getting the best predictions from the model requires tuning these hyperparameters.

The first hyperparameter = the `degree` of the polynomial used by the polynomial kernel.
```{r}
svm_model_polynomialD3 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 3)
paste("3rd degree polynomial accuracy:",
      with(df, mean(Label != ifelse(predict(svm_model_polynomialD3) > 0, 1, 0))))

svm_model_polynomialD5 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 5)
paste("5th degree polynomial accuracy:",
      with(df, mean(Label != ifelse(predict(svm_model_polynomialD5) > 0, 1, 0))))

svm_model_polynomialD10 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 10)
paste("10th degree polynomial accuracy:",
      with(df, mean(Label != ifelse(predict(svm_model_polynomialD10) > 0, 1, 0))))

svm_model_polynomialD12 <- svm(Label ~ X + Y, df, kernel = 'polynomial', degree = 12)
paste("12th degree polynomial accuracy:",
      with(df, mean(Label != ifelse(predict(svm_model_polynomialD12) > 0, 1, 0))))
```

Setting `degree` to 3 or 5 doesn’t have any effect on the quality of the model’s predictions. (default value = 3). But setting `degree` to 10 or 12 does have an effect. To see what’s happening, plot out the decision boundaries again:
```{r}
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
            data.frame(Degree3SVM = ifelse(predict(svm_model_polynomialD3) > 0, 1, 0),
                       Degree5SVM = ifelse(predict(svm_model_polynomialD5) > 0, 1, 0),
                       Degree10SVM = ifelse(predict(svm_model_polynomialD10) > 0, 1, 0),
                       Degree12SVM = ifelse(predict(svm_model_polynomialD12) > 0, 1, 0)))

# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))

ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

It’s clear using a larger degree improves the quality of the predictions, though it does so in a hackish way that’s not really mimicking the structure of the data. And, the model-fitting step becomes slower as the degree increases. Finally, the same overfitting problems we saw in polynomial regression will creep up. For that reason, always use cross-validation to experiment w/ setting the `degree` hyperparameter in applications that use SVMs w/ polynomial kernels. Although there’s no doubt SVMs w/ polynomial kernels are a valuable tool to have in your toolkit, they can’t be guaranteed to work well w/out some effort + thinking.

After playing w/ `degree` for the polynomial kernel, let’s try out the `cost` hyperparameter, which is used with all of the possible SVM kernels.
```{r}
svm_model_radial_cost1 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 1)
paste("Radial Cost 1 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_radial_cost1) > 0, 1, 0))))

svm_model_radial_cost2 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 2)
paste("Radial Cost 2 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_radial_cost2) > 0, 1, 0))))

svm_model_radial_cost3 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 3)
paste("Radial Cost 3 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_radial_cost3) > 0, 1, 0))))

svm_model_radial_cost4 <- svm(Label ~ X + Y, df, kernel = 'radial', cost = 4)
paste("Radial Cost 4 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_radial_cost4) > 0, 1, 0))))
```

Increasing `cost` makes the model progressively fit more + more poorly b/c `cost` = a **regularization hyperparameter** like `lambda`, +  increasing them will always make model fit the training data less tightly. Of course, this increase in regularization can improve model performance on test data, so you should always see what value of `cost` most improves your test performance using cross-validation.
To get some insight into what’s happening in terms of the fitted model, look at the predictions graphically
```{r}
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
            data.frame(Cost1SVM = ifelse(predict(svm_model_radial_cost1) > 0, 1, 0),
                       Cost2SVM = ifelse(predict(svm_model_radial_cost2) > 0, 1, 0),
                       Cost3SVM = ifelse(predict(svm_model_radial_cost3) > 0, 1, 0),
                       Cost4SVM = ifelse(predict(svm_model_radial_cost4) > 0, 1, 0)))

# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))

ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

Changes induced by the `cost` parameter are quite subtle, but can be seen on the periphery of the data. As `cost` goes up, the boundaries created by the radial kernel become more + more linear.

TO conclude our SVM hyperparameter experiments = play around w/ the `gamma` hyperparameter. For testing purposes, we’ll observe its effects on a sigmoid kernel by testing out 4 different gamma values:
```{r}
svm_model_sigmoid_gamma1 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 1)
paste("sigmoid gamma 1 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma1) > 0, 1, 0))))

svm_model_sigmoid_gamma2 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 2)
paste("sigmoid gamma 2 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma2) > 0, 1, 0))))

svm_model_sigmoid_gamma3 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 3)
paste("sigmoid gamma 3 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma3) > 0, 1, 0))))

svm_model_sigmoid_gamma4 <- svm(Label ~ X + Y, df, kernel = 'sigmoid', gamma = 4)
paste("sigmoid gamma 4 accuracy:",
      with(df, mean(Label == ifelse(predict(svm_model_sigmoid_gamma4) > 0, 1, 0))))
```

Every time we increase `gamma`, the model does a little better. To get a sense for the source of that improvement, turn to graphical diagnostics of the predictions:
```{r}
df <- df[, c('X', 'Y', 'Label')]
df <- cbind(df,
            data.frame(gamma1SVM = ifelse(predict(svm_model_sigmoid_gamma1) > 0, 1, 0),
                       gamma2SVM = ifelse(predict(svm_model_sigmoid_gamma2) > 0, 1, 0),
                       gamma3SVM = ifelse(predict(svm_model_sigmoid_gamma3) > 0, 1, 0),
                       gamma4SVM = ifelse(predict(svm_model_sigmoid_gamma4) > 0, 1, 0)))

# melt down cols for each SVM degress into single col
svm_predictions <- melt(df, id.vars = c("X","Y"))

ggplot(svm_predictions, aes(X, Y, color = factor(value))) +
  geom_point() +
  facet_grid(variable ~ .)
```

The rather complicated decision boundary chosen by the sigmoid kernel warps around as we change the value of `gamma`. To really get a better intuition for what’s happening, experiment w/ many more values of `gamma`

This ends the intro to the SVM, a valuable algorithm to have in a toolkit, but it’s time to stop building up your toolkit + instead start focusing on thinking critically about which tool is best for any given job. To do that, we’ll explore multiple models simultaneously on a single data set.

# Comparing Algorithms

Since we know how to use to SVMs, logistic regression, and kNN, let’s compare their performance on the `SpamAssassin` data set. Experimenting w/ multiple algorithms is a good habit to develop when working w/ real-world data b/c often you won’t be able to know in advance which algorithm will work best with your data set. Also, 1 of the major skills that separates the most experienced people in ML from those just starting is the ability to know from the structure of a problem when a certain algorithm won’t work well. The best way to build up this intuition is to apply all of the standard algorithms to every problem you come across until you have a sense of when they’ll fail.
```{r}
load('data/dtm.RData')
set.seed(1)

train_indices <- sort(sample(1:nrow(dtm), round(.5*nrow(dtm))))
test_indices <- which(! 1:nrow(dtm) %in% train_indices)

train_x <- dtm[train_indices, 3:ncol(dtm)]
train_y <- dtm[train_indices, 1]
test_x <- dtm[test_indices, 3:ncol(dtm)]
test_y <- dtm[test_indices, 1]

rm(dtm)
```

Now, w/ data in memory, we can immediately move forward to regularized logistic regression
```{r}
library('glmnet')
regularized_logistic_regression <- glmnet(train_x, train_y, family = "binomial")
```

Of course, this leaves us w/ a great deal of flexibility, so we’d like to compare various settings for `lambda` to see which gives best performance. 

To push through this example quickly, we’ll cheat a bit and test the hyperparameter settings on the test set rather than do repeated splits of the training data. If rigorously testing models, don't make this sort of simplification. For now, just try all values `glmnet` proposed + see which performs best on the test set:
```{r}
(lambdas <- regularized_logistic_regression$lambda)
```

So, these are the values of lambda we'll be using
```{r}
performance <- data.frame()
for (lambda in  lambdas) {
  # test model on new test data with each lambda
  temp_predictions <- predict(regularized_logistic_regression, test_x, s = lambda)
  predictions <- as.numeric(temp_predictions > 0)
  # calculate MSE
  mse <- mean(predictions != test_y)
  # add row back to DF
  performance <- rbind(performance, data.frame(Lambda = lambda, MSE = mse))
}

ggplot(performance, aes(Lambda, MSE)) + 
  geom_point() + 
  scale_x_log10()
```

We see a pretty clear region of values for `lambda` that gives the lowest possible error rate. To find the *best* value for our final analysis, use simple indexing + `min`
```{r}
final_lambda <- with(performance, max(Lambda[which(MSE == min(MSE))]))
```

In this case, there are actually 2 different values of `lambda` that give identical performance, so we extract the larger of the 2 b/c it's the one w/ greater regularization applied. We can then calculate MSE for our logistic model using this best value for lambda
```{r}
(mse <- with(subset(performance, Lambda == final_lambda), MSE))
```

Using regularized logistic regression, which requires only a trivial amount of hyperparameter tuning to implement, misclassifies only 6% of all emails in our test set. But we’d like to see how other methods do w/ similar amounts of work so we can decide whether we should be using logistic regression, SVMs, or kNN. Start by fitting a linear kernel SVM to see how it compares w/ logistic regression: