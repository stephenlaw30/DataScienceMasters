---
title: "Ch5_RegressionPredictingPageViews"
author: "Steve Newns"
date: "November 23, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(tm)
library(magrittr)
library(plyr) # ddply
```

# Introducing Regression

**Regression** = predict one set of #'s given another set of #'s
In classification problems, might use #'s as a dummy code for a categorical distinction so 0 = ham and 1 = spam. But these #'s are just symbols; we’re not exploiting the “numberness” of 0 or 1 using dummy variables. 

In regression, b/c predicting numbers, you want to be able to make strong statements about the relationship between the inputs+ the output (when # of packs of cigs a person smokes per day doubles, predicted life span gets cut in half).

Problem = wanting to make precise numerical predictions isn’t the same thing as actually being able to make predictions. To make quantitative predictions, we need to come up w/ some rule that can leverage the info we have access to. The various regression algorithms that statisticians have developed over thelast 200 years all provide different ways to make predictions by turning inputs into outputs. 

#The Baseline Model

Simplest possible way to use the info we have as inputs = ignore inputs entirely + predict future outcomes based only on mean value of the output seen in the past (completely ignore a person’s health records + simply guess that they’ll live as long as the average person lives).

Guessing the mean outcome for every case isn’t as naive as it might seem: if interested in making predictions that are as close to the truth as possible without using any other info, guessing the mean output turns out to be the best guess we can possibly make.

A little bit of work has to go into defining “best” to give this claim a definite meaning. 
```{r}
ages <- read.csv('data/longevity.csv')
ggplot(ages, aes(AgeAtDeath, fill = factor(Smokes))) +
  geom_density() +
  facet_grid(Smokes ~ .) + 
  ggtitle("Density plot of 1,000 people’s life spans, facetted by smokers")
```
It seem reasonable to believe smoking matters for longevity b/c the center of the nonsmokers’ life span distribution is shifted to the right of the center of the smokers’ life spans. In other words, the average life span of a non-smoker is longer than the average life span of a smoker. 

But before we describe how you can use info we have about a person’s smoking habits to make predictions about longevity, pretend we didn’t have any of this info --> need to pick a single # as the prediction for every new person, regardless of smoking habits. 

The question of # to pick isn’t a trivial one, b/c it should depend on what you think it means to make good predictions. There are a lot of reasonable ways to define accuracy of predictions, but there is a single measure of quality that’s been dominant for virtually the entire history of statistics = **squared error**. If you’re trying to predict a value y + guess h, squared error of your guess = (y - h)^2

If using squared error to measure quality of predictions, the best guess we can possibly make about a person’s life span (without any additional info about a person’s habits) = the average person’s longevity.

W/ `longevity` data set, the mean `AgeAtDeath` is:
```{r}
mean(ages$AgeAtDeath)
```

Then ask: “How badly would we have predicted the ages of the people in our data set if we’d guessed they all lived to be 73?”

```{r}
# combine all of squared errors for each person in data set by computing mean of the squared errors (MSE) 
guess <- 73
with(ages, mean((AgeAtDeath - guess) ^ 2))
```
MSE made by guessing 73 for every person in our data set is 32.991. That, by itself, shouldn’t be enough to convince you that we’d do worse with a guess that’s not 73. Consider how we’d perform if we made some of the other possible guesses

```{r}
# loop over a sequence of possible guesses ranging from 63 to 83:
guess.accuracy <- data.frame()
for (guess in seq(63, 83, by = 1)) {
  # for each guess in sequence, get MSE
  prediction.error <- with(ages, mean((AgeAtDeath - guess)**2))
  
  # add MSE to data frame of MSE's
  guess.accuracy <- rbind(guess.accuracy, data.frame(Guess = guess, Error = prediction.error))
}

ggplot(guess.accuracy, aes(Guess, Error)) +
  geom_point() +
  geom_line()
```
Using any guess other than 73 gives us worse predictions for our data set. This is actually a general theoretical result that can be proven mathematically: **to minimize squared error, predict the mean value in a data set**. 

1 important implication of this = predictive value of having info about smoking should be measured in terms of the amount of *improvement you get from using this info over just using the mean value as your guess for every person you see*.

# Regression Using Dummy Variables

How can we use info about whether or not people smoke to make better guesses about longevity? 1 simple idea = estimate mean age at death for smokers and nonsmokers separately +use these 2 separate values as guesses for future cases, depending on whether or not a new person smokes. 

This time, instead of using MSE, use **root mean squared error (RMSE)** = more popular in ML literature.
```{r}
# get mean Age at death over all observations
constant.guess <- with(ages, mean(AgeAtDeath))
# get RMSE for each age in observation
paste("RMSE without smoking information:",with(ages, sqrt(mean((AgeAtDeath - constant.guess) ^ 2))))

# do same as above but for split data sets on binary  smoking variable
smokers.guess <- with(subset(ages, Smokes == 1), mean(AgeAtDeath))
non.smokers.guess <- with(subset(ages, Smokes == 0), mean(AgeAtDeath))

ages <- transform(ages, NewPrediction = ifelse(Smokes == 0, non.smokers.guess, smokers.guess))
# get new RMSE
paste("RMSE with smoking information:",with(ages, sqrt(mean((AgeAtDeath - NewPrediction) ^ 2))))
```

Predictions really do get better after include more info about the people we’re studying = prediction error when estimating people’s life spans becomes 10% smaller when we include info about people’s smoking habits. 

In general, we can do better than using just the mean value whenever we have binary distinctions that separate 2 types of DP's, *assuming those binary distinctions are related to the output we’re trying to predict.* Some simple examples where binary distinctions might help = contrasting men w/ women or Democrats w/ Republicans in American political discourse.

So now we have a mechanism for incorporating dummy variables into predictions. But how can we use "richer" info about the objects in our data? 
<ul>
<li> Want to know how we can use inputs that *aren’t* binary distinctions, but instead continuous values such as heights or weights </li>
<li> Want to know how we can use multiple sources of info all at once to improve estimates. </li>
</ul>

Intuition = having separate sources of info should tell us more than either variable in isolation. But making use of all of the info we have isn’t an easy task. In practice, need to make some simplifying assumptions to get things to work, assumptions that underlie linear regression.

Using *only* linear regression is less of a restriction than it might seem, as linear regression is used in at least 90% of practical regression applications + can be hacked to produce more sophisticated forms of regression with only a little bit of work.

#Linear Regression in a Nutshell

2 biggest assumptions we make when using linear regression to predict outputs are the following:
<ul>
<li> **Separability/additivity** = If there are multiple pieces of info that would affect our guesses, we produce our guess by adding up the effects of each piece of info as if each piece were being used in isolation (ex: alcoholics live 1 year less than nonalcoholics + smokers live 5 years less than nonsmokers = alcoholic who’s also a smoker should live 1 + 5 = 6 years less than a nonalcoholic nonsmoker. **Interactions** = a technique for working around the separability assumption in linear regression (ex: effect of excessive drinking is much worse if you also smoke) </li>
<il> **Monotonicity/linearity** = monotomic model = changing 1 input always makes predicted output go up/down (ex: assuming every time somebody’s height increases, prediction of their weight will go up). Monotonicity is a strong assumption b/c output can go up for a bit + then start to go down, but monotonicity assumption is much less strong than the full assumption of the linear regression algorithm, **linearity** =  changing the input by 1 unit always adds/subtracts N units. Every linear model is monotonic, but curves can be monotonic without being linear. For that reason, linearity is more restrictive than monotonicity </li>
</ul>

```{r}
# linear example
heights.weights <- read.csv('data/01_heights_weights_genders.csv', header = TRUE, sep = ',')
ggplot(heights.weights, aes(Height, Weight)) + 
  geom_point() +
  geom_smooth(method = 'lm') # plot linear fit to DPs

regression.fit <- lm(Weight ~ Height, heights.weights)
coef(regression.fit)
slope <- coef(regression.fit)[2]
intercept <- coef(regression.fit)[1]
```

Every increase of 1 inch in someone’s height leads to an increase of 7.7 lbs in weight. That strikes us as pretty reasonable. Intercept tells you how much a person who is 0 inches tall would weigh, –350 lbs. This regression model isn’t so great for children or extremely short adults.

Systematic problem for linear regression in general = predictive models usually are not very good at predicting outputs for inputs that are very far removed from all inputs seen in the past (regressions are good at interpolation but not very good at extrapolation). Often you can do some work to improve quality of guesses outside of the range of data used to train a model, but in this case there’s probably no need b/c you’re usually only going to make predictions for people who are over 4’ tall and under 8’ tall.

When making predictions in a practical context, coefficients are all that you need to know. To get a sense of where a model is wrong, compute model’s predictions + compare against inputs. 
```{r}
y_hat <- predict(regression.fit)

#calculate differences between predictions + truth
y <- with(heights.weights,Weight)
residuals <- y - y_hat
```

**residuals** = the part of data left over after accounting for the part that a line can explain. A common way to diagnose any obvious mistakes when using linear regression is to plot the residuals against the truth

```{r}
# plot only 1st regression diagnostic plot
plot(regression.fit,which = 1)
```

Our linear model works well because there’s no systematic structure/pattern in the residuals, unlike below

```{r}
x <- 1:10
y2 <- x ^ 2
fitted.regression <- lm(y2 ~ x)
plot(fitted.regression, which = 1)
```

A model should divide the world into **signal** (`predict()` result) + **noise** (residuals). If you can see signal in the residuals using naked eye, the model isn’t powerful enough to extract all the signal + leave behind only real noise as residuals. 

To solve this problem, use more powerful models for regression than the simple linear regression model, but these models can be so powerful that they’re actually too powerful to be used w/out caution.

Simplest measurement of error is = **sum of squared errors**
```{r}
squared.errors <- residuals ^ 2
sum(squared.errors)
```

Simple sum of squared errors quantity = useful for comparing different models, but has some quirks most people end up disliking
<ul>
<li> larger for big data sets than for small data sets = solve w/ **mean squared error (MSE)** rather than sum. MSE won’t grow consistently as we get more data the way raw sum of squared errors wil </li>
<li> MSE still has a problem: if average prediction is only off by 5, MSE will be 25 b/c we’re squaring errors before taking their mean = take square root of the MSE to get **root mean squared error (RMSE)** = a very popular measure of performance for assessing ML algorithms, including algorithms that are far more sophisticated than linear regression (like the Netflix Prize) </li>
<li> 1 complaint for RMSE = not immediately clear what mediocre performance is. Perfect performance clearly gives RMSE of 0, but pursuit of perfection is not a realistic goal in these tasks. </li>
<li> Likewise, it isn’t easy to recognize when a model is performing very poorly. For example, if everyone’s heights are 5' + you predict 5,000', you’ll get a huge RMS + you can do worse by predicting 50,000' + still worse by predicting 5,000,000'. The *unbounded values* RMSE can take on make it difficult to know whether your model’s performance is reasonable. </li>
<li> Solve w/ **R2** = idea is to see how much better a model does than if just using the mean, as R2 will always be between 0-1, where 0 = doing no better than the mean + 1 = predicting every DP perfectly </li>
<li> Multiply it by 100 = the % of the variance in the data you’ve explained w/ your model = a handy way to build up an intuition for how accurate a model is, even in a new domain where you don’t have any experience about the standard values for RMSE. </li>
</ul>

To calculate R2, compute RMSE for a model that uses only the mean output to make predictions for all example data, + then compute the RMSE for your model. After that, it's just a simple arithmetic operation to produce
R2
```{r}
mean.rmse <- 1.09209343
model.rmse <- 0.954544
(r2 <- 1 - (model.rmse / mean.rmse))
```


# Predicting Web Traffic
Case study = predict amount of page views for the top 1,000 websites on the Internet as of 2011
```{r}
websites <- read_tsv("./data/top_1000_sites.tsv")
head(websites)
```

We’ll focus on five columns `Rank`, `PageViews` (in the year, the outcome), `UniqueVisitors` (monthly), `HasAdvertising`, + `IsEnglish`. A good exercise to work on would be comparing `PageViews`w/ `UniqueVisitors` to find a way to tell which sorts of sites have lots of repeat visits vs. those w/ very few repeat visits.

You might think ads would be annoying + that people would, all else being equal, tend to avoid sites that have ads, which we can explicitly test for w/ regression. 1 of the great values of regression is it lets us try to answer questions in which we have to talk about “all else being equal”. “Try” b/c the quality of a regression is only as good as the inputs we have. If an important variable is missing from our inputs, the results of a regression can be very far from the truth + for that reason, always assume the results of a regression are tentative: “If the inputs we had were sufficient to answer this question, then the answer would be....”.

Looking through the list, it’s clear most top sites are primarily either English or Chinese-language sites. It’s interesting to ask whether being in English is a positive thing or not. It’s also an example in which the direction of causality isn’t at all clear from a regression; "does being written in English make a site more popular, or do more popular sites decide to convert to English b/c it’s the lingua franca of the Internet?"" A regression model can tell if 2 things are related, but can’t tell whether one causes the other thing.

Start to get a sense of how these things relate to one another w/ a scatterplot that relates `PageViews` w/ `UniqueVisitors`. Always draw scatterplots for numerical variables before trying to relate them by using regression b/c a scatterplot can make it clear when linearity assumption of regression isn’t satisfied.
```{r}
ggplot(websites, aes(PageViews,UniqueVisitors)) + geom_point()
```
This looks terrible b/c almost all values are bunched together on the x-axis + only a very small # jump out from the pack, a common problem when working w/ data that’s not normally distributed. Using a scale that’s large enough to show the full range of values tends to place the majority of DPs so close to each other that they can’t be separated visually. 

To confirm the shape of the data is the problem w/ this plot, look at the distribution of `PageViews` by itself
```{r}
gridExtra::grid.arrange(
ggplot(websites, aes(PageViews)) + geom_density(),
ggplot(websites, aes(UniqueVisitors)) + geom_density(),
nrow  = 2)
```
This density plot looks as completely impenetrable as the earlier scatterplot. When you see nonsensical density plots, it’s a good idea to try taking the **log of the values** you’re trying to analyze + make a **density plot** of those log-transformed values. 
```{r}
gridExtra::grid.arrange(
ggplot(websites, aes(log(PageViews))) + geom_density(),
ggplot(websites, aes(log(UniqueVisitors))) + geom_density(),
nrow  = 2)
```
This density plot looks much more reasonable -->  start using log-transformed `PageViews` + `UniqueVisitors` from now on.
```{r}
ggplot(websites, aes(log(PageViews), log(UniqueVisitors))) + geom_point()
```
`ggplot2` also contains a convenience function to change the scale of an axis to the log = `scale_x_log` or `scale_y_log`, + in some cases you will want to use `logp` to avoid errors related to taking the log of 0.

See what the regression line will look like
```{r}
ggplot(websites, aes(log(PageViews), log(UniqueVisitors))) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```
The resulting line looks promising, so let’s find the values that define its slope + intercept 
```{r}
model <- lm(log(PageViews) ~ log(UniqueVisitors), websites)
summary(model)
```

Columns for “Std. Error,” the “t-value,” + the p-value of each coefficient are used to assess uncertainty in the estimates we’ve computed (measurements of confidence that values computed from a specific data set are accurate descriptions of the real world that generated the data). 

`Std. Error` can be used to produce a 95% CI for the coefficient which indicates the bounds for which we can say, “95% of the time, the algorithm we use to construct intervals will include the true coefficient inside of the intervals it produces.” Thankfully, the sort of qualitative distinctions that require attention to standard errors usually aren’t necessary if just trying to hack together models to predict something.

The “t-value” + p-value are both measures of how confident we are the true coefficient *isn’t zero* (reject the null). This is used to say "we are confident there is a real relationship between the output + the input we’re currently examining". Here we can use `t value` to assess how sure we are `PageViews` really are related to `UniqueVisitors`. These 2 #'s can be useful if you understand how to work w/ them, but are sufficiently complicated that their usage has helped to encourage people to assume they’ll never fully understand statistics. 

If you care whether you’re confident 2 variables are related, check whether the estimate is at least 2 SE's away from 0. (ex: coefficient for `log(UniqueVisitors)` = 1.33628 + SE = 0.04568 ==> coefficient is 1.33628 / 0.04568 == 29.25306 SE's away from 0). If more than 3 SE's away from 0, you can feel reasonably confident your 2 variables are related.

t-values and p-values are useful for deciding whether a relationship between 2 columns in data is "real" or just an accident of chance. Deciding that a relationship exists is valuable, but understanding the relationship is another matter entirely. Regressions can’t help you do that. People try to force them to, but in the end, if you want to understand the reasons why 2 things are related, you need more info than a simple regression can ever provide. Traditional cutoff for being confident an input is related to output = find a coefficient that’s at least 2 SE's away from zero.

Final pieces of info = related to the predictive power of the linear model fit to our data = **Residual standard error** = **RMSE** = `sqrt(mean(residuals(lm.fit)^2))`. The **degrees of freedom** refers to the notion that we’ve effectively used up 2 DPs's in our analysis by fitting 2 coefficients (intercept + coefficient for `log(UniqueVisitors)`. This number, 998, is relevant b/c it’s not very impressive to have a low RMSE if you’ve used 500 coefficients in your model to fit 1,000 DP's. Using lots of coefficients w/ very little data = a form of **overfitting** t

**Multiple R2** = standard “R2” = % of variance in data explained by model. Here we’re explaining 46% of the variance using our model, which is pretty good. **Adjusted R2** = a 2nd measure that penalizes Multiple R-squared for the # of coefficients used. 

**F-statistic** = a measure of the improvement of your model over using just the mean to make predictions (over baseline), an alternative to R2 that allows one to calculate a “p-value.” B/c a “p-value” is usually deceptive, do not put *too* much faith in the F-statistic. “p-values” have their uses if you completely understand the mechanism used to calculate them, but otherwise they can provide a false sense of security that will make you forget that the gold standard of model performance = predictive power on new data NOT used to fit the model, rather than the performance of your model on the data it was fit to.

Now include `HasAdvertising` + `IsEnglish` to see what happens when we give our model more inputs to work w/:
```{r}
model2 <- lm(log(PageViews) ~ HasAdvertising + log(UniqueVisitors) + InEnglish, websites)
summary(model2)
```
Now the model includes a **factor** = `HasAdvertising` was modeled so that sites for which `HasAdvertising == 'Yes'` are separated from the intercept, whereas sites for which `HasAdvertising == 'No'` are folded into the intercept (**baseline dummy variable**). Another way to describe this is to say the intercept = the prediction for a website w/ no advertising + has 0 `log(UniqueVisitors)`, which occurs when you have 1 `UniqueVisitor`. We can see the same logic play out for `InEnglish`, except this factor has many `NA` values, so there are really 3 levels of this dummy variable = `NA`, 'No', 'Yes'. In this case, R treats the `NA` value as the default to fold into the regression intercept + fits separate coefficients for 'No' + 'Yes' levels.

Compare each of the 3 inputs we’ve used in isolation to see which has the most predictive power when used *on its own* by extracting R2 for each summary in isolation:
```{r}
ad.fit <- lm(log(PageViews) ~ HasAdvertising, websites)
uniquevisitors.fit <- lm(log(PageViews) ~ log(UniqueVisitors), websites)
english.fit <- lm(log(PageViews) ~ InEnglish, websites)
summary(ad.fit)$r.squared
summary(uniquevisitors.fit)$r.squared
summary(english.fit)$r.squared
```

`HasAdvertising` explains only 1% of the variance, `UniqueVisitors` explains 46%, + `InEnglish` explains 3%. In practice, it’s worth including all inputs in a predictive model when they’re cheap to acquire, but if `HasAdvertising` were difficult to acquire programmatically, we’d advocate dropping it in a model w/ other inputs that have so much more predictive power.

# Defining Correlation

In the strictest sense, 2 variables are correlated if relationship between them can be described w/ a straight line = measure of how well linear regression could be used to model the relationship between 2 variables. 0 correlation = no interesting line that relates the 2 variables. A correlation of 1 = perfectly positive straight line (going up) that relates the two variables. Correlation of –1 = perfectly negative straight line (going down) that relates the 2 variables.

Generate some data that isn’t strictly linear and then plot it:
```{r}
x <- 1:10
y <- x ^ 2
ggplot(data.frame(X = x, Y = y), aes(X, Y)) +
    geom_point() +
    geom_smooth(method = 'lm', se = FALSE)
```

Line doesn’t pass through all points, so the relationship between x + y can’t be perfectly linear. To see how close it is:
```{r}
# compute correlation
cor(x, y)
```

So x + y can be related pretty well using a straight line

To compute the correlation for ourselves rather than using `cor`, use `lm` + scaling of both x + y by subtracting the mean of both variables + dividing out by the SD via `scale`
```{r}
coef(lm(scale(y) ~ scale(x)))
```

In this case the **correlation between x + y** is *exactly equal to the coefficient relating the 2 in linear regression after scaling both of them*, a general fact about how correlations work

B/c correlation is just a measure of how linear a relationship between 2 variables is, it tells us nothing about **causality**. Nevertheless, it’s very important to know whether 2 things are correlated if you want to use 1 to make predictions about the other.