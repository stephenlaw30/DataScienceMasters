---
title: "Ch5_RegressionPredictingPageViews"
author: "Steve Newns"
date: "November 23, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(tm)
library(magrittr)
library(plyr) # ddply
```

# Introducing Regression

**Regression** = predict one set of #'s given another set of #'s
In classification problems, might use #'s as a dummy code for a categorical distinction so 0 = ham and 1 = spam. But these #'s are just symbols; we’re not exploiting the “numberness” of 0 or 1 using dummy variables. 

In regression, b/c predicting numbers, you want to be able to make strong statements about the relationship between the inputs+ the output (when # of packs of cigs a person smokes per day doubles, predicted life span gets cut in half).

Problem = wanting to make precise numerical predictions isn’t the same thing as actually being able to make predictions. To make quantitative predictions, we need to come up w/ some rule that can leverage the info we have access to. The various regression algorithms that statisticians have developed over thelast 200 years all provide different ways to make predictions by turning inputs into outputs. 

#The Baseline Model

Simplest possible way to use the info we have as inputs = ignore inputs entirely + predict future outcomes based only on mean value of the output seen in the past (completely ignore a person’s health records + simply guess that they’ll live as long as the average person lives).

Guessing the mean outcome for every case isn’t as naive as it might seem: if interested in making predictions that are as close to the truth as possible without using any other info, guessing the mean output turns out to be the best guess we can possibly make.

A little bit of work has to go into defining “best” to give this claim a definite meaning. 
```{r}
ages <- read.csv('data/longevity.csv')
ggplot(ages, aes(AgeAtDeath, fill = factor(Smokes))) +
  geom_density() +
  facet_grid(Smokes ~ .) + 
  ggtitle("Density plot of 1,000 people’s life spans, facetted by smokers")
```
It seem reasonable to believe smoking matters for longevity b/c the center of the nonsmokers’ life span distribution is shifted to the right of the center of the smokers’ life spans. In other words, the average life span of a non-smoker is longer than the average life span of a smoker. 

But before we describe how you can use info we have about a person’s smoking habits to make predictions about longevity, pretend we didn’t have any of this info --> need to pick a single # as the prediction for every new person, regardless of smoking habits. 

The question of # to pick isn’t a trivial one, b/c it should depend on what you think it means to make good predictions. There are a lot of reasonable ways to define accuracy of predictions, but there is a single measure of quality that’s been dominant for virtually the entire history of statistics = **squared error**. If you’re trying to predict a value y + guess h, squared error of your guess = (y - h)^2

If using squared error to measure quality of predictions, the best guess we can possibly make about a person’s life span (without any additional info about a person’s habits) = the average person’s longevity.

W/ `longevity` data set, the mean `AgeAtDeath` is:
```{r}
mean(ages$AgeAtDeath)
```

Then ask: “How badly would we have predicted the ages of the people in our data set if we’d guessed they all lived to be 73?”

```{r}
# combine all of squared errors for each person in data set by computing mean of the squared errors (MSE) 
guess <- 73
with(ages, mean((AgeAtDeath - guess) ^ 2))
```
MSE made by guessing 73 for every person in our data set is 32.991. That, by itself, shouldn’t be enough to convince you that we’d do worse with a guess that’s not 73. Consider how we’d perform if we made some of the other possible guesses

```{r}
# loop over a sequence of possible guesses ranging from 63 to 83:
guess.accuracy <- data.frame()
for (guess in seq(63, 83, by = 1)) {
  # for each guess in sequence, get MSE
  prediction.error <- with(ages, mean((AgeAtDeath - guess)**2))
  
  # add MSE to data frame of MSE's
  guess.accuracy <- rbind(guess.accuracy, data.frame(Guess = guess, Error = prediction.error))
}

ggplot(guess.accuracy, aes(Guess, Error)) +
  geom_point() +
  geom_line()
```
Using any guess other than 73 gives us worse predictions for our data set. This is actually a general theoretical result that can be proven mathematically: **to minimize squared error, predict the mean value in a data set**. 

1 important implication of this = predictive value of having info about smoking should be measured in terms of the amount of *improvement you get from using this info over just using the mean value as your guess for every person you see*.

# Regression Using Dummy Variables

How can we use info about whether or not people smoke to make better guesses about longevity? 1 simple idea = estimate mean age at death for smokers and nonsmokers separately +use these 2 separate values as guesses for future cases, depending on whether or not a new person smokes. 

This time, instead of using MSE, use **root mean squared error (RMSE)** = more popular in ML literature.
```{r}
# get mean Age at death over all observations
constant.guess <- with(ages, mean(AgeAtDeath))
# get RMSE for each age in observation
paste("RMSE without smoking information:",with(ages, sqrt(mean((AgeAtDeath - constant.guess) ^ 2))))

# do same as above but for split data sets on binary  smoking variable
smokers.guess <- with(subset(ages, Smokes == 1), mean(AgeAtDeath))
non.smokers.guess <- with(subset(ages, Smokes == 0), mean(AgeAtDeath))

ages <- transform(ages, NewPrediction = ifelse(Smokes == 0, non.smokers.guess, smokers.guess))
# get new RMSE
paste("RMSE with smoking information:",with(ages, sqrt(mean((AgeAtDeath - NewPrediction) ^ 2))))
```

Predictions really do get better after include more info about the people we’re studying = prediction error when estimating people’s life spans becomes 10% smaller when we include info about people’s smoking habits. 

In general, we can do better than using just the mean value whenever we have binary distinctions that separate 2 types of DP's, *assuming those binary distinctions are related to the output we’re trying to predict.* Some simple examples where binary distinctions might help = contrasting men w/ women or Democrats w/ Republicans in American political discourse.

So now we have a mechanism for incorporating dummy variables into predictions. But how can we use "richer" info about the objects in our data? 
<ul>
<li> Want to know how we can use inputs that *aren’t* binary distinctions, but instead continuous values such as heights or weights </li>
<li> Want to know how we can use multiple sources of info all at once to improve estimates. </li>
</ul>

Intuition = having separate sources of info should tell us more than either variable in isolation. But making use of all of the info we have isn’t an easy task. In practice, need to make some simplifying assumptions to get things to work, assumptions that underlie linear regression.

Using *only* linear regression is less of a restriction than it might seem, as linear regression is used in at least 90% of practical regression applications + can be hacked to produce more sophisticated forms of regression with only a little bit of work.

#Linear Regression in a Nutshell

2 biggest assumptions we make when using linear regression to predict outputs are the following:
<ul>
<li> **Separability/additivity** = If there are multiple pieces of info that would affect our guesses, we produce our guess by adding up the effects of each piece of info as if each piece were being used in isolation (ex: alcoholics live 1 year less than nonalcoholics + smokers live 5 years less than nonsmokers = alcoholic who’s also a smoker should live 1 + 5 = 6 years less than a nonalcoholic nonsmoker. **Interactions** = a technique for working around the separability assumption in linear regression (ex: effect of excessive drinking is much worse if you also smoke) </li>
<il> **Monotonicity/linearity** = monotomic model = changing 1 input always makes predicted output go up/down (ex: assuming every time somebody’s height increases, prediction of their weight will go up). Monotonicity is a strong assumption b/c output can go up for a bit + then start to go down, but monotonicity assumption is much less strong than the full assumption of the linear regression algorithm, **linearity** =  changing the input by 1 unit always adds/subtracts N units. Every linear model is monotonic, but curves can be monotonic without being linear. For that reason, linearity is more restrictive than monotonicity </li>
</ul>

```{r}
# linear example
heights.weights <- read.csv('data/01_heights_weights_genders.csv', header = TRUE, sep = ',')
ggplot(heights.weights, aes(Height, Weight)) + 
  geom_point() +
  geom_smooth(method = 'lm') # plot linear fit to DPs

regression.fit <- lm(Weight ~ Height, heights.weights)
coef(regression.fit)
slope <- coef(regression.fit)[2]
intercept <- coef(regression.fit)[1]
```

Every increase of 1 inch in someone’s height leads to an increase of 7.7 lbs in weight. That strikes us as pretty reasonable. Intercept tells you how much a person who is 0 inches tall would weigh, –350 lbs. This regression model isn’t so great for children or extremely short adults.

Systematic problem for linear regression in general = predictive models usually are not very good at predicting outputs for inputs that are very far removed from all inputs seen in the past (regressions are good at interpolation but not very good at extrapolation). Often you can do some work to improve quality of guesses outside of the range of data used to train a model, but in this case there’s probably no need b/c you’re usually only going to make predictions for people who are over 4’ tall and under 8’ tall.

When making predictions in a practical context, coefficients are all that you need to know. To get a sense of where a model is wrong, compute model’s predictions + compare against inputs. 
```{r}
y_hat <- predict(regression.fit)

#calculate differences between predictions + truth
y <- with(heights.weights,Weight)
residuals <- y - y_hat
```

**residuals** = the part of data left over after accounting for the part that a line can explain. A common way to diagnose any obvious mistakes when using linear regression is to plot the residuals against the truth

```{r}
# plot only 1st regression diagnostic plot
plot(regression.fit,which = 1)
```

Our linear model works well because there’s no systematic structure/pattern in the residuals, unlike below

```{r}
x <- 1:10
y2 <- x ^ 2
fitted.regression <- lm(y2 ~ x)
plot(fitted.regression, which = 1)
```

A model should divide the world into **signal** (`predict()` result) + **noise** (residuals). If you can see signal in the residuals using naked eye, the model isn’t powerful enough to extract all the signal + leave behind only real noise as residuals. 

To solve this problem, use more powerful models for regression than the simple linear regression model, but these models can be so powerful that they’re actually too powerful to be used w/out caution.

Simplest measurement of error is = **sum of squared errors**
```{r}
squared.errors <- residuals ^ 2
sum(squared.errors)
```

Simple sum of squared errors quantity = useful for comparing different models, but has some quirks most people end up disliking
<ul>
<li> larger for big data sets than for small data sets = solve w/ **mean squared error (MSE)** rather than sum. MSE won’t grow consistently as we get more data the way raw sum of squared errors wil </li>
<li> MSE still has a problem: if average prediction is only off by 5, MSE will be 25 b/c we’re squaring errors before taking their mean = take square root of the MSE to get **root mean squared error (RMSE)** = a very popular measure of performance for assessing ML algorithms, including algorithms that are far more sophisticated than linear regression (like the Netflix Prize) </li>
<li> 1 complaint for RMSE = not immediately clear what mediocre performance is. Perfect performance clearly gives RMSE of 0, but pursuit of perfection is not a realistic goal in these tasks. </li>
<li> Likewise, it isn’t easy to recognize when a model is performing very poorly. For example, if everyone’s heights are 5' + you predict 5,000', you’ll get a huge RMS + you can do worse by predicting 50,000' + still worse by predicting 5,000,000'. The *unbounded values* RMSE can take on make it difficult to know whether your model’s performance is reasonable. </li>
<li> Solve w/ **R2** = idea is to see how much better a model does than if just using the mean, as R2 will always be between 0-1, where 0 = doing no better than the mean + 1 = predicting every DP perfectly </li>
<li> Multiply it by 100 = the % of the variance in the data you’ve explained w/ your model = a handy way to build up an intuition for how accurate a model is, even in a new domain where you don’t have any experience about the standard values for RMSE. </li>
</ul>

To calculate R2, compute RMSE for a model that uses only the mean output to make predictions for all example data, + then compute the RMSE for your model. After that, it's just a simple arithmetic operation to produce
R2
```{r}
mean.rmse <- 1.09209343
model.rmse <- 0.954544
(r2 <- 1 - (model.rmse / mean.rmse))
```


# Predicting Web Traffic
Case study = predict amount of page views for the top 1,000 websites on the Internet as of 2011
```{r}
websites <- read_tsv("./data/top_1000_sites.tsv")
head(websites)
```

We’ll focus on five columns `Rank`, `PageViews` (in the year, the outcome), `UniqueVisitors` (monthly), `HasAdvertising`, + `IsEnglish`. A good exercise to work on would be comparing `PageViews`w/ `UniqueVisitors` to find a way to tell which sorts of sites have lots of repeat visits vs. those w/ very few repeat visits.

You might think ads would be annoying + that people would, all else being equal, tend to avoid sites that have ads, which we can explicitly test for w/ regression. 1 of the great values of regression is it lets us try to answer questions in which we have to talk about “all else being equal”. “Try” b/c the quality of a regression is only as good as the inputs we have. If an important variable is missing from our inputs, the results of a regression can be very far from the truth + for that reason, always assume the results of a regression are tentative: “If the inputs we had were sufficient to answer this question, then the answer would be....”.

Looking through the list, it’s clear most top sites are primarily either English or Chinese-language sites. It’s interesting to ask whether being in English is a positive thing or not. It’s also an example in which the direction of causality isn’t at all clear from a regression; does being written in English make a site more popular, or do more popular sites decide to convert to English b/c it’s the lingua franca of the Internet? A regression model can tell if 2 things are related, but can’t tell whether one causes the other thing.

Start to get a sense of how these things relate to one another. We’ll 
start by making a scatterplot that relates 
PageViews
 with 
UniqueVisitors
. We always
suggest drawing scatterplots for numerical variables before you try to relate them by
using regression because a scatterplot can make it clear when the linearity assumption
of regression isn’t satisfied.
top.1000.sites <- read.csv('data/top_1000_sites.tsv',
                           sep = '\t',
                           stringsAsFactors = FALSE)
ggplot(top.1000.sites, aes(x = PageViews, y = UniqueVisitors)) +
  geom_point()
The scatterplot we get from this call to 
ggplot
 (shown in 
Figure 5-6
) looks terrible:
almost all the values are bunched together on the x-axis, and only a very small number
jump out from the pack. This is a common problem when working with data that’s not
normally distributed, because using a scale that’s large enough to show the full range
of values tends to place the majority of data points so close to each other that they can’t
be separated visually. To confirm that the shape of the data is the problem with this
plot, we can look at the distribution of 
PageViews
 by itself:
ggplot(top.1000.sites, aes(x = PageViews)) +
  geom_density()
This density plot (shown in 
Figure 5-7
) looks as completely impenetrable as the earlier
scatterplot. When you see nonsensical density plots, it’s a good idea to try taking the
log of the values you’re trying to analyze and make a density plot of those log-trans-
formed values. We can do that with a simple call to R’s 
log
 function:
ggplot(top.1000.sites, aes(x = log(PageViews))) +
  geom_density()
This density plot (shown in 
Figure 5-8
) looks much more
 reasonable. For that reason,
we’ll start using the log-transformed 
PageViews
 and 
UniqueVisitors
 from now on.
Recreating our earlier scatterplot on a log scale is easy:
ggplot(top.1000.sites, aes(x = log(PageViews), y = log(UniqueVisitors))) +
  geom_point()
Predicting Web Traffic
|
143
The 
ggplot2
 package also contains a convenience function to change
the  scale  of  an  axis  to  the  log.  You  can  use  the 
scale_x_log
  or
scale_y_log
 in this case. Also, recall from our discussion in 
Chapter 4
that in some cases you will want to use the 
logp
 function in R to avoid
the errors related to taking the log of zero. In this case, however, that is
not a problem.
The resulting scatterplot shown in 
Figure 5-9
 looks like there’s a potential line to be
drawn using regression. Before we use 
lm
 to fit a regression line, let’s use 
geom_smooth
with the 
method = 'lm'
 argument to see what the regression line will look lik
ggplot(top.1000.sites, aes(x = log(PageViews), y = log(UniqueVisitors))) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
The resulting line, shown in 
Figure 5-10
, looks promising, so let’s find the values that
define its slope and intercept by calling 
lm
:
lm.fit <- lm(log(PageViews) ~ log(UniqueVisitors),
            data = top.1000.sites)
Now that we’ve fit the line, we’d like to get a quick summary of it. We could look at
the coefficients using 
coef
, or we could look at the RMSE by using 
residuals
. But we’ll
introduce another function that produces a much more complex summary that we can
walk through step by step. That function is conveniently called 
summary
:
Figure 5-7. Density plot for PageViews
Predicting Web Traffic
|
145
summary(lm.fit)
#Call:
#lm(formula = log(PageViews) ~ log(UniqueVisitors), data = top.1000.sites)
#
#Residuals:
#    Min      1Q  Median      3Q     Max 
#-2.1825 -0.7986 -0.0741  0.6467  5.1549 
#
#Coefficients:
#                    Estimate Std. Error t value Pr(>|t|)    
#(Intercept)         -2.83441    0.75201  -3.769 0.000173 ***
#log(UniqueVisitors)  1.33628    0.04568  29.251  < 2e-16 ***
#---
Figure 5-8. Log-scale density plot for PageViews
146
|
Chapter 5:
Regression: Predicting Page Views
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
#
#Residual standard error: 1.084 on 998 degrees of freedom
#Multiple R-squared: 0.4616,    Adjusted R-squared: 0.4611 
#F-statistic: 855.6 on 1 and 998 DF,  p-value: < 2.2e-16
The first thing that 
summary
 tells us is the call we made to 
lm
. This isn’t very useful when
you’re working at the console, but it can be helpful when you’re working in larger
scripts that make multiple calls to 
lm
. When this is the case, this information helps you
keep all of the models organized so you have a clear understanding of what data and
variables went into each model.
Figure 5-9. Log-scale scatterplot of UniqueVisitors versus PageViews
Predicting Web Traffic
|
147
The next thing that 
summary
 tells us are the quantiles of the residuals that you would
compute if you called 
quantile(residuals(lm.fit))
. Personally, we don’t find this very
helpful, although others may prefer looking for symmetry in the minimum and maxi-
mum residual, rather than looking at a scatterplot of the data. However, we almost
always find graphical plots more informative than numerical summaries.
Next, 
summary
 tells us the coefficients of the regression in much more detail than 
coef
would. The output from 
coef
 ends up in this table as the “Estimate” column. After that,
there are columns for the “Std. Error,” the “t value,” and the p-value of each coefficient.
These values are used to assess the uncertainty we have in the estimates we’ve compu-
ted; in other words, they’re meant to be measurements of our confidence that the values
we’ve computed from a specific data set are accurate descriptions of the real world that
Figure 5-10. Log-scale scatterplot of UniqueVisitors versus PageViews with a regression line
148
|
Chapter 5:
Regression: Predicting Page Views
generated the data. The “Std. Error”, for instance, can be used to produce a 95% con-
fidence interval for the coefficient. The interpretation of this interval can be confusing,
and it is occasionally presented in a misleading way. The interval indicates the bounds
for which we can say, “95% of the time, the algorithm we use to construct intervals
will include the true coefficient inside of the intervals it produces.” If that’s unclear to
you, that’s normal: the analysis of uncertainty is profoundly important, but far more
difficult than the other material we’re covering in this book. If you really want to un-
derstand this material, we’d recommend buying a proper statistics textbook such as
All of Statistics 
[Wa03
] or 
Data Analysis Using Regression and Multilevel/Hierarchical
Models
 [GH06
], and working through them in detail. Thankfully, the sort of qualitative
distinctions that require attention to standard errors usually aren’t necessary if you’re
just trying to hack together models to predict something.
The “t-value” and the p-value (written as “Pr(>|t|)” in the output from 
summary
) are
both measures of how confident we are that the true coefficient isn’t zero. This is used
to say that we are confident that there is a real relationship between the output and the
input we’re currently examining. For example, here we can use the “t value” column
to assess how sure we are that 
PageViews
 really are related to 
UniqueVisitors
. In our
minds, these two numbers can be useful if you understand how to work with them,
but they are sufficiently complicated that their usage has helped to encourage people
to assume they’ll never fully understand statistics. If you care whether you’re confident
that two variables are related, you should check whether the estimate is at least two
standard errors away from zero. For example, the coefficient for 
log(UniqueVisitors)
is  1.33628  and  the  standard  error  is  0.04568.  That  means  that  the  coefficient  is
1.33628 / 0.04568 == 29.25306
 standard errors away from zero. If you’re more than
three standard errors away from zero, you can feel reasonably confident that your two
variables are related.
t-values and p-values are useful for deciding whether a relationship be-
tween two columns in your data is real or just an accident of chance.
Deciding that a relationship exists is valuable, but understanding the
relationship is another matter entirely. Regressions can’t help you do
that. People try to force them to, but in the end, if you want to under-
stand the reasons why two things are related, you need more informa-
tion than a simple regression can ever provide.
The traditional cutoff for being confident that an input is related to your output is to
find a coefficient that’s at least two standard errors away from zero.
The next piece of information that 
summary
 spits out are the significance codes for the
coefficients. These are asterisks shown along the side that are meant to indicate how
large the “t value” is or how small the p-value is. Specifically, the asterisks tell you
whether you’re passed a series of arbitrary cutoffs at which the p-value is less than 0.1,
less than 0.05, less than 0.01, or less than 0.001. Please don’t worry about these values;
they’re disturbingly popular in academia, but are really holdovers from a time when
Predicting Web Traffic
|
149
statistical analysis was done by hand rather than on a computer. There is literally no
interesting content in these numbers that’s not found in asking how many standard
errors your estimate is away from 0. Indeed, you might have noticed in our earlier
calculation that the “t value” for 
log(UniqueVisitors)
 was exactly the number of stan-
dard errors away from 0 that the coefficient for 
log(UniqueVisitors)
 was. That rela-
tionship between t-values and the number of standard errors away from 0 you are is
generally true, so we suggest that you don’t work with p-values at all.
The final pieces of information we get are related to the predictive power of the linear
model we’ve fit to our data. The first piece, the “Residual standard error,” is simply the
RMSE 
of 
the 
model 
that 
we 
could 
compute
using 
sqrt(mean(residuals(lm.fit) ^ 2))
. The “degrees of freedom” refers to the no-
tion that we’ve effectively used up two data points in our analysis by fitting two coef-
ficients: the intercept and the coefficient for 
log(UniqueVisitors)
. This number, 998,
is relevant because it’s not very impressive to have a low RMSE if you’ve used 500
coefficients in your model to fit 1,000 data points. Using lots of coefficients when you
have very little data is a form of overfitting that we’ll talk about more in 
Chapter 6
.
After that, we see the “Multiple R-squared”. This is the standard “R-squared” we de-
scribed earlier, which tells us what percentage of the variance in our data was explained
by our model. Here we’re explaining 46% of the variance using our model, which is
pretty good. The “Adjusted R-squared” is a second measure that penalizes the “Mul-
tiple R-squared” for the number of coefficients you’ve used. In practice, we personally
tend to ignore this value because we think it’s slightly ad hoc, but there are many people
who are very fond of it.
Finally, 
the last piece of information you’ll see is the “F-statistic.” This is a measure of
the improvement of your model over using just the mean to make predictions. It’s 
an
alternative to “R-squared” that allows one to calculate a “p-value.” Because we think
that a “p-value” is usually deceptive, we encourage you to not put too much faith in
the F-statistic. “p-values” have their uses if you completely understand the mechanism
used to calculate them, but otherwise they can provide a false sense of security that will
make you forget that the gold standard of model performance is predictive power on
data that wasn’t used to fit your model, rather than the performance of your model on
the data that it was fit to. We’ll talk about methods for assessing your model’s ability
to predict new data in 
Chapter 6
.
Those pieces of output from 
summary
 get us quite far, but we’d like to include some
other sorts of information beyond just relating 
PageViews
 to 
UniqueVisitors
. We’ll also
include 
HasAdvertising
 and 
IsEnglish
 to see what happens when we give our model
more inputs to work with:
lm.fit <- lm(log(PageViews) ~ HasAdvertising + log(UniqueVisitors) + InEnglish,
             data = top.1000.sites)
summary(lm.fit)
#Call:
#lm(formula = log(PageViews) ~ HasAdvertising + log(UniqueVisitors) + 
150
|
Chapter 5:
Regression: Predicting Page Views
#    InEnglish, data = top.1000.sites)
#
#Residuals:
#    Min      1Q  Median      3Q     Max 
#-2.4283 -0.7685 -0.0632  0.6298  5.4133 
#
#Coefficients:
#                    Estimate Std. Error t value Pr(>|t|)    
#(Intercept)         -1.94502    1.14777  -1.695  0.09046 .  
#HasAdvertisingYes    0.30595    0.09170   3.336  0.00088 ***
#log(UniqueVisitors)  1.26507    0.07053  17.936  < 2e-16 ***
#InEnglishNo          0.83468    0.20860   4.001 6.77e-05 ***
#InEnglishYes        -0.16913    0.20424  -0.828  0.40780    
#---
#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 
#
#Residual standard error: 1.067 on 995 degrees of freedom
#Multiple R-squared: 0.4798,    Adjusted R-squared: 0.4777 
#F-statistic: 229.4 on 4 and 995 DF,  p-value: < 2.2e-16
Again, we see that 
summary
 echoes our call to 
lm
 and prints out the residuals. What’s
new in this summary are the coefficients for all of the variables we’ve included in our
more complicated regression model. Again, we see the intercept printed out as 
(Inter
cept)
. The next entry is quite different from anything we saw before because our model
now includes a factor. When you use a factor in a regression, the model has to decide
to include one level as part of the intercept and the other level as something to model
explicitly. Here you can see that 
HasAdvertising
 was modeled so that sites for which
HasAdvertising == 'Yes'
 are separated from the intercept, whereas sites for which
HasAdvertising == 'No'
 are folded into the intercept. Another way to describe this is
to say that the intercept is the prediction for a website that has no advertising and has
zero 
log(UniqueVisitors)
, which actually occurs when you have one 
UniqueVisitor
.
We can see the same logic play out for 
InEnglish
, except that this factor has many 
NA
values, so there are really three levels of this dummy variable: 
NA
, 
'No'
, and 
'Yes'
. In
this case, R treats the 
NA
 value as the default to fold into the regression intercept and
fits separate coefficients for the 
'No'
 and 
'Yes'
 levels.
Now that we’ve considered how we can use these factors as inputs to our regression
model, let’s compare each of the three inputs we’ve used in isolation to see which has
the most predictive power when used on its own. To do that, we can extract the
R-squared for each summary in isolation:
lm.fit <- lm(log(PageViews) ~ HasAdvertising,
             data = top.1000.sites)
summary(lm.fit)$r.squared
#[1] 0.01073766
lm.fit <- lm(log(PageViews) ~ log(UniqueVisitors),
             data = top.1000.sites)
summary(lm.fit)$r.squared
#[1] 0.4615985
Predicting Web Traffic
|
151
lm.fit <- lm(log(PageViews) ~ InEnglish,
             data = top.1000.sites)
summary(lm.fit)$r.squared
#[1] 0.03122206
As you can see, 
HasAdvertising
 explains only 1% of the variance, 
UniqueVisitors
 ex-
plains 46%, and 
InEnglish
 explains 3%. In practice, it’s worth including all of the inputs
in a predictive model when they’re cheap to acquire, but if 
HasAdvertising
 were difficult
to acquire programmatically, we’d advocate dropping it in a model with other inputs
that have so much more predictive power