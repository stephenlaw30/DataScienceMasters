---
title: "Ch8_PCA_BuildingMarketIndex"
author: "Steve Newns"
date: "December 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(magrittr)
library(lubridate) # ymd()
library(reshape) # cast()
```

# Unsupervised Learning

Often we want to find structure w/out having any answers available to us about how well we’re doing  = unsupervised learning. Might want to perform **dimensionality reduction** = shrink a table w/ a huge # of cols into a table w/ a small # of cols. If you have too many cols to deal w//, this dimensionality reduction goes a long way toward making a data set comprehensible. Although you clearly lose info when you replace many columns w/ a single column, the gains in understanding are often valuable, especially when exploring a new data set. 

1 place where this type of dimensionality reduction is particularly helpful: dealing w/ stock market data. Might have data that looks like the real historical prices for 25 stocks from January 2, 2002 until May 25, 2011. There are 25, cols far too many to deal w/ in a thoughtful way. We want to create a single column that tells us how the market is doing on each day by combining info in the 25 cols we DO have + calling it an **index** of the market.

The simplest approach to this = **principal components analysis (PCA)** w/ the main idea = to create a *new* set of 25 cols that're ordered based on how much of the raw info in our data set they contain. The 1st new col/the 1st **principal component** will often contain the vast majority of the structure in the entire data set. PCA is particularly effective when cols in a data set are all strongly correlated. In that case, you can replace correlated columns w/ a single column that matches an underlying pattern that accounts for the correlation between both columns.

Test whether or not PCA will work by seeing how correlated the columns in our data set are.
```{r}
prices <- read.csv('data/stock_prices.csv')
# check 1st row
prices[1,]
```

Our raw data set isn’t in the format we’d like to work w/

# Preprocessing.

1) Translate all datestamps into properly encoded date variables via `ymd()` from `lubridate`/
```{r}
prices %<>% mutate(Date = ymd(Date))
```
Once we’ve done this, use `cast` to create a data matrix w/ rows = days + columns = separate stocks. 
```{r}
# convert date column into rows by stocks as cols with cells values = close value
date.stock.mtx <- cast(prices, Date ~ Stock, value = "Close")
head(date.stock.mtx)
summary(date.stock.mtx)
```

Notice there are some missing entries --> go back to `prices`, remove missing entries, + rerun cast.
```{r}
prices <- read.csv('data/stock_prices.csv')
#prices <- na.omit(prices)
prices %<>% mutate(Date = ymd(Date)) %>%
  filter(Date > ymd("2001-12-31") & Date != ymd("2002-02-01"),
         Stock != "DDR")

date.stock.mtx <- cast(prices, Date ~ Stock, value = "Close")
table(is.na(date.stock.mtx))
```

Now find the correlations between all numeric columns in the matrix using `cor`, turn the correlation matrix into
a single numeric vector, + create a density plot of the correlations to get a sense of both the mean correlation + the frequency w/ which low correlations occur:
```{r}
cor.mtx <- cor(date.stock.mtx[,2:ncol(date.stock.mtx)])
correlations <- as.numeric(cor.mtx)
ggplot(data.frame(Correlation = correlations), aes(Correlation, fill = 1)) +
  geom_density() + 
  guides(fill = F)
```

Vast majority of correlations are positive, so PCA will probably work well on this data set.

Having convinced ourselves we can use PCA, the entirety of PCA can be done in 1 line of code via `princomp`
```{r}
pca.stock <- princomp(date.stock.mtx[,2:ncol(date.stock.mtx)])
pca.stock
```

The SD's tell us how much of the variance in the data set is accounted for by the different principal components. The 1st component, `Comp.1`, accounts for 29% of the variance, while the next component accounts for 20%. By the end, the last component, `Comp.25`, accounts for less than 1/2% of the variance. This suggests that we can learn a lot about our data by just looking at the 1st principal component.

We can examine the 1st principal component in more detail by looking at its **loadings**, which tell us how much **weight** it gives to each of the columns. Extracting loadings gives us a big matrix that tells us how much each of the 25 columns gets puts into each of the principal components. We’re really only interested in the 1st principal component, so we pull out the 1st column of the `pca` loadings:
```{r}
# extract loadings element of the principal component from pca object
principal.component <- pca.stock$loadings[,1]
```

Now examine a density plot of the loadings to get a sense of how the 1st principal component is formed:
```{r}
loadings <- as.numeric(principal.component)
ggplot(data.frame(Loading = loadings), aes(Loading, fill = 1)) + 
  geom_density() + 
  guides(fill = F)
```

The results are a little suspicious b/c there’s a nice distribution of loadings, but they’re overwhelmingly negative. This is actually a trivial nuisance that a single line of code will fix.

Now that we have our principal component, we want to generate our 1-col summary of our data set w/ `predict`
```{r}
market.index <- predict(pca.stock)[,1]
```

To tell whether these predictions are any good, we compare against famous market indices, such as the Dow Jones Index, whose date we have to convert + b/c the DJI runs for so much longer than we want, subset to get only dates we’re interested in:
```{r}
dji.prices <- read.csv('data/DJI.csv')
dji.prices %<>% mutate(Date = ymd(Date)) %>%
  filter(Date > ymd("2001-12-31") & Date != ymd("2002-02-01"))
```

Now extract the parts of the DJI we’re interested in = daily closing prices + the associated dates. B/c they’re in the opposite order of our current data set, use `rev` to reverse them, + then make some simple graphical plots to compare our market index generated using PCA w/ the DJI:
```{r}
dji <- with(dji.prices, rev(Close))
dates <- with(dji.prices, rev(Date))
comparison <- data.frame(Date = dates, MarketIndex = market.index, DJI = dji)

ggplot(comparison, aes(MarketIndex, DJI)) + 
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

Those *negative loadings* that seemed suspicious before turn out to be a real source of trouble for our data set: our
index is *negatively* correlated w/ DJI. Fix by multiplying our index by  -1 to produce an index correlated in the right direction with the DJI + try our comparison again
```{r}
comparison %<>% mutate(MarketIndex = -1*MarketIndex)
ggplot(comparison, aes(MarketIndex, DJI)) + 
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```

Looks like our index matches the DJI really well using just the principal component (1st column). The only thing that’s missing is to get a sense of how well our index tracks the DJI over time. To get this, `melt` a dataframe that’s easy to work with for visualizing both indices at once + then make a line plot in which the x-axis = date + y-axis = price of each index.
```{r}
# melt down MarketIndex and DJI cols into single "index" col and put their values into a new col "Price" 
# make date = row #/index
alt.comparison <- melt(comparison, id.vars = 'Date')
names(alt.comparison) <- c('Date', 'Index', 'Price')
ggplot(alt.comparison, aes(Date, Price, group = Index, color = Index)) +
  geom_point() +
  geom_line()
```

This doesn’t look so good b/c the DJI takes on very high values, whereas our index takes on very small values. Fix this w/ `scale` to put both indices on a common scale:
```{r}
# scale data
comparison %<>% mutate(MarketIndex = scale(MarketIndex),
                       DJI = scale(DJI))
#head(comparison)

# melt down MarketIndex and DJI cols into single "index" col and put their values into a new col "Price" 
# make date = row #/index
alt.comparison <- melt(comparison, id.vars = 'Date')
names(alt.comparison) <- c('Date', 'Index', 'Price')
ggplot(alt.comparison, aes(Date, Price, group = Index, color = Index)) +
  geom_point() +
  geom_line()
```

This plot makes it seem like our market index created entirely w/ PCA w/out require any domain knowledge about the stock market, tracks the DJI incredibly well. In short, PCA really works to produce a reduced representation of stock prices that looks just like something we might create w/ more careful thought about how to express the general well-being of the stock market. We think that’s pretty amazing.

Hopefully this example convinces you that PCA is a powerful tool for simplifying data + can actually do a lot to discover structure in your data even when you’re not trying to predict something. If interested in this topic, look into **independent component analysis (ICA)**, an alternative to PCA that works well in some circumstances where PCA breaks down.