---
title: "ISLR_Ch4_LogisticRegression"
author: "Steve Newns"
date: "April 27, 2018"
output: html_document
---
## 4.6.1 The Stock Market Data

Begin by examining some numerical + graphical summaries of `Smarket` data (`ISLR` library) = % returns for S&P 500 stock index over 1,250 days, from the beginning of 2001 until end of 2005. For each date, we have recorded the % returns for each of the 5 previous trading days, `Lag1` through `Lag5` + recorded `Volume` (# of shares traded on previous day, in billions), `Today` (% return on date in question) + `Direction` (whether market was Up/Down on this date)
```{r,echo=F}
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
```
```{r,warning=F}
library(ISLR)
library(mosaic)
library(ggplot2)

names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
```

`cor()` = produce matrix w/ all of pairwise correlations among predictors in a data set (ignoring `Direction` = qualitative)
```{r,warning=F}
library(tidyverse)
library(corrplot)

Smarket[,1:8] %>%
  #select(Year,Lag1,Lag2,Lag3,Lag4,Lag5,Volume,Today) %>%
  cor()

corrplot(cor(dplyr::select(Smarket,-Direction),
                  use="pairwise.complete.obs"), method = "number", type = "lower")
```

As expected, correlations between `Lag` variables + today's returns = ~0, so appears to be little correlation between today's + previous days' returns, w/ only substantial correlation = between `Year` and `Volume`. 
```{r}
Smarket %>%
  mutate(rowNum = as.numeric(row.names(Smarket))) %>%
  ggplot(aes(rowNum,Volume)) + 
    geom_point() + 
    labs(x="Index",main="Volume of trades over Time")
#plot(Smarket$Volume)
```

By plotting the data we see `Volume` is *slightly* increasing over time == average # of shares traded daily increased from 2001 to 2005.

## 4.6.2 Logistic Regression

Next, we will fit a logistic regression model in order to predict `Direction` using `Lag1`-`Lag5` + `Volume` via `glm()` = fits **generalized linear models** (*class of models* that includes logistic regression) 

* like `lm()` but must pass in argument `family=binomial` for a logistic regression rather than some generalized linear model

```{r}
glm_fit <- glm(Direction ~ . - Year - Today, Smarket, family = "binomial")
msummary(glm_fit)
```

Smallest $p$-value here is associated w/ `Lag1` + negative coefficient suggests if the market had a positive return yesterday, it is less likely to go up today. However, at a value of 0.15, the $p$-value is still relatively large, and so there is no clear evidence of a real association between `Lag1` and `Direction`.

`coef()` accesses just the coefficients for this fitted model + `msummary()` access particular aspects of the fitted model, such as the $p$-values for the coefficients
```{r}
coef(glm_fit)
summary(glm_fit)$coef
summary(glm_fit)$coef[,4] # just p-values
```

`predict()` = predict probability market will go up, given values of the predictors w/ `type="response"` to output probabilities of the form $P(Y = 1|X)$, as opposed to other info, such as the **logit**

* If no data set is supplied to `predict()`, probabilities are computed for the *training* data used to fit the model. 

```{r}
glm_probs <- predict(glm_fit,type="response") # no newdata arg =  predict probability on training data used to fit model
head(glm_probs)
```

We know these values correspond to the probability of market going *up*, rather than down, b/c `contrasts()` indicates R has created a dummy variable w/ 1 = Up
```{r}
contrasts(Smarket$Direction)
```

To make a prediction as to whether the market will go up/down on a particular day, must convert predicted probabilities into class labels, `Up` or `Down`, w/in a vector of class predictions based on whether predicted probability of a market increase is > or <  0.5 (or some other chosen threshold)
```{r,warning=F,message=F}
glm_preds <- rep("Down",1250)   # create vector of '1250' Down elements.
glm_preds[glm_probs>.5] <- "Up" # set value of records  where p > .5 to 'Up'
(cm <- table(glm_preds,Smarket$Direction)) # confusion matrix

sensitivity <- cm[1] / (cm[2] + cm[1]) # true positive rate = TP / (TP + FN)
specificity <- cm[4] / (cm[4] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[3]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

#cat("\nFOR PREDICTING DOWN:")
#cat("\nSensitivity (TP rate or Recall): ",sensitivity)
#cat("\nSpecificity (TN rate): ",specificity)
#cat("\nPrecision (Positive Predictive Value): ",precision)
#cat("\nAccuracy: ",accuracy) # mean(glm_preds==Direction)

library(caret)

confusionMatrix(as.factor(glm_preds), Smarket$Direction) 
cat("\nTraining Error Rate: ", 1 - accuracy) # mean(glm_test_preds!=Direction)
```

Diagonal elements of a confusion matrix = *correct* predictions, while off-diagonals = *incorrect*. Hence our model correctly predicted market would go up on 507 days + correctly predicted it would go down on 145 days, for a total of 507 + 145 = 652 correct predictions = correct 52.2% of the time. 

At first glance, it appears the logistic regression model is working a little better than random guessing (50%). **However, this result is misleading** b/c we trained + tested on the *same set* of 1250 observations. **In other words, 100% - 52.2% = 47.8% = the *TRAINING* error rate, which is often *overly optimistic* as it tends to *underestimate* TEST error rate.** 

To better assess model accuracy: fit the model using *subset* of the data + then examine how well it predicts using *held out data* = will yield a more realistic error rate, in the sense that, in practice, we're interested in model's performance *out-of-sample*, and *not* on training data (i.e. days in the future for which market's movements are unknown)

1st create a vector corresponding to observations from 2001-2004 + use it to create a held out data set of observations from 2005.
```{r}
train <- subset(Smarket,Year<2005)
test <- subset(Smarket,Year>=2005)
dim(test)
```

Now fit a logistic regression model using only training (before 2005)
```{r}
glm_train <- glm(Direction~.-Year-Today,train,family = "binomial")
msummary(glm_train)
```

Then obtain predicted probabilities of the stock market going up for each of the days in test (days in 2005)
```{r}
glm_test_probs <- predict(glm_train,test,type="response")
```

Finally, compute predictions for 2005 + compare to actual movements of the market over that time period.
```{r}
glm_test_preds <- rep("Down",252)   # create vector of 252 (test length) Down elements.
glm_test_preds[glm_test_probs>.5] <- "Up" # set index of elements where p > .5 to Up
(cm <- table(glm_test_preds,test$Direction)) # confusion matrix

sensitivity <- cm[1] / (cm[2] + cm[1]) # true positive rate = TP / (TP + FN)
specificity <- cm[4] / (cm[4] + cm[3]) # recall = true negative rate = TN / (TN + FP)
precision <- cm[1] / (cm[1] + cm[3]) # TP / predicted Pos
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(glm_test_preds), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy) # mean(glm_test_preds!=Direction)
```

Results = rather disappointing, as test error rate = 52%, and *>50% = worse than random guessing*! Of course, this result = not all that surprising, given one wouldn't generally expect to be able to use previous days' returns to predict future market performance.

Recall this model had very underwhelming $p$-values associated w/ all predictors + the smallest $p$-value, though not very small, corresponded to `Lag1`. Perhaps by removing variables that appear not to be helpful in predicting `Direction`, we can
obtain a more effective model. After all, using predictors that have no relationship w/ the response tends to cause a deterioration in test error rate (since ***such predictors cause an increase in variance w/out a corresponding decrease in bias***), so removing such predictors may in turn yield an improvement. 

Refit the regression using just `Lag1` and `Lag2` (seemed to have the highest predictive power in original model)
```{r}
glm_lag12 <- glm(Direction ~ Lag1 + Lag2, train, family = "binomial")
msummary(glm_lag12)

# obtain predicted probabilities
glm_lag12_probs <- predict(glm_lag12,test,type="response")

# compute predictions and model statistics
glm_lag12_preds <- rep("Down",252)   # create vector of 252 (test length) Down elements.
glm_lag12_preds[glm_lag12_probs>.5] <- "Up" # set index of elements where p > .5 to Up
(cm <- table(glm_lag12_preds,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(glm_lag12_preds), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy) # mean(glm_test_preds!=Direction)```
```

Results appear to be a little better = 56% of daily movements have been correctly predicted (`accuracy`). *It is worth noting that in this case, the much simpler strategy of just predicting that the market will increase every day will **also** be correct 56% of the time!* Hence, in terms of overall error rate, the logistic regression method is no better than a naive approach. 

However, the confusion matrix shows that, on days when logistic regression predicts an increase in the market, it has a 58% accuracy rate (`Neg Pred Value`), which suggests a possible trading strategy of buying on days when the model predicts an increasing market + avoiding trades on days when a decrease is predicted. Of course ***one would need to investigate more carefully whether this small improvement was real or just due to random chance***.

## 4.6 Lab: Logistic Regression, LDA, QDA, and KNN

Suppose we want to predict probabilites associated w/ particular values of `Lag1` and `Lag2`, like when `Lag1 + Lag2 = 1.2 + 1.1`, respectively, and on a day when they equal `1.5 + -0.8` --> do this via `predict()`.
```{r}
predict(glm_lag12, newdata = data.frame(Lag1 = c(1.2,1.5), Lag2 = c(1.1,-.8)), type = "response")
```

## 4.6.3 Linear Discriminant Analysis

Now we will perform **LDA** on `Smarket` using `lda()` from `MASS` (identical to `lm()` + `glm()`except for absence of `family` arg) 
```{r}
library(MASS)
lda_fit <- lda(Direction ~ Lag1 + Lag2, train)
lda_fit
plot(lda_fit)
summary(lda_fit)
```

LDA result indicates that $\hat{\pi}_{1}$ = 0.492 + $\hat{\pi}_{2}$ = 0.508; in other words, 49.2% of training observations correspond to days during which the market went down and 50.8% to days when it went up. It also provides **group means** = the average of *each predictor within each class (up/down in `lag1` and `lag2` respectively*, + are used by LDA as estimates of $\mu_k$. 

These suggest there's a tendency for the previous 2 days' returns to be negative on days when the market increases, + a tendency for the previous days' returns to be positive on days when the market declines. The **coefficients of linear discriminants** provides the ***linear combination*** of `Lag1` + `Lag2` that're used to form the **LDA decision rule**. In other words, these $\beta$ = the **multipliers of the elements of $(X = x)$ w/in $\sigma_k(x) = x^{T}\sum^{-1}\mu_k - \frac {1} {2} \mu_k^{T}\sum^{-1}\mu_k + log(\pi_k)$**. 

*  vector/matrix version of $\sigma_k(x) = x\frac {\mu_k} {\sigma^2} - \frac {\mu_k^2} {2\sigma^2} + log(\pi_k)$

If $.642Lag1 - .514Lag2$ is large, LDA classifier will predict market increase, + if small, LDA classifier will predict a market decline. 

The plots show the **linear discriminants** obtained by computing $.642Lag1 - .514Lag2$ for *each training observation*.

`predict()` will return a list w/ 3 elements = **`class`** = LDA's predictions about the movement of the market, **`posterior`** = a matrix whose $k$th column contains the posterior probability that the corresponding observation belongs to the $k$th class, computed from (4.10) and **`x`** = contains the linear discriminants, described earlier.
```{r}
lda_preds <- predict(lda_fit,test)
names(lda_preds)
lda_preds$class
head(lda_preds$posterior)
head(lda_preds$x)
```

```{r}
(cm <- table(lda_preds$class,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(lda_preds$class), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

Applying a 50% threshold to our posterior probabilities allows us to recreate the predictions contained in `lda.pred$class`.
```{r}
head(lda_preds$posterior)
sum(lda_preds$posterior[,1] >= .5) # check posterior probabilities of Down that are above or equal to threshold
sum(lda_preds$posterior[,1] < .5) # check posterior probabilities of Down that are below threshold
```

Notice the **posterior probability output by the model = probability market will decrease**:
```{r}
data.frame("prob of down" = lda_preds$posterior[1:20,1],"classification"=lda_preds$class[1:20])
```

Suppose we wish to predict a market decrease *only if we're very certain market will indeed decrease on that day* == i.e. if posterior probability is at least 90%.
```{r}
sum(lda_preds$posterior[,1] >= .9) # check posterior probabilities of Down that are above or equal to threshold
sum(lda_preds$posterior[,1] < .9) # check posterior probabilities of Down that are below threshold
```

No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 = 52.02%.

## 4.6.4 Quadratic Discriminant Analysis

Now fit a QDA model to Smarket via `qda()` from `MASS` w/ syntax identical to that of `lda()`.
```{r}
qda_fit <- qda(Direction ~ Lag1 + Lag2, train)
qda_fit
```

Output contains group means but does *not* contain the coefficients of the linear discriminants, b/c a QDA classifier involves a
*quadratic* function, rather than a linear function, of the predictors. `predict()` works in exactly the same fashion as for LDA.
```{r}
qda_preds <- predict(qda_fit,test)
names(qda_preds)
qda_preds$class
```

```{r}
(cm <- table(qda_preds$class,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(qda_preds$class), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

Interestingly, QDA predictions = accurate ~60 % of the time, even though the 2005 (test) data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. 

*This suggests the quadratic form assumed by QDA may capture the true relationship more accurately than linear forms assumed by LDA + logistic regression*. However, we recommend evaluating this method's performance on a *larger* test set before betting that this approach will consistently beat the market

## 4.6.5 K-Nearest Neighbors

Perform KNN using `knn()` from `class` library, which works differently from other classification model-fitting functions, since `knn()` requires 4 inputs + forms predictions w/ a single command vs. 2-step approach used before.

* matrix containing predictors associated w/ training data = `train_x`
* matrix containing predictors associated w/ test data = `test_x`
* vector containing class labels for training = `train_labels`
* value for `K` = # of nearest neighbors to be used by classifier.

Use `cbind()` to bind `Lag1` + `Lag2` together into 2 matrices, 1 for training + 1 for test
```{r}
library(class)

train_x <- cbind(train$Lag1,train$Lag2)
test_x <- cbind(test$Lag1,test$Lag2)
train_labels <- train$Direction
```

Now use `knn()` to predict market movement for dates in 2005. Set a random seed before `knn()` b/c if several observations = tied as nearest neighbors, R randomly breaks the tie. Therefore, a seed must be set in order to ensure reproducibility of results.
```{r}
set.seed(1)

knn_preds <- knn(train = train_x, test = test_x, cl = train_labels, k = 1) # cl = class labels

(cm <- table(knn_preds,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

Results using $K = 1$ = not very good = only 50% accuracy on test observations. Of course, it may be that $K = 1$ gives an overly-flexible fit to the data. Below, repeat the analysis using $K = 3$.

```{r}
set.seed(1)

knn_preds <- knn(train = train_x, test = test_x, cl = train_labels, k = 3) # cl = class labels

(cm <- table(knn_preds,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

Results have improved slightly. But increasing $K$ further turns out to provide no further improvements. 
```{r}
set.seed(1)

knn_preds <- knn(train = train_x, test = test_x, cl = train_labels, k = 8) # cl = class labels

(cm <- table(knn_preds,test$Direction)) # confusion matrix

accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test$Direction) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

It appears that ***for this data, QDA provides best results of above methods***

### 4.6.6 An Application to Caravan Insurance Data

Finally, apply KNN to `Caravan` Insurance data set from `ISLR` = 85 predictors that measure demographic characteristics for 5,822 individuals where response = `Purchase` = whether or not given individual purchases a caravan insurance policy (In this data set, only 6% of people purchased it)
```{r}
library(ISLR)
dim(Caravan)
prop.table(table(Caravan$Purchase))
```

***B/c KNN classifier predicts class of a given test observation by IDing observations that're nearest to it, SCALE of the variables matters***. Any variables on a *large scale* = have a much *larger effect* on distance between observations, + therefore on the KNN classifier, than variables on a small scale. 

* Ex: data set w/ 2 variables, `salary` + `age` (measured in $ + years, respectively): As far as KNN is concerned, a difference of $1k in `salary` = enormous compared to a difference of 50 years in age. Consequently, salary drives KNN classification results + age will have almost no effect, contrary to intuition that `salary` difference = $1k = quite small compared to age difference of 50 years

Furthermore, importance of scale to KNN classifier leads to another issue: if we measured `salary` in Japanese yen or `age` in minutes, we'd get quite different results from if these 2 variables are measured in $ and years.

Good way to handle = **standardize the data so that all standardized variables have $\mu$ = zero + a $\sigma$ = 1**, so all variables will be on a comparable scale. `scale()` does this. In standardizing the data, exclude col 86 (the qualitative `Purchase`)
```{r}
std_x <- scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(std_x[,1])
var(std_x[,2])
```

Now every col of `std_x` has a $\sigma$ = 1 + $\mu$ = 0 ==> now split observations into a test set w/ 1st 1K observations + a training set w/remaining observations + fit a KNN model on training data w/ $K = 1$, + evaluate its performance on test data.
```{r}
train_x <- std_x[-(1:1000),]
test_x <- std_x[1:1000,]
train_y <- Caravan$Purchase[-(1:1000)]
test_y <- Caravan$Purchase[1:1000]

set.seed(1)

knn_preds <- knn(train_x,test_x,train_y,k=1)

cm <- table(knn_preds,test_y) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test_y) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

KNN error rate on the 1k test observations = just under 12%. At first glance, this may appear to be fairly good. ***However, since only 6% of customers purchased insurance, we could get the error rate down to 6% by always predicting `No`, regardless of the values of the predictors.***

Suppose there is some non-trivial cost to trying to sell insurance to a given individual (ex: salesperson must visit each potential customer). If company tries to sell insurance to a random selection of customers, success rate will be only 6%, which may be far too low given costs involved. Instead, company would like to try to sell insurance only to customers likely to buy. So, *overall* error rate = not of interest +, instead, the *fraction of individuals correctly predicted to buy insurance* IS of interest (`Neg Pred Value`). Turns out $K = 1$ does far better than random guessing among customers predicted to buy insurance. Among 77 such customers, 9, or 11.7%, actually *do* purchase insurance = double the rate one would obtain from random guessing.

```{r}
train_x <- std_x[-(1:1000),]
test_x <- std_x[1:1000,]
train_y <- Caravan$Purchase[-(1:1000)]
test_y <- Caravan$Purchase[1:1000]

set.seed(1)

knn_preds <- knn(train_x,test_x,train_y,k=3)

cm <- table(knn_preds,test_y) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test_y) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

Using $K = 3$, success rate (`Neg Pred Value`) increases to 20%

```{r}
train_x <- std_x[-(1:1000),]
test_x <- std_x[1:1000,]
train_y <- Caravan$Purchase[-(1:1000)]
test_y <- Caravan$Purchase[1:1000]

set.seed(1)

knn_preds <- knn(train_x,test_x,train_y,k=5)

cm <- table(knn_preds,test_y) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(knn_preds), test_y) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

W/ $K = 5$, success rate = 26.7% => 4X the rate resulting from random guessing  (6% for always predicting `No`). It appears KNN is finding some real patterns in a difficult data set.

As a comparison, fit a logistic regression model to the data w/ 0.5 as a threshold/cut-off for predicted probability for the classifier:
```{r}
train <- Caravan[-(1:1000),]
test <- Caravan[1:1000,]

glm_fit <- glm(Purchase ~ .,train, family = "binomial")
glm_probs <- predict(glm_fit, test, type="response")

glm_preds <- rep("No",1000)
glm_preds[glm_probs > .5] <- "Yes"

cm <- table(glm_preds,test$Purchase) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(glm_preds), test$Purchase) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

We have a problem: only 7 test observations are predicted to purchase insurance. **Even worse, we are wrong about all of these!** However, we are **not required to use a cut-off of 0.5**. If we instead *predict a purchase any time predicted probability of purchase > 0.25*, we get much better results

```{r}
glm_preds <- rep("No",1000)
glm_preds[glm_probs > .25] <- "Yes"

cm <- table(glm_preds,test$Purchase) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean(glm_preds==Direction)

confusionMatrix(as.factor(glm_preds), test$Purchase) 
cat("\nTest Error Rate: ", 1 - accuracy)
```

We predict 33 people will purchase insurance + are correct for about 33% of these people = > ~ 5X better than random guessing!
