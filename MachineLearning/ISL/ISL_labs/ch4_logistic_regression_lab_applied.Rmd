---
title: "ch4_logistic_regression_lab_applied"
author: "Steve Newns"
date: "May 23, 2018"
output: html_document
---
```{r,echo=F}
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
```
## Applied
**10. This question should be answered using the Weekly data set from ISLR package = similar to `Smarket` data but contains 1089 weekly returns for 21 years, from beginning of 1990 to end of 2010**
```{r}
library(ISLR)
library(tidyverse)
library(ggplot2)
library(mosaic) # summaries + EDA
library(caret) # ML training
library(MASS) #lda/qda
library(class) #knn
library(caTools) # sample.split
library(corrplot)
library(latex2exp)
```

**(a) Produce some numerical and graphical summaries of the Weeklydata. Do there appear to be any patterns?**
```{r}
summary(Weekly)
pairs(Weekly)

Weekly[,1:8] %>%
  cor()

Weekly %>%
  mutate(rowNum = as.numeric(row.names(Weekly))) %>%
  ggplot(aes(rowNum,Volume)) + 
    geom_point() + 
    labs(x="Index",main="Volume of trades over Time")
#plot(Smarket$Volume)
```

* Looks like an exponential increase starting @ index = ~700-900, and `Year` and `Volume` look to have an exponential relationship as well.

**(b) Use the full data set to perform a logistic regression (\*\*\*MODELS PROBABILITY OF BEING A CLASS\*\*\*) with `Direction` = response + the 5 `Lag` variables + `Volume` as predictors. Use summary() to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?**
```{r}
glm_fit <- glm(Direction ~ . - Year - Today, Weekly, family="binomial")
msummary(glm_fit)
```

* It appears that `Lag2` is statistically significant (barely)

**(c) Compute the confusion matrix + overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**
```{r}
#contrasts(Weekly$Direction)
glm_probs <- predict(glm_fit,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep("Down",nrow(Weekly))   # create vector of 1089 Down elements.
glm_preds[glm_probs>.5] <- "Up" # set index of elements where p > .5 to Up
True <- Weekly$Direction

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True) 
```
```{r,echo=F}
cat("Error Rate: ", 1 - accuracy)
```

* Our logistic regression model has an accuracy of 56% with an error rate of 44%. For the days when the market *did* increase, we were right ~56.5% of the time (`Neg Pred Value`  = number of negative/`Up` predictions that were correct = 557), and for those days where the market did not increase, we were correct 53% of the time (`Pos Pred Value` = number of positive/`Down` predictions that were correct = 54/102). With a `sensitivity`/`TPR`/`recall` of 11% (proportion of actual `Direction == Down` records correctly identified as such = 54/484) and `specificity` (proportion of actual `Direction == Up` records correctly identified as such = 557/605) of 92%, this model is good at predicting when the market will go up, but not so much as to when it will go down.

**(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).**
```{r}
train <- subset(Weekly,Year%in% c(1990:2008))
test <- subset(Weekly,Year >= 2009)

glm_fit <- glm(Direction ~ Lag2, train, family="binomial")
summary(glm_fit)

glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep("Down",nrow(test))   # create vector of 1089 Down elements.
glm_preds[glm_probs>.5] <- "Up" # set index of elements where p > .5 to Up
True <- test$Direction

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True)
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* This new logistic regression model has an accuracy of 62.5% with a test error rate of 37.5%. For the days the market did increase, we were right ~62% of the time (`Neg Pred Value`), and for those days where the market did not increase, we were correct 64% of the time (`Pos Pred Value`). With a sensitivity/TPR of 21% and specificity/TNR of 92%, this model is, again, good at predicting when the market will go up, but not so much as to when it will go down.

**(e) Repeat (d) using LDA.**

* ***LDA*** = alternative and less direct approach to estimating probabilities modeled by the logistic function: $P(X) = \frac {e^{\beta_0 + \beta_1X}} {(1+e^{\beta_0 + \beta_1X})} = \frac {P(X)} {(1-P(X)} = e^{\beta_0 + \beta_1X}$ where $log(\frac {P(X)} {(1-P(X)}) = \beta_0 + \beta_1X$
  * When classes = well-separated, $\beta$ estimates for logistic regression models = surprisingly unstable + LDA doesn't suffer from this
  * If $n$ = small + the distribution of predictors $X$ is approximately normal in each class, LDA model = more stable than logistic regression model
  * popular for > 2 response classes
  * $\pi_k$ = prior for $k$th class, and $f_k(x) = P(X = x|Y = k)$ = **Density function** of $X$ for observation from $k$th class and is large if there's high probability an observation in class $k$ has $X \equiv x$ and small if very unlikely if the opposite is true
  * Via **Bayes' Theorem**, we write $f_k(x) = P(X = x|Y = k) == \frac {\pi_kf_k(x)} {\sum_{l=1}^K\pi_lf_l(x)}$
  * To get **posterior** that an observation $X = x$ belongs to the $k$th class = $p_k(X)$, plug in estimates for the prior (fraction of training observations in $k$th class) and the density function (challenging w/out assuming simple forms for the densities)
  * Assuming $f_k(x)$ = Gaussian, write $f_k(x) = \frac {1} {\sqrt{2\pi} \sigma_k} \exp(- \frac {1} {2 \sigma^2_k} (x - \mu_k)^2)$ where $\mu_k, \sigma^2_k$ = mean + variance for class $k$. 
  * Assuming shared variance across classes ($\sigma$), write $p_k(x) = \frac {\pi_k \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_k)^2)} {\sum {\pi_l \frac {1} {\sqrt{2 \pi} \sigma} \exp(- \frac {1} {2 \sigma^2} (x - \mu_l)^2) }}$
  * For whichever class this formula is largest, assign $X = x$ to that class
  * Taking the log of this formula = assigning the observation to the class for which the ***DISCRIMINANT FUNCTION*** $\delta(x) = x \frac {\mu_k} {\sigma^2} - \frac {\mu^2_k} {2\sigma^2} + log(\pi_k)$ is largest (see that's its *linear* function of $x$, hence **LDA**)
  * Even if certain of our Gaussian assumption for $X$'s distribution within each class, still must estimate parameters $\mu_1,...,\mu_k, \pi_1,...,\pi_k, \sigma^2$
  * LDA approximates the Bayes' classifier above via estimates for $\mu_k, \pi_k, \sigma^2$:
    * $\mu_k = \frac {1} {n_k} \sum_{i:y_i = k}x_i$ = average of all training observations from $k$th class
    * $\sigma_k = \frac {1} {n - K} \sum_{k=1}^K \sum_{y+i=k} (x_i-\hat{\mu_k})^2$ = weighted average of the sample variances for each of the $K$ classes
  * In absence of any additional info for class membership probabilities, LDA estimates $\pi_k$ using the proportion of the training observations that belong to the $k$th class: $\hat{\pi_k} = \frac {n_k} {n}$
  * This means LDA assigns observation X = x to the class for which $\hat{\delta(x)} = x \frac {\hat{\mu_k}} {\hat{\sigma^2}} - \frac {\hat{\mu^2_k}} {2\hat{\sigma^2}} + log(\hat{\pi_k})$ is largest
  
```{r}
(lda_fit <- lda(Direction ~ Lag2, train))
plot(lda_fit)
```

```{r}
lda_preds <- predict(lda_fit,test)
True <- test$Direction

cm <- table(lda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(lda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* This LDA model ends up being the same as our logistic regression.

**(f) Repeat (d) using QDA.**
```{r}
(qda_fit <- qda(Direction ~ Lag2, train))
```
```{r}
True <- test$Direction
qda_preds <- predict(qda_fit,test)

cm <- table(qda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(qda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```
 
* ***Our QDA model only predicts `Direction = Up`*** and has an accuracy of ~59% and a test error rate of ~41%. For the days the market did increase, we were right ~58% of the time (`Neg Pred Value`), and for those days where the market did not increase, we were never correct (`Pos Pred Value`), since the model never predicted that the market would go down.
* With a sensitivity of 0% and specificity of 100%, this model perfectly predicts when the market will go up, but always fails to correctly predict when will go down.
* So, even thought the QDA predicted "Up" every day, it was still accurate > 58% of the time, which is close the logistic regression and LDA

**(g) Repeat (d) using KNN with K = 1.**
```{r, eval=F}
train_x_k <- as.matrix(train$Lag2)
test_x_k <- as.matrix(test$Lag2)
train_labels <- train$Direction

set.seed(1)

knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 1) # cl = class labels

True <- test$Direction

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Our KNN model with $K = 1$ has an accuracy of ~50% with a training error rate of 50%. For the days the market did increase, we were right ~58% of the time (`Neg Pred Value`), and for those days where the market did not increase, we correct ~41% of the time (`Pos Pred Value`). With a sensitivity of ~49% and specificity of ~51%, this model performs equally well at predicting when the market will go up or down.

**(h) Which of these methods appears to provide the best results on this data?**

* We have accuracies of 62.5% for our logistic regression and our LDA models, and an accuracy of ~59% with our QDA model, and finally an accuracy of 50% with our KNN model, suggesting that either logistic regression or LDA would be the prefered model choice if a minimized test error rate was desirable.

**(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.**

**11. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.**

**(a) Create a binary variable, `mpg01`, that contains a `1` if `mpg` contains a value above its median, and a `0` if `mpg` contains a value below its median. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.**
```{r}
data("Auto")
glimpse(Auto)
```
```{r}
(median_mpg <- median(Auto$mpg))
Auto <- Auto %>%
  mutate(mpg01 = as.factor(case_when(mpg > median_mpg ~ 1,
                           mpg < median_mpg ~ 0)))
glimpse(Auto)
```

**(b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**
```{r}
numerics <- Auto %>%
  select_if(is.numeric)

corrplot(cor(numerics, use="pairwise.complete.obs"), method = "number", type = "lower")
```

* We're lookign for variables such that `mpg` would increase/decrease when such variables' values change
* `weight`, `displacement`, `cylinders`, `horsepower` all seem to have a negative association with `mpg`, suggesting a relationhip with `mpg01`, as this was derived from `mpg`
```{r}
bwplot(weight~mpg01,Auto)
bwplot(displacement~mpg01,Auto)
bwplot(cylinders~mpg01,Auto)
bwplot(horsepower~mpg01,Auto)
```

* It indeed appears that those vehicles with lower values for `weight`, `displacement`, `cylinders`, and `horsepower` are all vehicles with `mpg` values greater than the median `mpg` value?

**(c) Split the data into a training set and a test set.**
```{r}
set.seed(1)
spl <- sample.split(Auto$mpg01, SplitRatio = 0.7) # 70% of mpg01 = 1 in both sets
train <- subset(Auto, spl==T)
test <- subset(Auto, spl==F)
```

**(d) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**
```{r}
(lda_fit <- lda(mpg01 ~ weight + displacement + cylinders + horsepower, train))
plot(lda_fit)
```

```{r}
lda_preds <- predict(lda_fit,test)
True <- test$mpg01

cm <- table(lda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(lda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Our LDA model had a very high accuracy of ~90% and a test error rate of ~10%. For the vehicles with above average MPG, we were right ~89% of the time (`Neg Pred Value`), and for the vehicles with below average MPG, we correct ~91% of the time (`Pos Pred Value`). With a sensitivity/TPR of 88% and specificity/TNR of ~92%, this model performs equally well at predicting vehicles with above or below average MPG values.

**(e) Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**
```{r}
(qda_fit <- qda(mpg01 ~ weight + displacement + cylinders + horsepower, train))

True <- test$mpg01

qda_preds <- predict(qda_fit, test)

cm <- table(qda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(qda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* The QDA model has improved upon the LDA model, with higher accuracy of ~91% and a lower test error rate of ~9.3%. For the vehicles with above average MPG, we were right ~93% of the time (`Neg Pred Value`), and for the vehicles with below average MPG, we correct ~89% of the time (`Pos Pred Value`). With a sensitivity/TPR of 93% and specificity/TNR of ~88%, this model performs equally well at predicting vehicles with above or below average MPG values, but is a bit better at classifying vehicles with below average MPG and a bit worse at classifying vehicles with above average MPG compared to LDA.

**(f) Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**
```{r}
glm_fit <- glm(mpg01 ~ weight + displacement + cylinders + horsepower, train, family="binomial")
msummary(glm_fit)
```

* It appears that logistic regression only blieves that `weight` is significant in predicting whether or not a vehicle has an above aveerage `mpg` value.

```{r}
glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep(0,nrow(test))   # create vector of 0 elements.
glm_preds[glm_probs>.5] <- 1 # set index of elements where p > .5 to 1
True <- test$mpg01

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Logistic regression performs a bit worse than LDA and QDA. It's similar to QDA in its ability to classify vehicles with below average MPG and but worse at classifying vehicles with above average MPG compared to LDA (sensitivity/TPR = 88%, sensitivity/TNR = ~84%)
* Try removing the predictors with largest $p$-values
```{r}
glm_fit <- glm(mpg01 ~ weight + cylinders + horsepower, train, family="binomial")
msummary(glm_fit)
```
```{r}
glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep(0,nrow(test))   # create vector of 0 elements.
glm_preds[glm_probs>.5] <- 1 # set index of elements where p > .5 to 1
True <- test$mpg01

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True) 
```

* The model has gotten worse.

```{r}
glm_fit <- glm(mpg01 ~ weight + horsepower, train, family="binomial")
msummary(glm_fit)

glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep(0,nrow(test))   # create vector of 0 elements.
glm_preds[glm_probs>.5] <- 1 # set index of elements where p > .5 to 1
True <- test$mpg01

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True) 
```

* Now, it's gotten *better*, due to the multicollinearity present among these predictors (see above correlation matrix)

```{r}
glm_fit <- glm(mpg01 ~ weight, train, family="binomial")
msummary(glm_fit)

glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep(0,nrow(test))   # create vector of 0 elements.
glm_preds[glm_probs>.5] <- 1 # set index of elements where p > .5 to 1
True <- test$mpg01

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True) 
```

**(g) Perform KNN on the training data, with several values of K, in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?**
```{r, eval=F}
train_x_k <- as.matrix(train[,c('weight','displacement','cylinders','horsepower')])
test_x_k <- as.matrix(test[,c('weight','displacement','cylinders','horsepower')])
train_labels <- train$mpg01

set.seed(1)

knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 1) # cl = class labels

True <- test$mpg01

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* KNN with $K=1$ was similary to LDA, with accuracy of ~87% and a test error rate of ~12.7%. For the vehicles with above average MPG, we were right ~92% of the time (`Neg Pred Value`), and for the vehicles with below average MPG, we correct ~83% of the time (`Pos Pred Value`). With a sensitivity/TPR of 93% and specificity/TNR of ~81%, this model was better at classifying vehicles with below average MPG

```{r}
knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 3) # cl = class labels

True <- test$mpg01

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

```{r}
knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 5) # cl = class labels

True <- test$mpg01

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Increasing $K$ makes the model worse

**12. This problem involves writing functions.**

**(a) Write a function, `Power()`, that prints out the result of raising 2 to the 3rd power. In other words, your function should compute $2^3$ and print out the results.**

* **Hint: Recall that `x^a` raises `x` to the power `a`. Use the print() function to output the result.**
```{r}
Power <- function(x) {
  print(x**3)
}
Power(2)
```

**(b) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line `Power2 = function (x, a) {`. You should be able to call your function by entering, for instance, `Power2(3, 8)`** on the command line. This should output the value of $3^8$, namely, `6561`.**
```{r}
Power2 <- function(x,a) {
  print(x**a)
}
Power2(3,8)
```

**(c) Using the Power2() function that you just wrote, compute $10^3$, $8^17$, and $131^3$.**
```{r}
cat(Power2(10,3),Power2(8,17),Power2(131,3))
```

**(d) Now create a new function, `Power3()`, that actually returns the result $x^a$ as an R object, rather than simply printing it to the screen. That is, if you store the value $x^a$ in an object called result within your function, then you can simply return() this return() result, using the following line: `return(result)`. The line above should be the last line in your function, before the `}` symbol.**
```{r}
Power3 <- function(x,a) {
  result <- x**a
  return(result)
}
print(Power3(3,8))
```

**(e) Now using the `Power3()` function, create a plot of f(x) = $x^2$. The x-axis should display a range of integers from 1 to 10, and the y-axis should display x2. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale.**
```{r}
df <- data.frame(x = seq(1,10), y = Power3(x,2))
ggplot(df,aes(x,y)) + geom_point() + labs(x='x',y=TeX('$x^2$'))
ggplot(df,aes(log(x),y)) + geom_point() + labs(x=TeX('$log(x)$'),y=TeX('$x^2$'))
ggplot(df,aes(x,log(y))) + geom_point() + labs(x=TeX('x'),y=TeX('$log(x^2)$'))
ggplot(df,aes(log(x),log(y))) + geom_point() + labs(x=TeX('$log(x)$'),y=TeX('$log(x^2)$'))
```

**(f) Create a function, PlotPower(), that allows you to create a plot of $x$ against $x^a$ for a fixed $a$ and for a range of values of $x$. For instance, if you call `PlotPower(1:10,3)` then a plot should be created with an x-axis taking on values 1, 2,..., 10, and a y-axis taking on values 13, 23,..., 103.**
```{r}
PlotPower <- function(x,a) {
  df <- data.frame(xs = x, ys = x^a)
  ggplot(df,aes(x,y)) + geom_point() + labs(x=TeX('$x$'),y='x^a')
}
print(PlotPower(1:10,3))
```

**13. Using the `Boston` data set, fit classification models in order to predict whether a given suburb has a crime rate above or below the median. Explore logistic regression, LDA, and KNN models using various subsets of the predictors. Describe your findings.**
```{r}
data(Boston)

med_crim <- median(Boston$crim)

Boston <- Boston %>%
  mutate(above_avg_crim = as.factor(case_when(crim > med_crim ~ 1,
                                    crim < med_crim ~ 0)),
         chas = as.factor(chas),
         rad = as.factor(rad))

set.seed(1)
spl <- sample.split(Boston$above_avg_crim, SplitRatio = 0.7) # 70% of mpg01 = 1 in both sets
train <- subset(Boston, spl==T)
test <- subset(Boston, spl==F)

glm_fit <- glm(above_avg_crim ~ . - crim - rad - lstat - rm, train, family="binomial")
msummary(glm_fit)

glm_probs <- predict(glm_fit,test,type="response") # no newdata arg =  predict probability on training data used to fit model

glm_preds <- rep(0,nrow(test))   # create vector of 0 elements.
glm_preds[glm_probs>.5] <- 1 # set index of elements where p > .5 to 1
True <- test$above_avg_crim

cm <- table(glm_preds,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(glm_preds),True)
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Our logistic regression model has an accuracy of 86% with an error rate of ~14%. For neighborhoods with below average crime rates, we were right ~89% of the time (`Neg Pred Value`  = number of below average predictions that were correct), and for neighborhoods with above average crime, we were correct ~84%% of the time (`Pos Pred Value` = number of above average crime rates predictions that were correct). 
* With a `sensitivity`/`TPR`/`recall` of ~89.5% (proportion of actual below average crime rates correctly identified as such) and `specificity`/`TNR` of ~82.9% (proportion of actual above average crime rates correctly identified as such), this model is good at predicting both neighborhoods with above and below average crime rates

```{r}
(lda_fit <- lda(above_avg_crim ~ . - crim - rad - lstat - rm, train))
plot(lda_fit)
```

```{r}
lda_preds <- predict(lda_fit,test)

cm <- table(lda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(lda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Our LDA model has a lower accuracy of 82% with an error rate of ~17.7%. For neighborhoods with below average crime rates, we were right ~86.5% of the time (`Neg Pred Value` = number of below average predictions that were correct), and for neighborhoods with above average crime, we were correct ~78.8% of the time (`Pos Pred Value` = number of above average crime rates predictions that were correct). 
* With a `sensitivity`/`TPR`/`recall` of ~88% (proportion of actual below average crime rates correctly identified as such) and `specificity`/`TNR` of ~76.3% (proportion of actual above average crime rates correctly identified as such), this LDA model is comparable to the logistic regression model at classifying neighborhoods with below average crime rates but is worse at classifying neighborhoods with above average crime rates

```{r}
(qda_fit <- qda(above_avg_crim ~ . - crim - rad - lstat - rm, train))

qda_preds <- predict(qda_fit, test)

cm <- table(qda_preds$class,True) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(qda_preds$class),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* Our QDA model has an accuracy of 84% with an error rate of ~15.7%. For neighborhoods with below average crime rates, we were right ~89.3% of the time (`Neg Pred Value` = number of below average predictions that were correct), and for neighborhoods with above average crime, we were correct ~80% of the time (`Pos Pred Value` = number of above average crime rates predictions that were correct). 
* With a `sensitivity`/`TPR`/`recall` of ~90.8% (proportion of actual below average crime rates correctly identified as such) and `specificity`/`TNR` of ~77.6% (proportion of actual above average crime rates correctly identified as such), this QDA model is comparable to the LDA model, but a bit better

```{r, eval=F}
train_x_k <- as.matrix(train[,c('zn','indus','chas','nox','age','dis','tax','ptratio','black','medv')])
test_x_k <- as.matrix(test[,c('zn', 'indus','chas','nox','age','dis','tax','ptratio','black','medv')])
train_labels <- train$above_avg_crim

set.seed(1)

knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 1) # cl = class labels

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

```{r}
knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 3) # cl = class labels

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

```{r}
knn_preds <- knn(train = train_x_k, test = test_x_k, cl = train_labels, k = 5) # cl = class labels

(cm <- table(knn_preds,True)) # confusion matrix
accuracy <- (cm[4]+cm[1]) / sum(cm) # TP + TN / n OR  mean

confusionMatrix(as.factor(knn_preds),True) 
```
```{r,echo=F}
cat("Test Error Rate: ", 1 - accuracy)
```

* It appears that KNN with an increasing of $K$ to 5 is out best bet for the classifier for above or below average neighborhood crime rates