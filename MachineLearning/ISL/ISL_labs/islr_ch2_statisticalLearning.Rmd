---
title: "ISLR_Lab2"
output: html_document
---

## Basic R

```{r, create and save a pdf, message=F,eval=F}
x <- rnorm(100)
y <- rnorm(100)

# create and save pdf of plot
pdf("Figure1_scatter.pdf")
plot(x,y)
dev.off() # signifies done plotting
```

**Contour plots** represent 3D data, like a topographical map

```{r create contour plot, message=F}
x <- seq(-pi,pi,length=50)
y <- x

# outer product of arrays = outer(vector1,vector2,function to perform on outer products)
f <- outer(x,y, function(x,y) cos(y)/(1*x^2))

# plot contour(x,y,z)
contour(x,y,f)
```

```{r variations of contour plots}
par(mfrow=c(1,2))
plot.new()
contour(x,y,f, nlevels = 45, add = T)

#plot.new()
f2 <- (f-t(f))/2
contour(x,y,f2, nlevels = 15)
```

Can also develop a similar plot, a **heatmap**,  with color-coded values for `z`.

```{r}
image(x,y,f2)
```

Can also create 3D plots with `persp()`, via its arguments `theta`, `phi`, which control angles at which plot is viewed
```{r}
par(mfrow=c(2,3))
persp(x,y,f2)
persp(x,y,f2,theta=30)
persp(x,y,f2,theta=30,phi=20)
persp(x,y,f2,theta=30,phi=70)
persp(x,y,f2,theta=30,phi=40)
```

Here's some more plots
```{r, message=F, warning=F}
library(ISLR)
# fix(Auto) # view data in another window as spreadsheet

par(mfrow=c(1,3))
attach(Auto) # don't have to use '$' operator for accessing cols
cylinders <- as.factor(cylinders)
plot(cylinders, mpg, col="red", xlab="cylinders", ylab="mpg")
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders", ylab="mpg")
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders", ylab="mpg", horizontal=T) # flip previous plot
```

```{r}
hist(mpg,col=2,breaks=15) # col=2 same as red
```

```{r}
# create scatterplot matrix
pairs(Auto)
```

```{r}
# create smaller scatterplot matrix
pairs(~ mpg+displacement+horsepower+weight+acceleration, Auto)
```

`identify(x,y,variable)` provides useful interactive method for IDing values for a particular variable for on a plot. Clicking on a given point in the plot causes R to print the value of the variable of interest. Right-clicking on the plot will exit `identify()` function (control-click on a Mac). The numbers printed under `identify()` correspond to the rows for the selected points
```{r}
plot(horsepower,mpg)
identify(horsepower,mpg,name)
```

### 2.4 Exercises
####Conceptual

1. Below, indicate whether we'd generally expect performance of a *flexible* statistical learning method to be better or worse than an inflexible method. Justify your answer.
  a. **If sample size $n$ is extremely large, + number of predictors $p$ is small**
    * Expect flexible learning methods to be *better w/ a larger sample size $n$* = *more DPs use in training* + method *will fit the data closer*. 
  b.  **If number of predictors $p$ is extremely large, + number of observations $n$ is small**
    * Expect a flexible learning method to be *worse with a large amount of predictors $p$* == tend to *overfit to the small number of observations* in *$n$*. 
  c. **If relationship between predictors + response = highly non-linear.**
    * Expect a more flexible method to *better fit the non-linear true structure of f*, as we'd have *more degrees of freedom w/ a more flexible method*.  
  d. **If the variance of the error terms, ($\sigma^2 = Var(\epsilon)$), is extremely high**
    * Expect a more flexible approach to *do worse* == would *tend to take into account the **noise** in the variable errors* that is *signaled via a higher variance* 

2. Explain whether each scenario is a classification or regression, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$. 

  a. **We collect a set of data on the top 500 firms in the US. For each firm we record profit, # of employees, industry, + CEO salary. We're interested in understanding which factors affect CEO salary** 
    * This is a regression, and are more interested in inference 
  b. **We are considering launching a new product + wish to know whether it will be a success or failure. We collect data on 20 similar products that were previously launched. For each, we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, + 10 other variables.**
    * This is a classification, and we are interested in prediction. 
  c. **We are interested in predicting % change in the USD/Euro. We collect weekly data for all of 2012 + for each week, record % change in the USD/Euro, % change in the US market, % change in the British market, + % change in the German market**
    * This is a regression and we're interested in prediction 

3. We now revisit the bias-variance decomposition. Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represented the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one
```{r}
plot.new()
```
  
  a. **Explain why each of the five curves has the shape displayed in part (a)**
    
    * **Squared Bias** (error introduced by approximating a real-life problem) = decreases as flexibility increases b/c we're now "less biased to our model"
    * **Variance** (amount by which estimated function $\hat{f}$ would change if estimated w/ different training set) = increases as flexibility increases b/c we're now more sensitive to different varieties of DPs (should not vary too much between training sets, and having high variance = changing any DP may cause $\hat{f}$ to change considerably)
      * ***NOTE***: *Relative rate of change* of bias + variance determines whether test MSE increases or decreases 
      * increasing flexibility of a class of methods == bias tends to initially decrease faster than variance increases + consequently, expected test MSE declines
      * However, at some point, increasing flexibility has little impact on bias but starts to significantly increase variance + therefore test MSE increases
    * **Bayes/Irreducible Error, $Var(\epsilon)$** = horizontal dashed line representing lowest achievable test MSE among all possible methods 
    * **Test Error** = initially decrease, hits minimum at the best match of low variance + low bias, starts to increase again as increased flexibility no longer affects bias but causes increases in variance + we start to overfit (always a U-shape) 
    * **Training Error** = Decreases, plateaus, start to decrease again as we overfit 
      * always expect `trainMSE < testMSE` since **statistical learning methods aim to reduce trainMSE**
    * **overfitting** = large testMSE + low trainMSE resulting from a method that's finding patterns in *noise of training set* that are NOT present in true properties of the test set/unknown $f$

4. You will now think of some real-life applications for statistical learning. 
  a. **Describe 3 real-life applications in which classification might be useful. Describe the response + predictors. Is the goal of each application inference or prediction? Explain your answer** 
    * predicting churn or not with predictors = balance, account age, # of cards, credit limit, student status, etc. w/ goal of prediction 
    * determing which factors affect churn probability with predictors = balance, account age, # of cards, credit limit, student status, etc. w/ goal of inference 
    * predicting which factors affect playoff chances of a hockey team with predictors = wins, goals, shooting%, save%, shot chances, quality of competition, etc. w/ goal of inference 
  b. **Describe 3 real-life applications in which regression might be useful. Describe the response + predictors. Is the goal of each application inference or prediction? Explain your answer.** 
    * predicting future wins of a hockey team using predictors = wins, losses, corsi, goals, shooting%, save%, shot chances, quality of competition, etc. from past seasons w/ goal of prediction  
    * determing which factors affect wins of a hockey team using predictors = wins, losses, corsi, goals, shooting%, save%, shot chances, quality of competition, etc. from past seasons w/ goal of inference 
    * predicting profit with predictors = # of products, sales, costs, past profits, budgets, stores, dates, etc. w/ goal of prediction 
  c. **Describe three real-life applications in which cluster analysis might be useful.** 
    * market-basket analyis/customer segmentation for product recommendations 
    * determining different groups of customers for a different styles of products from a company 
    * Finding similar pictures/videos of different subject matters (dogs, cars, trees, cats, etc.) 

5. **What are the advantages and disadvantages of a very flexible (vs. a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?**
  * advantages of very flexible method = decreased bias, possible better fit to non-linear models, *preferable for prediction vs. interpretability* (less flexible == caring about inference) 
  * linear models = non-flexible = relatively simple + interpretable inference, but may not yield as accurate predictions as some other approaches 
  * highly non-linear (flexible) approaches can potentially provide quite accurate predictions for $Y$ but @ expense of a less          interpretable model 
  * disadvantages of very flexible method = increased variance, possible worse fit to linear models, requires greater # of DPs to find true patterns b/c too many $p$ relative to $n$ = starts to model noise (overfit) 
  </ul>
  
6. **Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?** 
  * **Parametric** = **makes assumptions** about the underlying distribution of the data + reduces problem of estimating $f$ down to one of **estimating a set of parameters** 
    * 2-step model-based approach: 
        1. make assumption about functional form/shape of $f$ (i.e. "$f$ is linear"") 
        2. Select a model/method to estimate parameters + fit the model to estimate chosen model's parameters 
    * If assuming a linear relationship = only need to estimate *coefficients* of predictors (i.e. parameters = vector of $\beta_p$)
    * Generally much easier to estimate a set of parameters than to fit an entirely arbitrary function $f$ 
    * Potential disadvantage = model we choose will usually not match true unknown form of $f$.
    * If too far from true $f$, estimate will be poo
      * can try to address this by choosing flexible models that can fit many different possible functional forms for $f$
      * but in general, more flexible models require estimating greater # of parameters + these more complex models can lead overfitting (follow errors/noise, too closely)
  * **Non-Parametric** = **makes no/little assumptions** about the underlying distribution of the data/does not make explicit assumptions about functional form of $f$
    * Instead seek an estimate of $f$ with a $hat{f}$ that gets as close to the DP's as possible w/out being too rough/wiggly 
    * Advantage: by avoiding assumptions of functional forms for $f$ = have potential to accurately fit wider range of possible shapes for $f$ 
    * A major disadvantage =  does not reduce problem of estimating $f$ to a small # of parameters, so a very large # of observations (far more typically needed for a parametric approach) is required to obtain accurate $f$ estimate 
    * Can still overfit 

7. **Table below provides a training data set containing 6 observations, 3 predictors, and one qualitative response variable.** 
```{r,echo=F}
x1 <- c(0,-2,0,0,-1,1)
x2 <- c(3,0,1,1,0,1)
x3 <- c(0,0,3,2,1,1)
Y <- c("R","R","R","G","G","R")
(df <- data.frame(x1,x2,x3,Y))
```

**Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors. Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0.** 

```{r}
euc.dist <- function(x1, x2, x3) {
  sqrt(sum((x1-0)**2 + (x2-0)**2 + (x3-0)**2))
}
c(euc.dist(df[1,1],df[1,2],df[1,3])
,euc.dist(df[2,1],df[2,2],df[2,3])
,euc.dist(df[3,1],df[3,2],df[3,3])
,euc.dist(df[4,1],df[4,2],df[4,3])
,euc.dist(df[5,1],df[5,2],df[5,3])
,euc.dist(df[6,1],df[6,2],df[6,3]))
```
  a. **What is our prediction with $K$ = 1? Why?** = $K$ = 1 gives a prediction of `G`, as this has the lowest Euclidean distance (closest point) to the test point of 0 
  b. **What is our prediction with $K$ = 3? Why?** = $K$ = 3 gives a prediction of `R`, as the lowest Euclidean distances comes from `G`, `R`, `R`, so the majority of neighbors are in the class `R`, and that is the class to which we assign the test point. 
  c. **If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for $K$ to be large or small? Why?** = We'd expect a lower $K$ since this provides a more flexible decision boundary, since as $K$ grows, the decision boundary becomes less flexible/more linear (**level of flexibility = inversely proportional to $K$ in KNN**, + to fit non-linear decision boundaries, we need a more-flexible approach)  

#### Applied
8. This exercise relates to the `College` data set from the `ISLR` package, which contains a # of variables for 777 different universities + colleges in the US. The variables are: 
  * **Private** : Public/private indicator 
  * **Apps** : Number of applications received 
  * **Accept** : Number of applicants accepted 
  * **Enroll** : Number of new students enrolled 
  * **Top10perc** : New students from top 10 % of high school class 
  * **Top25perc** : New students from top 25 % of high school class 
  * **F.Undergrad** : Number of full-time undergraduates 
  * **P.Undergrad** : Number of part-time undergraduates 
  * **Outstate** : Out-of-state tuition 
  * **Room.Board** : Room and board costs 
  * **Books** : Estimated book costs 
  * **Personal** : Estimated personal spending 
  * **PhD** : Percent of faculty with Ph.D's 
  * **Terminal** : Percent of faculty with terminal degree 
  * **S.F.Ratio** : Student/faculty ratio 
  * **perc.alumni** : Percent of alumni who donate 
  * **Expend** : Instructional expenditure per student 
  * **Grad.Rate** : Graduation rate 
```{r}
head(College) # from ISLR package
```

Notice the 1st column = name of each university. We don't really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:
```{r}
# rownames(College) <- College [,1] # use 1st col as rownames
#^^^^^^ALREADY DONE VIA PACKAGE ISLR
head(College)
```

R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the 1st column in the data where the names are stored 
```{r}
#college =college [,-1]
#head(college)
```
Now 1st data column should be `Private` 

Use `summary()` to produce a numerical summary of the variables in the data set, `pairs()` to produce a scatterplot matrix of the 1st 10 variables of the data, `plot()` to produce side-by-side boxplots of `Outstate` v. `Private`, create a new qualitative variable, `Elite`, by binning `Top10perc`, + divide universities into 2 groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50%. 
```{r, warning=F,message=F}
library(tidyverse)

# scatterplot matrix of numerical variables
pairs(College[,1:10])

# compare boxplots
plot(Outstate ~ Private, data = College, col=c("Red","Blue"),
     main="Out-of-State Tuition by Private Status")

# new qualitative variable 'Elite' = divides colleges into elite v. non-elite
College <- College %>% 
  mutate(Elite = ifelse(Top10perc>50,"Yes","No"),
         Elite = factor(Elite))

summary(College)
```

We can see what look like some positive linear relationships in that we can look further into.
```{r,warning=F,message=F}
library(corrplot)

college_numeric <- College %>%
  dplyr::select(-Private,-Elite)
cor_mtrx <- round(cor(college_numeric),3)
corrplot(cor_mtrx,method="number")
#corrplot(cor_mtrx) = gives circles, not #'s of r values
```

* So, `Apps`, `Accept`, `Enroll`, and `F.undegrad` are all highly positively correlated, and so are `Top10perc` and `Top25perc`. `Top10perc` is also moderately correlated with `Expence`, which itself is moderately correlated with `Outstate`, who has a moderately negative correlation with `S.F.Ratio`. There are no strong negative associations.
* We also see that it is quite clear that out of state tuition is higher for Private schools (and both distributions do not appear to have a skew), that we have 78 Elite schools and 699 non-Elite schools.

Use `summary()` to see how many elite universities there are + `plot()` to produce side-by-side boxplots of Outstate versus Elite. 
```{r}
summary(College$Elite)
plot(Outstate ~ Elite, data = College, col=c("Red","Blue"),
     main="Out-of-State Tuition by Elite Schools")
```

* Same as with out of state tuition for Private schools, out of state tuition is higher for Elite schools compared to non-elite schools, with Elite schools skewing towards the higher end of tuition, and non-elite *slightly* skewing towads lower tuition.

Use `hist()` to produce some histograms w/ differing #'s of bins for a few of the quantitative variables. You may find the command `par(mfrow=c(2,2))` useful (divides print window into 4 regions for 4 simultaneous plots) 
```{r}
par(mfrow=c(2,2))
hist(College$Books, col="red",main="Distribution of Estimated Books Cost",
     xlab="Estimated Books Cost ($)")
hist((College$Accept/College$Apps)*100, col="blue",main="Distribution of Acceptance Rate",
     xlab="Acceptance Rate (%)")
hist(College$Personal, col="orange",main="Distribution of Est. Personal Spending",
     xlab="Estimated Personal Spending ($)")
hist(College$Grad.Rate, col="brown",main="Distribution of Graduation Rate",
     xlab="Graduation Rate (%)", xlim = c(0,100))
```

* It appears the distribution of estimated book costs is quite normal, but with some outliers pulling the right tail outward, acceptance rate is left-skewed, indicating a larger number of higher acceptance rates, estimated personal spending is right-skewed, with some outliers estimated to spend much more than the general population, and finally, graduation rate is quite normal.

Continue exploring the data, and provide a brief summary of what you discover. 
```{r}
par(mfrow=c(2,2))
plot(PhD ~ Elite, data = College, col=c("Red","Blue"),
     main="Percentage of Faculty with Ph.D's by School Status")
plot(College$Books, College$Personal, col="green",main="Esimated Spending by Estimated Book Costs")
plot(Accept/Apps ~ Elite, data = College, col=c("Red","Blue"),
     main="Acceptance Rate by Elite Status")
plot(Accept/Apps ~ Private, data = College, col=c("Red","Blue"),
     main="Acceptance Rate by Private Status")
```

* We see its more typical for Elite schools to have PhD's as faculty, and to have a lower acceptance rate, albeit with greater variance than non-elite schools. We ccan also see that acceptance rate for Private Schools is comparable to public schools

```{r}
ggplot(College,aes(Elite, Accept/Apps)) + 
  geom_boxplot() + facet_grid(~ Private)
```

Looking more into the acceptance rate by school statuses (elite and/or private), it appears that both private elite and non-private elite have lower acceptance rates.

9. This exercise involves the `Auto` data set (Make sure missing values have been removed from the data)
```{r}
summary(Auto)
```
  
  a. **Which of the predictors are quantitative, and which are qualitative?**
  * quant = `{mpg, cylinders, displacement, horsepower, acceleration}`
  * qual = `{year, origin, name}`
    * `origin` = American, European, Japanese
  b. What is the range, mean + standard deviation of each *quantitative* predictor? 
```{r}
Auto %>% dplyr::select(-year,-origin,-name) %>% 
  gather() %>% 
  group_by(key) %>%
  summarise(mean = mean(value), sd = sd(value), range = max(value)-min(value))
```
  
  c. Now remove the 10th through 85th observations. What is the range, mean, + SD of each predictor in the subset of the data that remains? 
```{r}
Auto[-c(11:85),] %>% dplyr::select(-year,-origin,-name) %>% 
  gather() %>% 
  group_by(key) %>%
  summarise(mean = mean(value), sd = sd(value), range = max(value)-min(value))
```

  d. Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. 
```{r}
pairs(Auto[,1:6])

auto_numeric <- Auto %>% dplyr::select(-year,-origin,-name)
cor_mtrx <- round(cor(auto_numeric),3)
corrplot(cor_mtrx,method="number")
```
  
  * There's a strong negative relationship between `cylinders` and `mpg` indicating an increase in either number of cylinders or MPG would lead to a decrease in the other (probably increasing # of cylinders decreases MPG). 
  * There is a very strong positive correlation between `cylinders` and `displacement`, between `horsepower` and `displacement` and between `weight` and `displacement`, while `displacement` is strongly negatively associated with `mpg` and moderately so with `acceleration`. 
  * `acceleration` has the weakest associations with the other variables
  * No factors increase `mpg` except `acceleration`, though this relationship is not strong

```{r}
ggplot(Auto,aes(as.factor(cylinders), mpg)) + 
  geom_boxplot(varwidth = T)
ggplot(Auto,aes(as.factor(cylinders), mpg)) + 
  geom_boxplot(varwidth = T) + facet_grid(~ origin)
```

  * Generally, we see an increase in `mpg` for going form 3 cylinders to 4 and 5, with 3 and 5 cylinders also being the least represented (skinner bars)
  * But, we can see that for American and European cars (`origin` = 1 or 2), as we increase `cylinders`, our `mpg` does down
  * Meanwhile, Japan sees an increase in `mpg` for 4 cylinders vs. 3 cylinders, and hten decreases again for 6 cylinders
  * Only Japan has 3 cylinder cars and only Europe has 5 cylinder cars

  c. Suppose we wish to predict gas mileage on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer == all, but acceleration is pretty weak relative to others 
```{r}
corrplot(cor_mtrx,method="number")
```

  * It looks like `weight` and `displacement` are best, followed by `horsepower` and `cylinders`

```{r}
pairs(Auto[,c("mpg","cylinders","horsepower","displacement","weight")])
```
  
10. This exercise involves the Boston housing data set, part of the MASS library in R. 
a. How many rows are in this data set? How many columns? What do the rows and columns represent? 
```{r}
library(MASS)
glimpse(Boston) # Housing Values in Suburbs of Boston
# ?Boston # = read about dataset
```

  * Each of the 506 rows = suburb/town in Boston, with 14 predictors/variables of per capita crime rate, proportions of residential land zoned for lots > 25k sqft., blacks per town, non-retail business acres per town, + owner-occupied units build since '40, NO2 concentration, avg # rooms per dwelling, weighted mean distancse to 5 Boston employment centers, index of radial highway accessbility, property-tax rate pe r$10k, pupil/teach ratio, median owner-occupied home values (\$1k's), and a dummy variable for if tract bounds Charles River 

b. Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings. 
```{r}
pairs(Boston[,-c(4,9)])
```

  *  Difficult to see at a glance

```{R}
boston_numeric <- Boston %>% dplyr::select(-chas,-rad)
cor_mtrx <- round(cor(boston_numeric),3)
corrplot(cor_mtrx,method="number")
```
  
c. Are any of the predictors associated with per capita crime rate? If so, explain the relationship. 
  * Appears that `indus`,`nox`,`tax`, and `lstat` have a moderately positive correlations with `crim`. And `dis`, `black`, `medv`, and `zn` have negative correlations, to note, but they are weaker

d. Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor. 
```{r}
library(ggplot2)
par(mfrow=c(1,3))
boxplot(Boston$crim, ylab="Crime Rate")
boxplot(Boston$tax, ylab="Tax rate")
boxplot(Boston$ptratio, ylab="Pupil-teacher ratio")
```  
  
  * Looks like a lot of outliers for crime rate, none for Tax rate, some quite low values for Pupil-Teacher ratio

```{r}
Boston %>% filter(crim >= quantile(crim,.75)+(IQR(crim)*1.5)) %>% summarise(highcrim = n())
Boston %>% filter(ptratio <= quantile(ptratio,.25)-(IQR(ptratio)*1.5)) %>% summarise(ptratio = n())
```

```{r}
Boston %>% dplyr::select(crim,tax,ptratio) %>% 
  gather() %>% 
  group_by(key) %>%
  summarise(range = max(value)-min(value),iqr = IQR(value))
```

  * The outliers in `crim` cause a very large range, relative to its IQR, while `tax` and `ptratio` aren't so bad, relatively speaking

e. How many of the suburbs in this data set bound the Charles river? 
```{r}
Boston %>% 
  filter(chas==1) %>% 
  summarise(charles = n())
```

f. What is the median pupil-teacher ratio among the towns in this data set? 
```{r}
median(Boston$ptratio)
hist(Boston$ptratio) # makes sense since skewed
```
  
g. Which suburb of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors? Comment on your findings. 
```{r}
Boston[which.min(Boston$medv),]
summary(Boston)
```

 * slightly below average `rm`, greater than average `crim`, `indus`, `nox`, `dis`, `tax`, `ptratio`, `lstat`, average `zn`, the max `age`, `rad`, `black`

h. In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling 
```{r}
Boston %>%
  filter(rm > 7) %>%
  summarise(count = n())
Boston %>%
  filter(rm > 8) %>%
  summarise(count = n())
summary(Boston)
Boston %>%
  filter(rm > 8) %>%
  summary()
```

  * Suburbs that average more than eight rooms per dwelling tend to have less crime, less non-retail business acres, less houses bound by the Charles, are *barely* older, a *slightly* larger weighed mean of distance to employment centres, lower index of accessibility to radial highways, higher property tax rate, slightly greater pupil-teacher ratio, much lower value for lower status % of the population, and lower median value of the homes
  * I'm assuming these are suburbs where families live, a bit further away from city center to raise such families in neighborhoods