---
title: "ISLR_Ch3_LinearRegression_exercises_conceptual"
author: "Steve Newns"
date: "April 20, 2018"
output: html_document
---
```{r,message=F,warning=F}
library(tidyverse)
```
## 3.7 Exercises
### Conceptual
**1. Describe the null hypotheses to which the $p$-values given in Table 3.4 (hyperplane) correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.**

* $H_{01}$ : There is no relationship between TV and Sales ($\beta_1$ = 0)
* $H_{02}$ : There is no relationship between Radio and Sales ($\beta_2$ = 0)
* $H_{03}$ : There is no relationship between Newspaper and Sales ($\beta_3$ = 0)
* If our **p-values** (*probability of observing any value >= $|t|$ in absolute value, assuming $\beta_x$ = 0*) for any of these predictors are below our threshold (here, it's .05), then we have the confidence to reject the null hypothesis $H_0$ that there is no relationship between the specified predictor and `Sales` such that the values we've observed are not due to chance.

**2. Carefully explain the differences between the KNN classifier and KNN regression methods.**

* **KNN classifier** = ID's the $k$ closest neighbors to a new point, $x_0$, and then estimates the **conditional probability $P(Y=j|X=x_0)$** for class $j$ as the fraction of DPs in the neighborhood where the response $y$ = class $j$, and then classifying the new $x_0$ DP to the class $j$ with the highest conditional probability (\*\*\*most neighbors in the neighborhood\*\*\*)
* **KNN regression** = ID's the $k$ closest neighbors to a new point, $x_0$, and then estimates $f(x_0)$ as the *average* of all training responses in that neighborhood = $\hat{f}(x_0)$
  
    * takes an abstract point as input + treats as a *real* observation + estimates the output $f(x_0)$ it would predict if real
    * becomes a function $\hat{f}$ that outputs a prediction $\hat{y}$ for any input $x_0$
    * **TLDR: it's a *quantitative estimate by averaging the result of the $k$ nearest neighbors***

**3. Suppose we have a data set with 5 predictors, $X_1$ = GPA, $X_2$ = IQ, $X_3$ = Gender (1 = Female, 0 = Male), $X_4$ = Interaction between GPA and IQ, and $X_5$ = Interaction between GPA and Gender. The response $y$ = starting salary after graduation (in thousands of $). Suppose we use least squares to fit the model, and get $\hat{\beta_0}$ = 50, $\hat{\beta_1}$ = 20, $\hat{\beta_2}$ = 0.07, $\hat{\beta_3}$ = 35, $\hat{\beta_4}$ = 0.01, $\hat{\beta_5}$ = -10**

**(a) Which answer is correct, and why?**
  
  * For a fixed value of IQ and GPA, males earn more on average than females.
  * For a fixed value of IQ and GPA, females earn more on average than males.
  * **For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough. **
    * Least Squares model for females = $50 + 20X_1 + .07X_2 + 35 + .01X_3 -10X_5 == 85 + 20(GPA) + .07(IQ) + .01(GPA:IQ) - 10(GPA:Gender) == 85 + 10(GPA) + .07(IQ) + .01(GPA:IQ)$
    * Least Squares model for males = $50 + 20X_2 + .07X_2 + .01X_3 == 50 + 20(GPA) + .07(IQ) + .01(GPA:IQ)$
    * Therefore, starting salary for males is greater if GPA is large enough such that GPA >= 3.5 (simplifying, adding the 2 models gives $35 = 10GPA$)
  * For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.

**(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.**
  
  * $85 + 10(GPA) + .07(IQ) + .01(GPA:IQ) == 85 + 10(4) + .07(110) + .01((110*4)) = 85 + 40 + 7.7 + 4.4 = \$137.1K/yr$

**(c) T/F: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.**
  
  * ***False***. The *actual value* of the interaction term's coefficient does *NOT* matter in deciding if there's evidence of an interaction effect is present. What we must do is check the p-value for the interaction term's coefficient which tests $H_{04}: \beta^4 = 0$ and associate the resulting p-value with the $t$-statistic and F score
    * **F-score = *F-Test for overall significance in Regression*** = $\frac {\frac {TSS-RSS} {p}} {\frac {RSS} {n-p-1}}$ ==> if no relationship between the response and predictors, expect F-statistic to take on a value close to 1
      * F = ratio of the mean regression sum of squares divided by the mean error sum of squares = $\frac {\frac {explained variation} {p}} {\frac {unexplained variation} {n-p-1}}$
      * TSS = $\sum_{i=1}^{n}(y_i - \bar{y})^2$, RSS = $\sum_{i=1}^{n}(y_i - \hat{y_i})^2$
      * compares an intercept-only regression model w/ current model + tries to comment on whether addition of given predictors together is significant enough for them to be there or not.
      * $H_0$: Fit of intercept only model + current model is same. (additional predictors do not provide value taken together)
      * $H_1$: Fit of intercept only model is significantly less compared to current model. (additional predictors make model significantly better)

**4. I collect a set of data ($n$ = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 +\beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$**

* **(a) Suppose the true relationship between $X$ + $Y$ is linear, i.e. $Y = \beta_0 + \beta_1X + \epsilon$. Consider the *training* residual sum of squares ($RSS$) for the *linear* regression, + also the *training* RSS for the *cubic* regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough info to tell?**
    * We *cannot* expect the cubic regression to be better, as it ***is still a linear model and our true relationship is linear***. Therefore, we do *NOT* have enough info to tell which training RSS would be lower. 
    * We *could* estimate that by adding the extra cubic term, our least squares fit *could* be a bit closer to the true regression line, and thereby giving the cubic model a lower training RSS, \*\*\*but it may not\*\*\*.
* **(b) Answer (a) using test rather than training RSS**
    * We *still do not have enough to make confident estimates*, but a conservative one would be that the cubic model *could* overfit the data (move towards non-linear fits) + therefore its test RSS would be higher.
* **(c) Suppose the true relationship between $X$ + $Y$ is *not* linear, but we *don't know how far it is from linear*. Consider the training RSS for the linear regression + the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough info to tell?**
    * Even though we do not know how far the relationship is from linear, the fact that we have added in a higher-order polynomial in the cubic regression would give a more flexible model that would better fit a non-linear relationship, thereby giving a lower training RSS
* **(d) Answer (c) using test rather than training RSS**
    * We don't have enough info say which test RSS would be lower, since we don't know how far it is from linear. 
    * If the true non-linear relationship is *closer to linear than cubic*, linear regression test RSS could be lower, or vice versa (see: ***Bias-Variance trade-off***)

**5. Consider the fitted values that result from performing linear regression *without an intercept*. In this setting, the $i$th fitted value takes the form $\hat{y}_i = x_i\hat{\beta}$, where $\hat{\beta} = \frac {\sum_{i=1}^{n}(x_iy_i)} {\sum_{i'=1}^{n}x_{i'}^2}$. Show that we can write $\hat{y_i} = \sum_{i'=1}^{n}(a_{i'}y_{i'})$, and what is $a_{i'}$?**

  * **Note: We interpret this result by saying that the fitted values from linear regression are *linear combinations* of the response values.**
  * Plugging in $\frac {\sum_{i=1}^{n}(x_iy_i)} {\sum_{i'=1}^{n}x_{i'}^2}$ for $\hat{\beta}$ in $\hat{y}_i = x_i\hat{\beta}$, we get $\hat{y}_i = x_i\frac {\sum_{i=1}^{n}(x_iy_i)} {\sum_{i'=1}^{n}x_{i'}^2}$, flipping around terms, we get $\sum_{i=1}^{n} \frac {(x_iy_i)} {\sum_{i'=1}^{n}x_{i'}^2}\hat{y_i}$ so $a_{i'} = \sum_{i=1}^{n} \frac {(x_iy_i)} {\sum_{i'=1}^{n}x_{i'}^2}$     
  * see https://blog.princehonest.com/stat-learning/ 

**6. Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x}, \bar{y})$.**

  * (3.4):
    * $\hat{\beta_1} = \frac {\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})} {\sum_{i=1}^{n}(x_i - \bar{x})^2}$
    * $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
  * least square line equation:  $\hat{y} = \hat{\beta_0} + \hat{\beta_1}x$, so if we substitute $\bar{x}$ for $x$, we get $y = \hat{\beta_0} + \hat{\beta_1}\bar{x}$
  * (3.4) hows us that the intercept = $\hat{\beta_0} = (0, \bar{y} - \hat{\beta_1}\bar{x})$
  * Plug in $\beta_0$ from (3.4) into the linear equation gives us $\hat{y} = \bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1}x$
  * This simplifies to $\hat{y} = \bar{y}$ when $x = \bar{x}$
  * So, the only way to get $\hat{y} = \bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1}x = 0$ is if $\hat{y} = \bar{y}$ and $x = \bar{x}$, which would mean $(\bar{x}, \bar{y})$ is on the line
  * http://www.science.smith.edu/~jcrouser/SDS293/assignments/solutions/a1-solution.pdf 
  
**7. It is claimed in the text that in the case of simple linear regression of $Y$ onto $X$, the $R^2$ statistic (3.17) is equal to the square of the correlation between $X$ and $Y$ (3.18). Prove that this is the case. For simplicity, you may assume that $\bar{x} = \bar{y} = 0$**
  
  * We know $R^2 = \frac {TSS - RSS} {TSS} = 1 - \frac {RSS} {TSS} = 1 - \frac {\sum_{i=1}^{n}(y_i - \hat{y_i})^2} {\sum_{i=1}^{n}(y_i - \bar{y_i})^2}$.
  * $R = Cor(X,Y) = \frac {\sum_{i=1}^{n}(x_i - \bar{x_i})(y_i - \bar{y_i})} {\sqrt{\sum_{i=1}^{n}(x_i - \bar{x_i})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y_i})^2}}$
  * Having $\hat{y_i} = \hat{\beta_i}x_i$, write $R^2 = 1 - \frac{\sum_i(y_i - \sum_jx_jy_j/\sum_jx_j^2 x_i)^2}{\sum_jy_j^2} = \frac{\sum_jy_j^2 - (\sum_iy_i^2 - 2\sum_iy_i(\sum_jx_jy_j/\sum_jx_j^2)x_i + \sum_i(\sum_jx_jy_j/\sum_jx_j^2)^2x_i^2)}{\sum_jy_j^2}$
  * This goes to $\frac{2(\sum_ix_iy_i)^2/\sum_jx_j^2 - (\sum_ix_iy_i)^2/\sum_jx_j^2}{\sum_jy_j^2} = \frac{(\sum_ix_iy_i)^2}{\sum_jx_j^2\sum_jy_j^2} = Cor(X, Y)^2 = R^2$