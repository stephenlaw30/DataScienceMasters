---
title: "ISLR_Ch5_CV_and_Bootstrap"
author: "Steve Newns"
date: "April 27, 2018"
output: html_document
---
```{r,echo=F}
knitr::opts_chunk$set(warning=FALSE,message=FALSE)
```
## 5.3 Lab: Cross-Validation and the Bootstrap

### 5.3.1 The Validation Set Approach

We explore the use of the validation set approach in order to estimate test error rates that result from fitting various linear models on `Auto`
```{r}
library(ISLR)
library(caTools)
library(tidyverse)
library(caret)
library(mosaic)
library(boot) # cv.glm()

set.seed(1)
train = sample(392,196)

lm_fit <- lm(mpg ~ horsepower, Auto, subset=train)
msummary(lm_fit)
```

Now use `predict()` to estimate the response for all 392 observations, + use `mean()` to calculate $MSE$ of the 196 observations in the validation set.
```{r}
attach(Auto)
(mse_auto <- mean((mpg - predict(lm_fit, Auto))[-train]^2))
```

Estimated test MSE for the linear regression fit = 26.14. Can use `poly()` to estimate the test error for the quadratic and cubic regressions
```{r}
quad_lm_fit <- lm(mpg ~ poly(horsepower,2), Auto, subset=train)
cubic_lm_fit <- lm(mpg ~ poly(horsepower,3), Auto, subset=train)

(quad_mse_auto <- mean((mpg - predict(quad_lm_fit, Auto))[-train]^2))
(cubic_mse_auto <- mean((mpg - predict(cubic_lm_fit, Auto))[-train]^2))
```

If we choose a different training set instead, then we will obtain somewhat different errors on the validation set.
```{r}
set.seed(2)
train = sample(392,196)

lm_fit <- lm(mpg ~ horsepower, Auto, subset=train)
quad_lm_fit <- lm(mpg ~ poly(horsepower,2), Auto, subset=train)
cubic_lm_fit <- lm(mpg ~ poly(horsepower,3), Auto, subset=train)

(mse_auto <- mean((mpg - predict(lm_fit, Auto))[-train]^2))
(quad_mse_auto <- mean((mpg - predict(quad_lm_fit, Auto))[-train]^2))
(cubic_mse_auto <- mean((mpg - predict(cubic_lm_fit, Auto))[-train]^2))
```

These results are consistent with previous findings: a model that predicts `mpg` using a quadratic function of `horsepower` performs better than a model that involves only a linear function of `horsepower`, and there is little evidence in favor of a model that uses a cubic function of `horsepower`.

### 5.3.2 Leave-One-Out Cross-Validation

The **LOOCV** estimate can be automatically computed for any GLM using `glm()` and `cv.glm()`.
```{r}
glm_fit <- glm(mpg~horsepower,data=Auto)
coef(glm_fit)
```

`glm()` can be used together with `cv.glm()` from `boot` while `lm()` cannot. It produces a list w/ several components. The 2 numbers in the `delta` vector contain the CV results for the test error:
```{r} 
cv_err=cv.glm(Auto, glm_fit)
cv_err$delta
```

In this case the numbers = identical (up to 2 decimal places) + correspond to the LOOCV statistic given in $CV_(n) = \frac {1} {n} \sum_{i=1}^n MSE_i$. Below, we discuss a situation in which the 2 #'s differ. Our CV estimate  is approximately 24.23.

We can repeat this procedure for increasingly complex polynomial fits. To automate the process, use 1for()1 to initiate a **for loop** to iteratively fit polynomial regressions for polynomials of order $i = 1$ to $i = 5$, compute associated CV error, + store it in the $i$th element of the vector `cv_error`.
```{r}
cv_err <- rep(0, 5)
xs <- rep(0, 5)

for (i in 1:5) {
  glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  xs[i] <- i
  cv_err[i] <- cv.glm(Auto, glm_fit)$delta[1]
}

cv_err
```
```{r}
df <- data.frame(xs = xs, ys = cv_err)
ggplot(df,aes(xs,ys)) + geom_point() + geom_line() + theme_bw()
```

See a sharp drop in the estimated test MSE between linear + quadratic fits, but no clear improvement onward.

### 5.3.3 k-Fold Cross-Validation

`cv.glm()` can also be used to implement **k-fold CV**. 
```{r}
set.seed(17)

k_cv_err <- rep(0, 10)
xs <- rep(0, 10)

for (i in 1:10) {
  glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  xs[i] <- i
  k_cv_err[i] <- cv.glm(Auto, glm_fit)$delta[1]
}

cv_err
```
```{r}
df <- data.frame(xs = xs, ys = k_cv_err)
ggplot(df,aes(xs,ys)) + geom_point() + geom_line() + theme_bw()
```

[1] 24.21 19.19 19.31 19.34 18.88 19.02 18.90 19.71 18.95 19.50
Notice computation time = much shorter than LOOCV. (In principle, LOOCV computation for an OLS regression should be faster than for k-fold CV, due to the availability of $CV_(n) = \frac {1} {n} \sum_{i=1}^n (\frac {y_i - \hat{y_i}} {1=h_i})^2$ for LOOCV

 * $h_i$ = **leverage** defined by $\frac {1} {n} + \frac {(x_i - \bar{x})^2} {\sum_{i`=1}^2 (x_{i`}- \bar{x})^2}$

However, `cv.glm()` does not make use of this formula. We still see little evidence for using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.

Above, the 2 #'s associated w/ delta = essentially the same when LOOCV is performed. When we instead perform k-fold CV, the 2 #'s associated w/ delta differ slightly. The 1st = the **standard k-fold CV estimate**, as in $CV_(k) = \frac {1} {k} \sum_{i=1}^k MSE_i$. The 2nd = a *bias-corrected version*. 
```{r}
glm_fit <- glm(mpg ~ poly(horsepower, 2), data = Auto)
cv.glm(Auto, glm_fit)$delta
```

On this data set, the 2 estimates are very similar to each other.

### 5.3.4 The Bootstrap

#### Estimating the Accuracy of a Statistic of Interest

1 great advantage of the bootstrap approach = it can be applied in almost all situations. No complicated mathematical calculations
are required. Performing a bootstrap analysis in R entails only two steps.

* create a function that computes the statistic of interest.
* use `boot()` to perform the bootstrap by repeatedly sampling observations from the data set with replacement.

Using `Portfolio` data set in `ISLR` package, create a function, `alpha_fn()`, which takes as input: the $(X,Y)$ data as well as
a vector indicating which observations should be used to estimate $\alpha$ + outputs the estimate for $\alpha$ based on the selected observations.
```{r}
alpha_fn <- function (data, index) {
  
  X <- data[index,1]
  Y <- data[index,2]
  
  return((var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y)))
}
```

This function returns an estimate for $\alpha$ based on applying $\hat{\alpha} = \frac {\hat{\sigma}^2_Y - \hat{\sigma}_XY} {\hat{\sigma}^2_Y + \hat{\sigma}^2_Y - 2\hat{\sigma}_XY}$ to the observations indexed by the `index` arg.
```{r}
# estimate alpa using all 100 observations.
alpha_fn(Portfolio, 1:100)
```

Next, use `sample()` to randomly select 100 observations from the range 1 to 100, with replacement (equivalent to constructing the resulting standard deviation). `boot()` automates this approach. 
```{r}
# produce R = 1K bootstrap estimates for alpha.
boot(data = Portfolio, statistic= alpha_fn, R = 1000)
```

Final output shows that, using the original data, $\hat{\alpha} = 0.5758$, and that the bootstrap estimate for $SE(\hat{\alpha}) = 0.0896$

#### Estimating the Accuracy of a Linear Regression Model

Bootstrap approach can be used to assess the variability of the coefficient $\beta$ estimates + predictions from a statistical learning method. Here, we use the bootstrap approach to assess the variability of the estimates for $\beta_0$ abd $\beta_1$, the intercept + slope terms for the linear regression model that uses `horsepower` to predict `mpg` in `Auto`. 

Will compare the estimates obtained using the bootstrap to those obtained using the formulas for $SE(\hat{\beta_0})$and $SE(\hat{\beta_1})$ described in Section 3.1.2. 

1st, create a simple function, 1boot.fn()1, which takes in `Auto` data as well as a set of indices for the observations, + returns the intercept + slope estimates for the linear regression model.Then, apply this function to the full set of 392 observations to compute the estimates of $\beta_0$ abd $\beta_1$ on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3.
```{r}
boot_fn <- function(data,index) {
  return(coef(lm(mpg ~ horsepower, data=data, subset=index)))
}

boot_fn(Auto,1:392)
```
`boot.fn()` can also be used in order to create bootstrap estimates for the intercept + slope terms by randomly sampling from among
the observations with replacement. 
```{r}
set.seed(1)

boot_fn(Auto, sample(x = 392, size = 392, replace = T))

boot_fn(Auto, sample(x = 392, size = 392, replace = T))
```

Next, use `boot()` to compute the standard errors of 1k bootstrap estimates for the intercept and slope terms.
```{r}
boot(data = Auto, statistic = boot_fn, R = 1000)
```

This indicates that the bootstrap estimates for $SE(\hat{\beta}_0) = 0.86$ and $SE(\hat{\beta}_1) = 0.0074$. As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the `summary()` or `msummary()`
```{r}
msummary(lm(mpg ~ horsepower, Auto))$coef
```

The SE estimates for $\hat{\beta}_0$ and $\hat{\beta}_1$ obtained using the formulas from Section 3.1.2 are $0.717$ for the intercept and $0.0064$ for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, ***it suggests the opposite***. Recall the standard formulas given in $SE(\hat{\beta}_0) = \sigma^2[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}, SE(\hat{\beta}_1) = \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}$ rely on certain assumptions. 

For example, they depend on the unknown parameter $\sigma^2 = Var(\epsilon)$, the **noise variance**. We then estimate $\sigma^2 $ using $RSS$. Now, although the formula for the standard errors do not rely on the linear model being correct, the estimate for $\sigma^2$ does. We see in Figure 3.8 on page 91 that there is a non-linear relationship in the data, + so the residuals from a linear fit will be inflated, and so will $\hat{\sigma}^2$. Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, + all variability comes from the variation in the errors $\epsilon_i$. 

***The bootstrap approach does NOT rely on any of these assumptions***, and so it is likely giving a *MORE* accurate estimate of the standard errors of $\beta_0$ and $\beta_1$ than `summary()` is.

Below we compute the bootstrap standard error estimates + the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates +
the standard estimates of $SE(\hat{\beta}_0)$, $SE(\hat{\beta}_1)$, and $SE(\hat{\beta}_2)$
```{r}
boot_fn_quad<- function(data,index) {
  return(coef(lm(mpg ~ horsepower + I(horsepower^2), data=data, subset=index)))
}

set.seed(1)
boot(data=Auto, statistic=boot_fn_quad, R = 1000)
msummary(lm(mpg ~ horsepower + I(horsepower^2), Auto))$coef

```