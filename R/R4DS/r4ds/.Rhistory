# Encode outcome
social %>%
mutate(Purchased <- factor(Purchased, levels = c(0,1)))
head(social)
# Encode outcome
social %>%
mutate(Purchased <- factor(Purchased, levels = c(0,1)))
levels(social$Purchased)
# Encode outcome
social %>%
mutate(Purchased <- factor(social$Purchased, levels = c(0,1)))
levels(social$Purchased)
# Encode outcome
social %>%
mutate(Purchased = factor(Purchased, levels = c(0,1)))
levels(social$Purchased)
# Encode outcome
social <- social %>%
mutate(Purchased = factor(Purchased, levels = c(0,1)))
levels(social$Purchased)
# split data into Training + Test sets
set.seed(123)
split <- social %>%
sample.split(Purchased, SplitRatio = 0.75)
# split data into Training + Test sets
set.seed(123)
split <- social %>%
sample.split(Purchased, SplitRatio = 0.75)
social
# split data into Training + Test sets
set.seed(123)
split <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
training <- social %>%
subset(split == TRUE)
test <- social %>%
subset(split == FALSE)
#test_set = subset(dataset, split == FALSE)
# split data into Training + Test sets
set.seed(123)
split <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(split == TRUE))
(test <- social %>%
subset(split == FALSE))
install.packages("e1071")
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
# Importing dataset
(social <- read_csv('Social_Network_Ads.csv') %>%
select(Age,EstimatedSalary,Purchased))
# Encode outcome
social <- social %>%
mutate(Purchased = factor(Purchased, levels = c(0,1)))
levels(social$Purchased)
# split data into Training + Test sets
set.seed(123)
split <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(split == TRUE))
(test <- social %>%
subset(split == FALSE))
?vsm
?svm
# split data into Training + Test sets
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(spl == TRUE))
(test <- social %>%
subset(spl == FALSE))
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test_set[-3])
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
# Making the Confusion Matrix
cm <= table(test_set[, 3], y_pred)
## Making the Confusion Matrix
cm <= table(test[, 3], y_pred.svm)
## Making the Confusion Matrix
cm <= table(test[, 3], y_pred.svm)
y_pred.svm
test[, 3]
## Making the Confusion Matrix
cm <- table(test[, 3], y_pred.svm)
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
# split data into Training + Test sets
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(spl == TRUE))
(test <- social %>%
subset(spl == FALSE))
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
## Making the Confusion Matrix
(cm <- table(test[, 3], y_pred.svm))
## Making the Confusion Matrix
(cm <- table(y_pred.svm))
## Making the Confusion Matrix
(cm <- table(test[, 3]))
## Making the Confusion Matrix
(cm <- table(y_pred.svm))
## Making the Confusion Matrix
(cm <- table(test[, 3], y_pred.svm))
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
# split data into Training + Test sets
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(spl == TRUE))
(test <- social %>%
subset(spl == FALSE))
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y.pred.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y.pred.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
plot.svm.model(test)
library(statsr)
library(tidyverse)
library(ggplot2)
library(statsr) # Duke package
library(tidyverse)
library(ggplot2)
data(atheism)
head(atheism)
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012")
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012")
table(us12$response)
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012") %>%
group_by(response) %>%
summarise(prop = mean(response))
#table(us12$response)
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012")# %>%
#group_by(response) %>%
#summarise(prop = mean(response))
prop.table(us12$response)
us12 <- atheism %>%
filter(nationality == "United States" , atheism$year == "2012")# %>%
#group_by(response) %>%
#summarise(prop = mean(response))
prop.table(table(us12$response))
inference(y = response, data = us12, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
.0499 * .0364
.0499 * .0354
.0499 - .0364
ire12 <- atheism %>%
filter(nationality == "Ireland" , atheism$year == "2012")# %>%
#group_by(response) %>%
#summarise(prop = mean(response))
prop.table(table(ire12$response))
ire12 <- atheism %>%
filter(nationality == "Ireland" , atheism$year == "2012")
prop.table(table(ire12$response))
pol12 <- atheism %>%
filter(nationality == "Poland" , atheism$year == "2012")
prop.table(table(pol12$response))
ire12 <- atheism %>%
filter(nationality == "Ireland" , atheism$year == "2012")
prop.table(table(ire12$response))
inference(y = response, data = ire12, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
pol12 <- atheism %>%
filter(nationality == "Poland" , atheism$year == "2012")
prop.table(table(pol12$response))
inference(y = response, data = pol12, statistic = "proportion", type = "ci", method = "theoretical", success = "atheist")
ire.moe <- .0990099 - .0806
pol.moe <- .04952381 - 0.031
i(re.moe <- .0990099 - .0806)
(ire.moe <- .0990099 - .0806)
(pol.moe <- .04952381 - 0.031)
d <- data.frame(p <- seq(0, 1, 0.01))
n <- 1000
d <- d %>%
mutate(me = 1.96*sqrt(p*(1 - p)/n))
d %>%
ggplot(aes(p, me)) +
geom_line()
?inference
esp12 <- atheism %>%
filter(nationality == "Spain") %>%
inference(esp12, est = "proportion", type = "ht", method = "theoretical", success = "atheist",
null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain") %>%
inference(esp12, statistic = "proportion", type = "ht", method = "theoretical", success = "atheist",
null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain") %>%
inference(data = esp12, statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(esp12, statistic = "proportion", type = "ht", method = "theoretical", success = "atheist",
null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(data = esp12, statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
library(tidyverse)
source("http://bit.ly/dasi_inference")
# create set of the actual observed values
paul <- factor(c(rep("y",8),rep("n",0)), levels = c("y","n"))
# perform 10k simulations (default) to estimate a proportion using a hyp. test via a simulation
inference(paul, est = "proportion", type = "ht", method = "simulation", success = "y",
null = .5, alternative = "greater")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(data = factor(esp12), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(data = factor(esp12$year), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12$year), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(esp12$response, factor(esp12$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp <- atheism %>%
filter(nationality == "Spain")
inference(esp12$response, factor(esp$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
View(esp12)
us <- atheism %>%
filter(nationality == "Spain")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
us <- atheism %>%
filter(nationality == "Spain")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
us <- atheism %>%
filter(nationality == "United States")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
# sig level * # of countries
39 * .05
tail(d)
kable(pressure)
??kable
kable(pressure)
library(tidyverse)
library(knitr)
kable(pressure)
?kable
#library(knitr)
kable(pressure)
kable(mtcars)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov*sd.hs)/R)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)/R)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)*R)
pov_bar <- 11.35
hs_bar <- 86.01
(int <- pov_bar - slope*hs_bar)
library(tidyverse)
library(ggplot2)
library(nycflights13)
airlines
airports
# check that tailnum uniquely ID's a plane
planes %>%
count(tailnum) %>%
filter(n > 1)
# check that tailnum uniquely ID's a plane
planes %>%
count(tailnum) %>%
filter(n > 1)
# check each YMD:H + origin combo uniquely ID's each weather record
weather %>%
count(year, month, day, hour, origin) %>%
filter(n > 1)
# check if YMD + flight # uniquely ID's a flight
flights %>%
count(year, month, day, flight) %>%
filter(n > 1)
# check if YMD + tail # uniquely ID's a flight
flights %>%
count(year, month, day, tailnum) %>%
filter(n > 1)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)*R)
pov_bar <- 11.35
hs_bar <- 86.01
(int <- pov_bar - slope*hs_bar)
int - slope*.82
int - slope*82
int - slope*.82
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)*R)*100
pov_bar <- 11.35
hs_bar <- 86.01
(int <- pov_bar - slope*hs_bar)
int - slope*82
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)*R)
pov_bar <- 11.35
hs_bar <- 86.01
(int <- pov_bar - slope*hs_bar)
int - slope*82
int - (slope*82)
(slope*82)
int + (slope*82)
int <- 64.68
HSgr <- 81.4
slope <- 0.62
(int <- pov_bar - slope*HSgr)
int <- 64.68
HSgr <- 81.4
slope <- 0.62
(int - slope*HSgr)
int <- 64.68
HSgr <- 81.4
slope <- -0.62
(int + slope*HSgr)
library(tidyverse)
library(ggplot2)
library(nycflights13)
Lahman::Batting,
Lahman::Batting
Lahman::Batting %>%
count(playerID) %>%
filter(n > 1)
count(playerID, year) %>%
c
Lahman::Batting %>%
count(playerID, year) %>%
filter(n > 1)
Lahman::Batting %>%
count(playerID, year) %>%
Lahman::Batting %>%
count(playerID, year) %>%
Lahman::Batting %>%
filter(n > 1)
Lahman::Batting %>%
count(playerID, yearID) %>%
filter(n > 1)
Lahman::Batting %>%
count(playerID, yearID, stint) %>%
filter(n > 1)
head(babynames::babynames)
babynames::babynames
head(diamonds)
diamonds %>%
count(carat,cut,color) %>%
filter( n > 1)
diamonds %>%
count(carat,cut,color,clarity) %>%
filter( n > 1)
(flights2 <- flights %>%
select(year:day, hour, origin, dest, tailnum, carrier))
flights2 <- flights %>%
select(year:day, hour, origin, dest, tailnum, carrier)
flights2 %>%
select(-origin, -dest) %>%
left_join(airlines, by = "carrier")
x <- tribble(
~key, ~val_x,
1, "x1",
2, "x2",
3, "x3"
)
y <- tribble(
~key, ~val_y,
1, "y1",
2, "y2",
4, "y3"
)
x %>%
inner_join(y, by = "key")
x <- tribble(
~key, ~val_x,
1, "x1",
2, "x2",
2, "x3",
1, "x4"
)
y <- tribble(
~key, ~val_y,
1, "y1",
2, "y2"
)
left_join(x, y, by = "key")
flights2 %>%
left_join(weather)
flights2 %>%
left_join(planes, by = "tailnum")
flights2 %>%
left_join(airports, c("dest" = "faa"))
