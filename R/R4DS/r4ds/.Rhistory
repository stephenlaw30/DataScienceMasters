esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(data = factor(esp12), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(data = factor(esp12$year), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12$year), statistic = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(factor(esp12$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp12 <- atheism %>%
filter(nationality == "Spain")
inference(esp12$response, factor(esp12$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
esp <- atheism %>%
filter(nationality == "Spain")
inference(esp12$response, factor(esp$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
View(esp12)
us <- atheism %>%
filter(nationality == "Spain")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
us <- atheism %>%
filter(nationality == "Spain")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
us <- atheism %>%
filter(nationality == "United States")
inference(us$response, factor(us$year), est = "proportion", type = "ht", method = "theoretical",
success = "atheist", null = 0, alternative = "twosided")
# sig level * # of countries
39 * .05
tail(d)
kable(pressure)
??kable
kable(pressure)
library(tidyverse)
library(knitr)
kable(pressure)
?kable
#library(knitr)
kable(pressure)
kable(mtcars)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov*sd.hs)/R)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)/R)
sd.pov <- 3.1
sd.hs <- 3.73
R <- -.75
(slope <- (sd.pov/sd.hs)*R)
pov_bar <- 11.35
hs_bar <- 86.01
(int <- pov_bar - slope*hs_bar)
library(tidyverse)
library(ggplot2)
library(nycflights13)
airlines
airports
# check that tailnum uniquely ID's a plane
planes %>%
count(tailnum) %>%
filter(n > 1)
# check that tailnum uniquely ID's a plane
planes %>%
count(tailnum) %>%
filter(n > 1)
# check each YMD:H + origin combo uniquely ID's each weather record
weather %>%
count(year, month, day, hour, origin) %>%
filter(n > 1)
# check if YMD + flight # uniquely ID's a flight
flights %>%
count(year, month, day, flight) %>%
filter(n > 1)
# check if YMD + tail # uniquely ID's a flight
flights %>%
count(year, month, day, tailnum) %>%
filter(n > 1)
library(tidyverse)
library(ggplot2)
candy <- read_csv("candyhierarchy2017.csv")
head(candy)
candy %>%
summarise()
candy %>%
summarise(count())
candy %>%
summarise(count())
candy %>%
summarise(`Q4: Country`)
candy %>%
summarise(`Q4: COUNTRY`)
candy %>%
group_by(`Q4: COUNTRY`)
candy %>%
group_by(`Q4: COUNTRY`) %>%
summarise(count())
candy %>%
group_by(`Q4: COUNTRY`) %>%
summarise(count = n())
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(count = n())
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(count = n(), na.rm = T)
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(count = n(, na.rm = T), na.rm = T)
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(count = n(na.rm = T), na.rm = T)
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(count = n())
candy %>%
group_by(`Q2: GENDER?`) %>%
summarise(count = n())
candy %>%
group_by(`Q2: GENDER`) %>%
summarise(count = n())
candy %>%
filter(`Q2: GENDER` = "Female") %>%
parse_integer(`Q3: AGE`)
candy %>%
filter(`Q2: GENDER` == "Female") %>%
parse_integer(`Q3: AGE`)
candy %>%
filter(`Q2: GENDER` == "Female") %>%
as.integer(`Q3: AGE`)
candy %>%
filter(`Q2: GENDER` == "Female") #%>%
# as.integer(`Q3: AGE`)
candy <- candy %>%
as.integer(`Q3: AGE`)
#candy <- candy %>%
as.integer(candy$.Q3: AGE`)
#candy <- candy %>%
head(as.integer(candy$.`Q3: AGE`))
#candy <- candy %>%
head(as.integer(candy$`Q3: AGE`))
candy <- candy %>%
as.integer(`Q3: AGE`)
candy <- candy %>%
as.integer(candy$`Q3: AGE`)
candy$`Q3: AGE` <- as.integer(candy$`Q3: AGE`)
candy$`Q3: AGE` <- as.integer(candy$`Q3: AGE`)
candy %>%
filter(is.na(`Q3: AGE`) == F)
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_hist(aes(`Q3: AGE`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Male") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Male") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Male") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`), na.rm = T)
candy %>%
filter(is.na(`Q3: AGE`) == F,
`Q2: GENDER` == "Male") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`), na.rm = T)
candy %>%
filter(is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`))
candy %>%
filter(is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Male") %>%
ggplot() +
geom_bar(aes(`Q1: GOING OUT?`))
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "Yes") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "No") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "Yes") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 1)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "No") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "Yes") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 3)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Female",
`Q1: GOING OUT?` == "No") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Male",
`Q1: GOING OUT?` == "Yes") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 3)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Male",
`Q1: GOING OUT?` == "No") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Male",
`Q1: GOING OUT?` == "Yes") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 4)
candy %>%
filter(is.na(`Q3: AGE`) == F,
is.na(`Q1: GOING OUT?`) == F,
`Q2: GENDER` == "Male",
`Q1: GOING OUT?` == "No") %>%
ggplot() +
geom_histogram(aes(`Q3: AGE`), binwidth = 2)
candy %>%
sapply(function(x)length(unique(x)))
candy %>%
summarise_all(funs(n_distinct(.)))
?funs
candy %>%
summarise_all(n_distinct(.))
candy %>%
summarise_all(funs(n_distinct(.)))
candy %>%
distinct() %>%
count(var)
candy %>%
gather(var, value) %>%
distinct() %>%
count(var)
# %>%
candy %>%
group_by(`Q1: GOING OUT?`) %>%
summarise(no_rows = length(`Q1: GOING OUT?`))
# %>%
candy %>%
group_by(6) %>%
summarise(no_rows = length(6))
# %>%
candy %>%
group_by([6]) %>%
library(tidyverse)
library(ggplot2)
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
head(demons)
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
head(demons)
devil2 <- devil %>%
select(DevilRef,DevilID,CaseRef,Devil_Type)
demon2 <- demon %>%
select(DemonicRef,DemonicID,CaseReg,Demonic_Type)
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
head(demons)
devil2 <- devil %>%
select(DevilRef,DevilID,CaseRef,Devil_Type)
demons2 <- demons %>%
select(DemonicRef,DemonicID,CaseReg,Demonic_Type)
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
head(demons)
devil2 <- devil %>%
select(DevilRef,DevilID,CaseRef,Devil_Type)
demons2 <- demons %>%
select(DemonicRef,DemonicID,CaseRef,Demonic_Type)
devil2 %>%
inner_join(demons2, by = "CaseRef")
demons <- read_csv("WDB_DemonicPact.csv")
devil <- read_csv("WDB_DevilAppearance.csv")
head(devil)
head(demons)
devil2 <- devil %>%
select(DevilRef,DevilID,CaseRef,Devil_Type)
demons2 <- demons %>%
select(DemonicRef,DemonicID,CaseRef,Demonic_Type)
devil_demon <- devil2 %>%
inner_join(demons2, by = "CaseRef")
table(devil_demon$Devil_Type)
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
## import data
social <- read_csv("Social_Network_Ads.csv")
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
social
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# split data into Training + Test sets
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(spl == TRUE))
(test <- social %>%
subset(spl == FALSE))
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'radial')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
plot.svm.model(test)
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("Radial SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
plot.svm.model(test)
library(tidyverse)
library(ggplot2)
library(caTools)
library(e1071) # most popular SVM libary, other is `kernlab`
library(ElemStatLearn)
## import data
social <- read_csv("Social_Network_Ads.csv")
## Cut down dataset
social <- social %>%
select(Age,EstimatedSalary,Purchased) %>% # cut down cols
mutate(Purchased = factor(Purchased, levels = c(0,1)), # encode outcome
# scale data (age is in 10s, salary in 10k's)
Age = c(scale(Age)), # use c(scale)) to prevent scale from changing class of the column
EstimatedSalary = c(scale(EstimatedSalary))) # Encode target/outcome variable as fact
# split data into Training + Test sets
set.seed(123)
spl <- sample.split(social$Purchased, SplitRatio = 0.75) # tidy does not work here
(training <- social %>%
subset(spl == TRUE))
(test <- social %>%
subset(spl == FALSE))
social.svm <- svm(Purchased ~ ., training, type = 'C-classification', kernel = 'linear')
# Predict on test set results
y_pred.svm <- predict(social.svm, test[-3])
## Making the Confusion Matrix
(table(test$Purchased, y_pred.svm))
## Create function to visualize different set results
plot.svm.model <- function(set) {
x1 <- seq(min(set[,1]) - 1, max(set[,1]) + 1, by = 0.05) # get sminimum scaled age + go up to the max by increments of .1
x2 <- seq(min(set[,2]) - 1, max(set[,2]) + 1, by = 0.05) # get minimum scaled salary + go up to the max by increments of .1
set.grid <- expand.grid(x1,x2) # Create DF from all possible combos of the age + salary vectors to make a grid
colnames(set.grid) <- c("Age","EstimatedSalary")
y_pred.svm.grid <- predict(social.svm, set.grid) # use grid to predict values w/ our model
plot(set[,-3],
main = c("SVM (", deparse(substitute(set))," set)"), # get set name from argument given
xlab = "Age",
ylab = "Estimated Salary",
xlim = range(x1),
ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_pred.svm.grid), length(x1), length(x2)), add = T) # add logistic regression "split" line
points(set.grid, pch = 19, col = if_else(y_pred.svm.grid == 1, "springgreen3", "tomato"))
points(set, pch = 19, col = if_else(set[,3] == 1, "green4", "red3"))
}
plot.svm.model(training)
plot.svm.model(test)
