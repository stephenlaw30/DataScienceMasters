---
title: "Ch11_DataImport"
author: "Steve Newns"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```

# Intro

Load **flat files** in R as data frames w/ `readr` package (part of core tidyverse)

* `read_csv()` = comma delimited, `read_csv2()` = semicolon separated, `read_tsv()` = tab delimited, `read_delim()`  = any delimiter.
* `read_fwf()` = fixed width files
    * specify fields either by their widths w/ `fwf_widths()` or position w/ `fwf_positions()`. 
    * `read_table()` = reads common variation of fixed width files where columns are separated by white space.
* `read_log()` = Apache style log files. (also check `webreadr` = built on top of `read_log()` + provides many more helpful tools.)

Can also supply an **inline csv file** = useful for experimenting w/ `readr` + for creating reproducible examples to share w/ others

```{r}
read_csv("a,b,c
1,2,3
4,5,6")
```

`read_csv()` uses 1st line of data for column names (very common convention), + there are 2 cases where you might want to tweak this behaviour:

* 1) few lines of metadata at the top of the file --> `skip = n` to skip first n lines or use `comment = "#"` to drop all lines that start with (e.g.) #.

```{r}
read_csv("The first line of metadata
  The second line of metadata
  x,y,z
  1,2,3", 
  skip = 2)
```

```{r}
read_csv("# A comment I want to skip
  x,y,z
  1,2,3", comment = "#")
```

  * 2) Data might not have column names
    * use `col_names = FALSE` to tell read_csv() *not* to treat the 1st row as headings + instead label them sequentially from X1 to Xn:
    * Or can pass `col_names` a character vector to be used as the column names:

```{r}
read_csv("1,2,3\n4,5,6", col_names = FALSE)
read_csv("1,2,3
         4,5,6", col_names = FALSE)
read_csv("1,2,3\n4,5,6", col_names = c("x","y","z"))

```

Another option that commonly needs tweaking = `na` = specifies value(s) used to represent missing values in a file:

```{r}
read_csv("a,b,c\n1,2,.", na = ".")
```

To read in more challenging files, learn more about how `readr` **parses** each column, turning them into R **vectors**

# Compared to base R

`readr` functions are typically much faster (~10x) than base equivalents. 

* If looking for raw speed, try `data.table::fread()` = doesn’t fit quite so well into tidyverse, but can be quite a bit faster.

`readr` produces tibbles, don’t convert character vectors to factors, use row names, or munge column names (common sources of frustration w/ base R)

`readr`  = more reproducible b/c Base R functions inherit some behaviour from your OS + environment variables, so import code that works on 1 CPU might not work on another

## 11.2.2 Exercises

**What function would you use to read a file where fields were separated with “|”?**

```{r}
read_delim("a|b|c\n1|2|3", delim = "|")
```

**Apart from file, skip, and comment, what other arguments do ?read_csv() and ?read_tsv() have in common?**

* *All arguments are the same*

**What are the most important arguments to read_fwf()?**

* *file, col_position, skip, col_types*

**Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character. By convention, `read_csv()` assumes the quoting character will be ", + to change it you’ll need to use `read_delim()` instead. What arguments do you need to specify to read the following text into a data frame?**

```{r}
read_delim("x,y\n1,'a,b'", quote = "\'", delim = ",")
```

**ID what is wrong with each of the following inline CSV files. What happens when you run the code?**


  * read_csv("a,b\n1,2,3\n4,5,6") = only 2 cols, but then 3 values given
  * read_csv("a,b,c\n1,2\n1,2,3,4") = only 2 values given in 1st row after headers so last value = missing, final value in 2nd row dropped
  * read_csv("a,b\n\"1") = 1 should be a character (?) but it results in an int due 
  * read_csv("a,b\n1,2\na,b") = ?
  * read_csv("a;b\n1;3") = no specification that the delimiter is ";"

# 11.3 Parsing a vector

`parse_*()` functions take a character vector + return a more specialised vector like a logical, integer, or date:

```{r}
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))
```

These functions are useful in their own right, but are also important building blocks for `readr`.

Like all functions in tidyverse, `parse_*()` functions = **uniform**: 1st argument = a character vector to parse, + `na` argument specifies *which* strings should be treated as missing:

```{r}
parse_integer(c("1", "231", ".", "456"), na = ".")
```

If parsing fails, you’ll get a warning + the failures will be missing in the output:

```{r}
(x <- parse_integer(c("123", "345", "abc", "123.45")))
```

If there *are* many parsing failures, use `problems()` to get the complete set as a tibble, which we can manipulate w/ `dplyr`.

```{r}
problems(x)
```

Using parsers is mostly a matter of understanding what’s available + how they deal w/ different types of input.

8 particularly important parsers:

* `parse_logical()` + `parse_integer()` = basically nothing can go wrong w/ these parsers
* `parse_double()` = *strict* numeric parser
* `parse_number()` = *flexible* numeric parser (more complicated than you might expect b/c different parts of the world write #'s in different ways()
* `parse_character()` = 1 complication makes it quite important: **character encodings**
* `parse_factor()` = creates factors, the data structure that R uses to represent categorical variables with fixed and known values.
`parse_datetime()`, `parse_date()`, `parse_time()` = parse various date & time specifications = most complicated b/c there are so many different ways of writing dates

# 11.3.1 Numbers

Issues w/ #'s:

* 1) People write #'s differently in different parts of the world (some countries use . in between integer + fractional parts of a real number, while others use ,.)
* 2) Numbers often surrounded by other characters that provide some context, like “$1000” or “10%”.
* 3) Numbers often contain "grouping” characters to make easier to read, like “1,000,000”, + these vary around the world.

To address the 1st problem, `readr` has a **locale** arg = an object that specifies parsing options that differ from place to place. 
* When parsing #'s the most important option = character used for the decimal
* Can override default value of `.` by creating a *new* locale + setting the `decimal_mark` argument:

```{r}
parse_double("1.23")
parse_double("1,23", locale = locale(decimal_mark = ","))
```
readr’s default locale = US-centric, b/c generally R is US-centric (documentation of base R = written in American English). 
Alternative approach = try + guess defaults from your OS (hard to do well + more importantly, makes code fragile)

* even if it works on 1 CPU, it might fail when emailed it to a colleague in another country.

`parse_number()` addresses the 2nd problem = ignores non-numeric characters before + after the # (particularly useful for currencies + %'s, but also works to extract numbers embedded in text)

```{r}
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45")
```

Final problem is addressed by a combo of `parse_number()` + `locale` w/in `parse_number()` that'll ignore the “grouping mark”:

```{r}
# Used in America
parse_number("$123,456,789")
# Used in many parts of Europe
parse_number("123.456.789", locale = locale(grouping_mark = "."))
#> [1] 1.23e+08
# Used in Switzerland
parse_number("123'456'789", locale = locale(grouping_mark = "'"))
```

# 11.3.2 Strings

There're multiple ways to represent the same string --> need to dive into details of how CPUs represent strings --> In R, look @ underlying representation of a string using `charToRaw()`:

```{r}
charToRaw("Hadley")
```

Each **hexadecimal #** represents a byte of info: 48 = H, 61 = a, + so on. 

The **mapping** from hexadecimal # to character = the **encoding**, + in this case the encoding = **ASCII**, which does a great job of representing English characters, b/c ASCII = **American Standard Code for Information Interchange**.

Things get more complicated for languages other than English. In early days of CPUs = many competing standards for encoding non-English characters + to correctly interpret a string you needed to know *both* the values *and* the encoding. 

* Ex: 2 common encodings = **Latin1** (aka ISO-8859-1 = Western European languages) + **Latin2** (aka ISO-8859-2 = Eastern European languages). 
* In **Latin1**, byte b1 = “±”, but in **Latin2**, b1 = “ą”!

Fortunately, today there is 1 standard supported almost everywhere: **UTF-** = can encode just about every character used by humans today, as well as many extra symbols (like emoji)

`readr` uses **UTF-8** everywhere = assumes data is UTF-8 encoded when read + always uses it when writing. 

This is a good default but will fail for data produced by older systems that don’t understand UTF-8. 
If this happens, strings will look weird when printed = Sometimes just 1 or 2 chars might be messed up; other times it's complete gibberish. 

```{r}
x1 <- "El Ni\xf1o was particularly bad this year"
x2 <- "\x82\xb1\x82\xf1\x82\xc9\x82\xbf\x82\xcd"

#x1
#> [1] "El Ni\xf1o was particularly bad this year"
#x2
#> [1] "\x82\xb1\x82\xf1\x82ɂ\xbf\x82\xcd"
```

To fix the problem, specify the encoding in `parse_character()`:

```{r}
parse_character(x1, locale = locale(encoding = "Latin1"))
parse_character(x2, locale = locale(encoding = "Shift-JIS"))
```

How do you find the correct encoding? If lucky, it’ll be included somewhere in the data documentation

* Rarely the case, so `readr` provides `guess_encoding()` to help figure it out
    * Not foolproof + it works better w/ lots of text (unlike here), but it’s a reasonable place to start. 
* Expect to try a few different encodings before finding the right one.
* 1st arg to `guess_encoding()` = either a path to a file or a raw vector (useful if the strings are already in R).

Encodings = rich + complex topic + to learn, read detailed explanation at http://kunststube.net/encoding/.

```{r}
guess_encoding(charToRaw(x1))
guess_encoding(charToRaw(x2))
```

# 11.3.3 Factors

R uses **factors** to represent **categorical variables** w/ a known set of possible values. Give `parse_factor()` a vector of known levels to generate a warning whenever an unexpected value is present:

```{r}
fruit <- c("apple", "banana")
parse_factor(c("apple", "banana", "bananana"), levels = fruit)
```

If you have many problematic entries, it’s often easier to leave them as character vectors _ then use the tools for strings + factors to clean them up.

# 11.3.4 Dates, date-times, and times

3 parsers depending on whether you want a date (# of days since 1970-01-01), date-time (# of secs since midnight 1970-01-01), or a time (# of secs since midnight). 

When called w/out any additional arguments:

* `parse_datetime()` expects an **ISO8601 date-time** = an international standard in which components of a date are organised from biggest to smallest --> Y M D H M S

* ISO8601 = most important date/time standard + if you work w/ dates + times frequently, read https://en.wikipedia.org/wiki/ISO_8601

```{r}
parse_datetime("2010-10-01T2010")
# If time is omitted, it will be set to midnight
parse_datetime("20101010")
```

`parse_date()` expects a 4 digit year, a `-` or `/`, the month, a `-` or `/`, then day:

```{r}
parse_date("2010-10-01")
```

`parse_time()` expects hour, `:`, minutes, + optionally `:` + `seconds`, + an optional am/pm specifier:

```{r}
library(hms)
parse_time("01:10 am")
parse_time("20:10:01")
```

Base R doesn’t have a great built-in class for time data, so we use the one provided in the `hms` package.

If these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces:

### Year
  * `%Y` (4 digits).
  * `%y` (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999.
  
### Month
  * `%m` (2 digits).
  * `%b` (abbreviated name, like “Jan”).
  * `%B` (full name, “January”)
  
### Day
  * `%d` (2 digits).
  * `%e` (optional leading space)
  
### Time
  * `%H` 0-23 hour
  * `%I` 0-12, must be used with %p
  * `%p` AM/PM indicator
  * `%M` minutes
  * `%S` integer seconds
  * `%OS` real seconds
  * `%Z` Time zone (as name, e.g. America/Chicago)
    * Beware of abbreviations: if American, note “EST” = a *Canadian* time zone that does *NOT* have daylight savings time
  * `%z` (as offset from UTC, e.g. +0800)
  
### Non-digits
  * `%.` skips one non-digit character
  * `%*` skips any number of non-digits

*Best way to figure out correct format* = create a few examples in a character vector + test w/ 1 of the parsing functions

```{r}
parse_date("01/02/15", "%m/%d/%y")
parse_date("01/02/15", "%d/%m/%y")
parse_date("01/02/15", "%y/%m/%d")
```

If using `%b` or `%B` w/ *non-English month names*, you need to set `lang` argument to `locale()`. 

* See list of built-in languages in `date_names_langs()`, or if a language is not already included, create your own w/ `date_names()`.

```{r}
parse_date("1 janvier 2015", "%d %B %Y", locale = locale("fr"))
```

## 11.3.5 Exercises

**What are the most important arguments to ?locale()?**

* encoding + decimal mark + grouping mark, date_names, tz

**What happens if you try + set decimal_mark + grouping_mark to the same character?**

```{r}
parse_number("123.456.789", locale = locale(decimal_mark = ".", grouping_mark = "."))
```

**What happens to the default value of grouping_mark when you set decimal_mark to “,”?** 

```{r}
parse_number("123.456,789", locale = locale(decimal_mark = ","))
```

**What happens to the default value of decimal_mark when you set the grouping_mark to “.”?**

```{r}
parse_number("123.456,789", locale = locale(grouping_mark = "."))
```

**Didn’t discuss `date_format` and `time_format` options to locale(). What do they do? Construct an example that shows when they might be useful.**

**If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly**

**What’s the difference between read_csv() and read_csv2()?**

**What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out.**

**Generate the correct format string to parse each of the following dates and times:**

d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"

# 11.4 Parsing a file

Now that you’ve learned how to parse an individual vector, it’s time to return to the beginning and explore how readr parses a file. There are two new things that you’ll learn about in this section:

How readr automatically guesses the type of each column.
How to override the default specification.
11.4.1 Strategy

readr uses a heuristic to figure out the type of each column: it reads the first 1000 rows and uses some (moderately conservative) heuristics to figure out the type of each column. You can emulate this process with a character vector using guess_parser(), which returns readr’s best guess, and parse_guess() which uses that guess to parse the column:

guess_parser("2010-10-01")
#> [1] "date"
guess_parser("15:01")
#> [1] "time"
guess_parser(c("TRUE", "FALSE"))
#> [1] "logical"
guess_parser(c("1", "5", "9"))
#> [1] "integer"
guess_parser(c("12,352,561"))
#> [1] "number"

str(parse_guess("2010-10-10"))
#>  Date[1:1], format: "2010-10-10"
The heuristic tries each of the following types, stopping when it finds a match:

logical: contains only “F”, “T”, “FALSE”, or “TRUE”.
integer: contains only numeric characters (and -).
double: contains only valid doubles (including numbers like 4.5e-5).
number: contains valid doubles with the grouping mark inside.
time: matches the default time_format.
date: matches the default date_format.
date-time: any ISO8601 date.
If none of these rules apply, then the column will stay as a vector of strings.

11.4.2 Problems

These defaults don’t always work for larger files. There are two basic problems:

The first thousand rows might be a special case, and readr guesses a type that is not sufficiently general. For example, you might have a column of doubles that only contains integers in the first 1000 rows.

The column might contain a lot of missing values. If the first 1000 rows contain only NAs, readr will guess that it’s a character vector, whereas you probably want to parse it as something more specific.

readr contains a challenging CSV that illustrates both of these problems:

challenge <- read_csv(readr_example("challenge.csv"))
#> Parsed with column specification:
#> cols(
#>   x = col_integer(),
#>   y = col_character()
#> )
#> Warning: 1000 parsing failures.
#>  row col               expected             actual                                                 file
#> 1001   x no trailing characters .23837975086644292 '/home/travis/R/Library/readr/extdata/challenge.csv'
#> 1002   x no trailing characters .41167997173033655 '/home/travis/R/Library/readr/extdata/challenge.csv'
#> 1003   x no trailing characters .7460716762579978  '/home/travis/R/Library/readr/extdata/challenge.csv'
#> 1004   x no trailing characters .723450553836301   '/home/travis/R/Library/readr/extdata/challenge.csv'
#> 1005   x no trailing characters .614524137461558   '/home/travis/R/Library/readr/extdata/challenge.csv'
#> .... ... ...................... .................. ....................................................
#> See problems(...) for more details.
(Note the use of readr_example() which finds the path to one of the files included with the package)

There are two printed outputs: the column specification generated by looking at the first 1000 rows, and the first five parsing failures. It’s always a good idea to explicitly pull out the problems(), so you can explore them in more depth:

problems(challenge)
#> # A tibble: 1,000 × 5
#>     row   col               expected             actual
#>   <int> <chr>                  <chr>              <chr>
#> 1  1001     x no trailing characters .23837975086644292
#> 2  1002     x no trailing characters .41167997173033655
#> 3  1003     x no trailing characters  .7460716762579978
#> 4  1004     x no trailing characters   .723450553836301
#> 5  1005     x no trailing characters   .614524137461558
#> 6  1006     x no trailing characters   .473980569280684
#> # ... with 994 more rows, and 1 more variables: file <chr>
A good strategy is to work column by column until there are no problems remaining. Here we can see that there are a lot of parsing problems with the x column - there are trailing characters after the integer value. That suggests we need to use a double parser instead.

To fix the call, start by copying and pasting the column specification into your original call:

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_integer(),
    y = col_character()
  )
)
Then you can tweak the type of the x column:

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_character()
  )
)
That fixes the first problem, but if we look at the last few rows, you’ll see that they’re dates stored in a character vector:

tail(challenge)
#> # A tibble: 6 × 2
#>       x          y
#>   <dbl>      <chr>
#> 1 0.805 2019-11-21
#> 2 0.164 2018-03-29
#> 3 0.472 2014-08-04
#> 4 0.718 2015-08-16
#> 5 0.270 2020-02-04
#> 6 0.608 2019-01-06
You can fix that by specifying that y is a date column:

challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)
#> # A tibble: 6 × 2
#>       x          y
#>   <dbl>     <date>
#> 1 0.805 2019-11-21
#> 2 0.164 2018-03-29
#> 3 0.472 2014-08-04
#> 4 0.718 2015-08-16
#> 5 0.270 2020-02-04
#> 6 0.608 2019-01-06
Every parse_xyz() function has a corresponding col_xyz() function. You use parse_xyz() when the data is in a character vector in R already; you use col_xyz() when you want to tell readr how to load the data.

I highly recommend always supplying col_types, building up from the print-out provided by readr. This ensures that you have a consistent and reproducible data import script. If you rely on the default guesses and your data changes, readr will continue to read it in. If you want to be really strict, use stop_for_problems(): that will throw an error and stop your script if there are any parsing problems.

11.4.3 Other strategies

There are a few other general strategies to help you parse files:

In the previous example, we just got unlucky: if we look at just one more row than the default, we can correctly parse in one shot:

challenge2 <- read_csv(readr_example("challenge.csv"), guess_max = 1001)
#> Parsed with column specification:
#> cols(
#>   x = col_double(),
#>   y = col_date(format = "")
#> )
challenge2
#> # A tibble: 2,000 × 2
#>       x      y
#>   <dbl> <date>
#> 1   404   <NA>
#> 2  4172   <NA>
#> 3  3004   <NA>
#> 4   787   <NA>
#> 5    37   <NA>
#> 6  2332   <NA>
#> # ... with 1,994 more rows
Sometimes it’s easier to diagnose problems if you just read in all the columns as character vectors:

challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
This is particularly useful in conjunction with type_convert(), which applies the parsing heuristics to the character columns in a data frame.

df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
df
#> # A tibble: 3 × 2
#>       x     y
#>   <chr> <chr>
#> 1     1  1.21
#> 2     2  2.32
#> 3     3  4.56

# Note the column types
type_convert(df)
#> Parsed with column specification:
#> cols(
#>   x = col_integer(),
#>   y = col_double()
#> )
#> # A tibble: 3 × 2
#>       x     y
#>   <int> <dbl>
#> 1     1  1.21
#> 2     2  2.32
#> 3     3  4.56
If you’re reading a very large file, you might want to set n_max to a smallish number like 10,000 or 100,000. That will accelerate your iterations while you eliminate common problems.

If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with read_lines(), or even a character vector of length 1 with read_file(). Then you can use the string parsing skills you’ll learn later to parse more exotic formats.

11.5 Writing to a file

readr also comes with two useful functions for writing data back to disk: write_csv() and write_tsv(). Both functions increase the chances of the output file being read back in correctly by:

Always encoding strings in UTF-8.

Saving dates and date-times in ISO8601 format so they are easily parsed elsewhere.

If you want to export a csv file to Excel, use write_excel_csv() — this writes a special character (a “byte order mark”) at the start of the file which tells Excel that you’re using the UTF-8 encoding.

The most important arguments are x (the data frame to save), and path (the location to save it). You can also specify how missing values are written with na, and if you want to append to an existing file.

write_csv(challenge, "challenge.csv")
Note that the type information is lost when you save to csv:

challenge
#> # A tibble: 2,000 × 2
#>       x      y
#>   <dbl> <date>
#> 1   404   <NA>
#> 2  4172   <NA>
#> 3  3004   <NA>
#> 4   787   <NA>
#> 5    37   <NA>
#> 6  2332   <NA>
#> # ... with 1,994 more rows
write_csv(challenge, "challenge-2.csv")
read_csv("challenge-2.csv")
#> Parsed with column specification:
#> cols(
#>   x = col_integer(),
#>   y = col_character()
#> )
#> # A tibble: 2,000 × 2
#>       x     y
#>   <int> <chr>
#> 1   404  <NA>
#> 2  4172  <NA>
#> 3  3004  <NA>
#> 4   787  <NA>
#> 5    37  <NA>
#> 6  2332  <NA>
#> # ... with 1,994 more rows
This makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. There are two alternatives:

write_rds() and read_rds() are uniform wrappers around the base functions readRDS() and saveRDS(). These store data in R’s custom binary format called RDS:

write_rds(challenge, "challenge.rds")
read_rds("challenge.rds")
#> # A tibble: 2,000 × 2
#>       x      y
#>   <dbl> <date>
#> 1   404   <NA>
#> 2  4172   <NA>
#> 3  3004   <NA>
#> 4   787   <NA>
#> 5    37   <NA>
#> 6  2332   <NA>
#> # ... with 1,994 more rows
The feather package implements a fast binary file format that can be shared across programming languages:

library(feather)
write_feather(challenge, "challenge.feather")
read_feather("challenge.feather")
#> # A tibble: 2,000 x 2
#>       x      y
#>   <dbl> <date>
#> 1   404   <NA>
#> 2  4172   <NA>
#> 3  3004   <NA>
#> 4   787   <NA>
#> 5    37   <NA>
#> 6  2332   <NA>
#> # ... with 1,994 more rows
Feather tends to be faster than RDS and is usable outside of R. RDS supports list-columns (which you’ll learn about in many models); feather currently does not.

11.6 Other types of data

To get other types of data into R, we recommend starting with the tidyverse packages listed below. They’re certainly not perfect, but they are a good place to start. For rectangular data:

haven reads SPSS, Stata, and SAS files.

readxl reads excel files (both .xls and .xlsx).

DBI, along with a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a database and return a data frame.

For hierarchical data: use jsonlite (by Jeroen Ooms) for json, and xml2 for XML. Jenny Bryan has some excellent worked examples at https://jennybc.github.io/purrr-tutorial/.

For other file types, try the R data import/export manual and the rio package.

