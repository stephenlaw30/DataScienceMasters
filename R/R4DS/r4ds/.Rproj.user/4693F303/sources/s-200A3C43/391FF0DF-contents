---
title: "Ch11_DataImport"
author: "Steve Newns"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
```

# Intro

Load **flat files** in R as data frames w/ `readr` package (part of core tidyverse)

* `read_csv()` = comma delimited, `read_csv2()` = semicolon separated, `read_tsv()` = tab delimited, `read_delim()`  = any delimiter.
* `read_fwf()` = fixed width files
    * specify fields either by their widths w/ `fwf_widths()` or position w/ `fwf_positions()`. 
    * `read_table()` = reads common variation of fixed width files where columns are separated by white space.
* `read_log()` = Apache style log files. (also check `webreadr` = built on top of `read_log()` + provides many more helpful tools.)

Can also supply an **inline csv file** = useful for experimenting w/ `readr` + for creating reproducible examples to share w/ others

```{r}
read_csv("a,b,c
1,2,3
4,5,6")
```

`read_csv()` uses 1st line of data for column names (very common convention), + there are 2 cases where you might want to tweak this behaviour:

* 1) few lines of metadata at the top of the file --> `skip = n` to skip first n lines or use `comment = "#"` to drop all lines that start with (e.g.) #.

```{r}
read_csv("The first line of metadata
  The second line of metadata
  x,y,z
  1,2,3", 
  skip = 2)
```

```{r}
read_csv("# A comment I want to skip
  x,y,z
  1,2,3", comment = "#")
```

  * 2) Data might not have column names
    * use `col_names = FALSE` to tell read_csv() *not* to treat the 1st row as headings + instead label them sequentially from X1 to Xn:
    * Or can pass `col_names` a character vector to be used as the column names:

```{r}
read_csv("1,2,3\n4,5,6", col_names = FALSE)
read_csv("1,2,3
         4,5,6", col_names = FALSE)
read_csv("1,2,3\n4,5,6", col_names = c("x","y","z"))

```

Another option that commonly needs tweaking = `na` = specifies value(s) used to represent missing values in a file:

```{r}
read_csv("a,b,c\n1,2,.", na = ".")
```

To read in more challenging files, learn more about how `readr` **parses** each column, turning them into R **vectors**

# Compared to base R

`readr` functions are typically much faster (~10x) than base equivalents. 

* If looking for raw speed, try `data.table::fread()` = doesn’t fit quite so well into tidyverse, but can be quite a bit faster.

`readr` produces tibbles, don’t convert character vectors to factors, use row names, or munge column names (common sources of frustration w/ base R)

`readr`  = more reproducible b/c Base R functions inherit some behaviour from your OS + environment variables, so import code that works on 1 CPU might not work on another

## 11.2.2 Exercises

**What function would you use to read a file where fields were separated with “|”?**

```{r}
read_delim("a|b|c\n1|2|3", delim = "|")
```

**Apart from file, skip, and comment, what other arguments do ?read_csv() and ?read_tsv() have in common?**

* *All arguments are the same*

**What are the most important arguments to read_fwf()?**

* *file, col_position, skip, col_types*

**Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character. By convention, `read_csv()` assumes the quoting character will be ", + to change it you’ll need to use `read_delim()` instead. What arguments do you need to specify to read the following text into a data frame?**

```{r}
read_delim("x,y\n1,'a,b'", quote = "\'", delim = ",")
```

**ID what is wrong with each of the following inline CSV files. What happens when you run the code?**


  * read_csv("a,b\n1,2,3\n4,5,6") = only 2 cols, but then 3 values given
  * read_csv("a,b,c\n1,2\n1,2,3,4") = only 2 values given in 1st row after headers so last value = missing, final value in 2nd row dropped
  * read_csv("a,b\n\"1") = 1 should be a character (?) but it results in an int due 
  * read_csv("a,b\n1,2\na,b") = ?
  * read_csv("a;b\n1;3") = no specification that the delimiter is ";"

# 11.3 Parsing a vector

`parse_*()` functions take a character vector + return a more specialised vector like a logical, integer, or date:

```{r}
str(parse_logical(c("TRUE", "FALSE", "NA")))
str(parse_integer(c("1", "2", "3")))
str(parse_date(c("2010-01-01", "1979-10-14")))
```

These functions are useful in their own right, but are also important building blocks for `readr`.

Like all functions in tidyverse, `parse_*()` functions = **uniform**: 1st argument = a character vector to parse, + `na` argument specifies *which* strings should be treated as missing:

```{r}
parse_integer(c("1", "231", ".", "456"), na = ".")
```

If parsing fails, you’ll get a warning + the failures will be missing in the output:

```{r}
(x <- parse_integer(c("123", "345", "abc", "123.45")))
```

If there *are* many parsing failures, use `problems()` to get the complete set as a tibble, which we can manipulate w/ `dplyr`.

```{r}
problems(x)
```

Using parsers is mostly a matter of understanding what’s available + how they deal w/ different types of input.

8 particularly important parsers:

* `parse_logical()` + `parse_integer()` = basically nothing can go wrong w/ these parsers
* `parse_double()` = *strict* numeric parser
* `parse_number()` = *flexible* numeric parser (more complicated than you might expect b/c different parts of the world write #'s in different ways()
* `parse_character()` = 1 complication makes it quite important: **character encodings**
* `parse_factor()` = creates factors, the data structure that R uses to represent categorical variables with fixed and known values.
`parse_datetime()`, `parse_date()`, `parse_time()` = parse various date & time specifications = most complicated b/c there are so many different ways of writing dates

# 11.3.1 Numbers

Issues w/ #'s:

* 1) People write #'s differently in different parts of the world (some countries use . in between integer + fractional parts of a real number, while others use ,.)
* 2) Numbers often surrounded by other characters that provide some context, like “$1000” or “10%”.
* 3) Numbers often contain "grouping” characters to make easier to read, like “1,000,000”, + these vary around the world.

To address the 1st problem, `readr` has a **locale** arg = an object that specifies parsing options that differ from place to place. 
* When parsing #'s the most important option = character used for the decimal
* Can override default value of `.` by creating a *new* locale + setting the `decimal_mark` argument:

```{r}
parse_double("1.23")
parse_double("1,23", locale = locale(decimal_mark = ","))
```
readr’s default locale = US-centric, b/c generally R is US-centric (documentation of base R = written in American English). 
Alternative approach = try + guess defaults from your OS (hard to do well + more importantly, makes code fragile)

* even if it works on 1 CPU, it might fail when emailed it to a colleague in another country.

`parse_number()` addresses the 2nd problem = ignores non-numeric characters before + after the # (particularly useful for currencies + %'s, but also works to extract numbers embedded in text)

```{r}
parse_number("$100")
parse_number("20%")
parse_number("It cost $123.45")
```

Final problem is addressed by a combo of `parse_number()` + `locale` w/in `parse_number()` that'll ignore the “grouping mark”:

```{r}
# Used in America
parse_number("$123,456,789")
# Used in many parts of Europe
parse_number("123.456.789", locale = locale(grouping_mark = "."))
#> [1] 1.23e+08
# Used in Switzerland
parse_number("123'456'789", locale = locale(grouping_mark = "'"))
```

# 11.3.2 Strings

There're multiple ways to represent the same string --> need to dive into details of how CPUs represent strings --> In R, look @ underlying representation of a string using `charToRaw()`:

```{r}
charToRaw("Hadley")
```

Each **hexadecimal #** represents a byte of info: 48 = H, 61 = a, + so on. 

The **mapping** from hexadecimal # to character = the **encoding**, + in this case the encoding = **ASCII**, which does a great job of representing English characters, b/c ASCII = **American Standard Code for Information Interchange**.

Things get more complicated for languages other than English. In early days of CPUs = many competing standards for encoding non-English characters + to correctly interpret a string you needed to know *both* the values *and* the encoding. 

* Ex: 2 common encodings = **Latin1** (aka ISO-8859-1 = Western European languages) + **Latin2** (aka ISO-8859-2 = Eastern European languages). 
* In **Latin1**, byte b1 = “±”, but in **Latin2**, b1 = “ą”!

Fortunately, today there is 1 standard supported almost everywhere: **UTF-** = can encode just about every character used by humans today, as well as many extra symbols (like emoji)

`readr` uses **UTF-8** everywhere = assumes data is UTF-8 encoded when read + always uses it when writing. 

This is a good default but will fail for data produced by older systems that don’t understand UTF-8. 
If this happens, strings will look weird when printed = Sometimes just 1 or 2 chars might be messed up; other times it's complete gibberish. 

```{r}
x1 <- "El Ni\xf1o was particularly bad this year"
x2 <- "\x82\xb1\x82\xf1\x82\xc9\x82\xbf\x82\xcd"

#x1
#> [1] "El Ni\xf1o was particularly bad this year"
#x2
#> [1] "\x82\xb1\x82\xf1\x82ɂ\xbf\x82\xcd"
```

To fix the problem, specify the encoding in `parse_character()`:

```{r}
parse_character(x1, locale = locale(encoding = "Latin1"))
parse_character(x2, locale = locale(encoding = "Shift-JIS"))
```

How do you find the correct encoding? If lucky, it’ll be included somewhere in the data documentation

* Rarely the case, so `readr` provides `guess_encoding()` to help figure it out
    * Not foolproof + it works better w/ lots of text (unlike here), but it’s a reasonable place to start. 
* Expect to try a few different encodings before finding the right one.
* 1st arg to `guess_encoding()` = either a path to a file or a raw vector (useful if the strings are already in R).

Encodings = rich + complex topic + to learn, read detailed explanation at http://kunststube.net/encoding/.

```{r}
guess_encoding(charToRaw(x1))
guess_encoding(charToRaw(x2))
```

# 11.3.3 Factors

R uses **factors** to represent **categorical variables** w/ a known set of possible values. Give `parse_factor()` a vector of known levels to generate a warning whenever an unexpected value is present:

```{r}
fruit <- c("apple", "banana")
parse_factor(c("apple", "banana", "bananana"), levels = fruit)
```

If you have many problematic entries, it’s often easier to leave them as character vectors _ then use the tools for strings + factors to clean them up.

# 11.3.4 Dates, date-times, and times

3 parsers depending on whether you want a date (# of days since 1970-01-01), date-time (# of secs since midnight 1970-01-01), or a time (# of secs since midnight). 

When called w/out any additional arguments:

* `parse_datetime()` expects an **ISO8601 date-time** = an international standard in which components of a date are organised from biggest to smallest --> Y M D H M S

* ISO8601 = most important date/time standard + if you work w/ dates + times frequently, read https://en.wikipedia.org/wiki/ISO_8601

```{r}
parse_datetime("2010-10-01T2010")
# If time is omitted, it will be set to midnight
parse_datetime("20101010")
```

`parse_date()` expects a 4 digit year, a `-` or `/`, the month, a `-` or `/`, then day:

```{r}
parse_date("2010-10-01")
```

`parse_time()` expects hour, `:`, minutes, + optionally `:` + `seconds`, + an optional am/pm specifier:

```{r}
library(hms)
parse_time("01:10 am")
parse_time("20:10:01")
```

Base R doesn’t have a great built-in class for time data, so we use the one provided in the `hms` package.

If these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces:

### Year
  * `%Y` (4 digits).
  * `%y` (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999.
  
### Month
  * `%m` (2 digits).
  * `%b` (abbreviated name, like “Jan”).
  * `%B` (full name, “January”)
  
### Day
  * `%d` (2 digits).
  * `%e` (optional leading space)
  
### Time
  * `%H` 0-23 hour
  * `%I` 0-12, must be used with %p
  * `%p` AM/PM indicator
  * `%M` minutes
  * `%S` integer seconds
  * `%OS` real seconds
  * `%Z` Time zone (as name, e.g. America/Chicago)
    * Beware of abbreviations: if American, note “EST” = a *Canadian* time zone that does *NOT* have daylight savings time
  * `%z` (as offset from UTC, e.g. +0800)
  
### Non-digits
  * `%.` skips one non-digit character
  * `%*` skips any number of non-digits

*Best way to figure out correct format* = create a few examples in a character vector + test w/ 1 of the parsing functions

```{r}
parse_date("01/02/15", "%m/%d/%y")
parse_date("01/02/15", "%d/%m/%y")
parse_date("01/02/15", "%y/%m/%d")
```

If using `%b` or `%B` w/ *non-English month names*, you need to set `lang` argument to `locale()`. 

* See list of built-in languages in `date_names_langs()`, or if a language is not already included, create your own w/ `date_names()`.

```{r}
parse_date("1 janvier 2015", "%d %B %Y", locale = locale("fr"))
```

## 11.3.5 Exercises

**What are the most important arguments to ?locale()?**

* encoding + decimal mark + grouping mark, date_names, tz

**What happens if you try + set decimal_mark + grouping_mark to the same character?**

* It gives an error that they must be different

**What happens to the default value of grouping_mark when you set decimal_mark to “,”?** 

```{r}
parse_number("123.456,789", locale = locale(decimal_mark = ","))
```

* It switches the default grouping marker from `,` to `.`

**What happens to the default value of decimal_mark when you set the grouping_mark to “.”?**

```{r}
parse_number("123.456,789", locale = locale(grouping_mark = "."))
```

* It switches the default decimal marker from `.` to `,`

**Didn’t discuss `date_format` and `time_format` options to locale(). What do they do? Construct an example that shows when they might be useful.**

* They default values for date (%Y-%m-%d) and time which are used when guessing col types and parsing datetimes, respectively. They'd be helpful to parse data that is ingested in a "non-standard" format.

**If you live outside the US, create a new locale object that encapsulates the settings for the types of file you read most commonly**

```{r}
parse_character("Bonjour", locale = locale("fr"))
```

**What’s the difference between read_csv() and read_csv2()?**

* read_csv() is comma delimited, read_csv2() is semicolon delimited

**What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out.**

* Europe = Latin I, Greek (all), ISO 8859-1, ISO 8859-15 and cp1252 (western), cp1250, ISO-8859-2 (eastern)
* Asia = GB2312 (China), EUC-KR (Korea), cp1251, KOI8-R (Cyrillic),.

**Generate the correct format string to parse each of the following dates and times:**

```{r}
d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"

parse_date(d1, format = "%B %d, %Y")
parse_date(d2, format = "%Y-%b-%d")
parse_date(d3, format = "%d-%b-%Y")
parse_date(d4, format = "%B %d (%Y)")
parse_date(d5, format = "%m/%d/%y")
parse_time(t1, format = "%H%M")
parse_time(t2, format = "%I:%M:%OS %p")
```
# 11.4 Parsing a file

Now that you’ve learned how to parse an individual vector, it’s time to return to the beginning and explore how readr parses a *file*. 

2 new things to learn about:

* How readr automatically guesses type of each column.
* How to override default specification.

## 11.4.1 Strategy

readr reads 1st 1K rows + uses some (moderately conservative) **heuristics** to figure out each column type 
* Can emulate this process w/ a character vector using `guess_parser()` = returns readr’s best guess, + `parse_guess()` which uses that guess to parse the column:

```{r}
guess_parser("2010-10-01")
guess_parser("15:01")
guess_parser(c("TRUE", "FALSE"))
guess_parser(c("1", "5", "9"))
guess_parser(c("12,352,561"))

str(parse_guess("2010-10-10"))
```
The heuristic tries each of the following types, stopping when it finds a match:

<ul> **logical**: contains only “F”, “T”, “FALSE”, or “TRUE”. </ul>
<ul> **integer**: contains only numeric characters (and -). </ul>
<ul> **double**: contains only valid doubles (including #'s like 4.5e-5). </ul>
<ul> **number**: contains valid doubles w/ grouping mark inside. </ul>
<ul> **time**: matches *default* time_format. </ul>
<ul> **date**: matches *default* date_format. </ul>
<ul> **date-time**: *any* ISO8601 date. </ul>

If *none* of these rules apply, the col stays as a vector of strings.

## 11.4.2 Problems

Defaults don’t always work for larger files --> 2 basic problems:

<li> 1sts 1K rows might be a special case + `readr` guesses a type that is not sufficiently general.</li>
    <li> Ex: Might have a col of doubles that only contains INTs in the 1st 1k rows. </li>
<li> A col might contain a lot of missing values + if the 1st 1K rows contain *only* NAs, `readr` guesses it’s a character vector, whereas you probably want to parse it as something more specific. </li>

`readr` contains a challenging CSV that illustrates both of these problems:

```{r}
challenge <- read_csv(readr_example("challenge.csv"))
problems(read_csv(readr_example("challenge.csv")))
```

<li> `readr_example()` finds the path to 1 of the files included w/ the package </li>

See 2 printed outputs: col specification generated by looking at the 1st 1K rows + the 1st 5 parsing failures. 

It’s always a good idea to explicitly pull out the `problems()` so you can explore them in more depth:

A good strateg = work col by col until there are no problems remaining. Here there are a lot of parsing problems w/ the `x` column = there are trailing characters after the INT value, which suggests we need to use a **double parser** instead.

To fix the call, use a column specification in the call:

```{r}
challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_integer(),
    y = col_character()
  )
)
```

Then tweak the type of the x column:

```{r}
challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_character()
  )
)
```

That fixes the 1st problem, but if we look at the last few rows, see they’re *dates* stored in a *character* vector:

```{r}
tail(challenge)
```

Fix by specifying `y` is a `date` column:

```{r}
challenge <- read_csv(
  readr_example("challenge.csv"), 
  col_types = cols(
    x = col_double(),
    y = col_date()
  )
)
tail(challenge)
```

Every `parse_xyz()` function has a corresponding **`col_xyz()`** function.

<li> Use `parse_xyz()` when data is in a *character* vector in R already </li>
<li> Use `col_xyz()` when you want to tell readr how to load the data. </li>

Highly recommended to alway supply `col_types`, building up from the print-out provided by `readr.` 

<li> Ensures you have a consistent + reproducible data import script. </li>
<li> If you rely on default guesses + then the data changes, `readr` will continue to read it in. 
<li> If you want to be *really* strict, use `stop_for_problems()` to throw an error + stop a script if there are any parsing problems.

## 11.4.3 Other strategies

There are a few other general strategies to help parse files:

In the previous example, we were unlucky: if we looked at just *1 more row* than the default = 1K, we can correctly parse in 1 shot:

```{r}
challenge2 <- read_csv(readr_example("challenge.csv"), guess_max = 1001)
challenge2
```

Sometimes it’s easier to diagnose problems if you just read in all cols as character vectors:

```{r}
challenge2 <- read_csv(readr_example("challenge.csv"), 
  col_types = cols(.default = col_character())
)
challenge2
```

This is particularly useful in conjunction w/ `type_convert()`, which applies the parsing heuristics to the character cols in a data frame.

```{r}
df <- tribble(
  ~x,  ~y,
  "1", "1.21",
  "2", "2.32",
  "3", "4.56"
)
df

# Note the column types
type_convert(df)
```

If reading a very large file, might want to set `n_max` to a smallish # like 10k or 100k to accelerate iterations while you eliminate common problems.

If having major parsing problems, sometimes it’s easier to just read into a character vector of lines w/ `read_lines()`, or even a character vector of length 1 w/ `read_file()`, + then you can use string parsing skills to parse more exotic formats.

## 11.5 Writing to a file

`readr` also comes w/ 2 useful functions for **writing** data back to disk: `write_csv()` + `write_tsv()`. 

Both functions increase chances of the output file being read back in correctly by:

<li> Always encoding strings in UTF-8 </li> 
<li> Saving dates + date-times in ISO8601 format so they're easily parsed elsewhere. </li> 

If you want to export a CSV to Excel, use `write_excel_csv()` = writes a special character (a **byte order mark**) at the start of the file which tells Excel you’re using the UTF-8 encoding.

The most important arguments = `x` (data frame to save) + `path` (location to save it)
<li> can also specify how missing values are written w/ `na` + if you want to append to an existing file. </li> 

```{r}
write_csv(challenge, "challenge.csv")
```

Note that type info is lost when you save to csv:
```{r}
challenge
write_csv(challenge, "challenge-2.csv")
read_csv("challenge-2.csv")
```

This makes CSVs a little unreliable for caching interim results --> you need to recreate the col specification every time you load in. 

There are 2 alternatives:

<li> `write_rds()` + `read_rds()` = uniform wrappers around base functions `readRDS()` and `saveRDS()`. </li>  
<li> These store data in R’s custom binary format called RDS:</li>  

```{r}
write_rds(challenge, "challenge.rds")
read_rds("challenge.rds")
```
The `feather` package implements a fast binary file format that can be shared across programming languages:

```{r}
library(feather)
write_feather(challenge, "challenge.feather")
read_feather("challenge.feather")
```

`Feather` tends to be faster than RDS + is usable outside of R, but`RDS` supports list-columns, while `feather` currently does not.

## 11.6 Other types of data

To get other types of data into R, recommended to start w/ `tidyverse` packages (certainly not perfect, but are a good place to start) 

<li> For rectangular data: </li>
  <li> `haven` reads SPSS, Stata, + SAS files. </li>
  <li> `readxl` reads excel files (both .xls and .xlsx). </li>
  <li> `DBI`, along w/ a database specific backend (e.g. RMySQL, RSQLite, RPostgreSQL etc) allows you to run SQL queries against a DB + return a data frame. </li>
<li> For hierarchical data: 
  <li> use `jsonlite` (by Jeroen Ooms) for json
  <li> use `xml2` for XML (Jenny Bryan has excellent worked examples @  https://jennybc.github.io/purrr-tutorial/)
<li> For other file types, try the R data import/export manual and the `rio` package.