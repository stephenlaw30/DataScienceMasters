---
title: "R4DS_Ch19_Functions"
author: "Steve Newns"
date: "Nov 2, 2017"
output: html_document
---

1 of the best ways to improve reach as a data scientist = write functions = allow you to automate common tasks in a more powerful + general way than copy-and-pasting w/ 3 big advantages over using copy-and-paste:
<ul>
<li> Can give a function an evocative name to makes code easier to understand. </li>
<li> As requirements change, only need to update code in 1 place. </li>
<li> Eliminate chance of making incidental mistakes when copy + pasting (i.e. updating a variable name in 1 place, but not in another). </li>
</ul>

Writing good functions = a lifetime journey + good code style = like correct punctuation, Youcanmanagewithoutit, but it sure makes things easier to read

# 19.2 When should you write a function?

Consider writing a function whenever you’ve copied + pasted a block of code more than twice (3 copies of the same code)

```{r}
df <- tibble::tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)

df$a <- (df$a - min(df$a, na.rm = TRUE)) / 
  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))
df$b <- (df$b - min(df$b, na.rm = TRUE)) / 
  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))
df$c <- (df$c - min(df$c, na.rm = TRUE)) / 
  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))
df$d <- (df$d - min(df$d, na.rm = TRUE)) / 
  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))
```
This rescales each column to have a range from 0-1 but w/ the mistake when copying-and-pasting the code for df$b = forgot to change an `a` to a `b` ==> functions prevent this type of mistake.

This code only has 1 input: `df$letter`. To make the inputs more clear, rewrite code using temp variables w/ general names
```{r}
x <- df$a
(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
```
There is some duplication in this code ==> computing the range of the data 3 times, but it makes sense to do it in 1 step:
```{r}
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
```
Pulling out intermediate calculations into named variables = good practice b/c it makes it more clear what code is doing. 

Now that I’ve simplified the code + checked that it still works, turn it into a function:
```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE) # get range of input
  (x - rng[1]) / (rng[2] - rng[1]) # scale input to scale 0-1
}
rescale01(c(0, 5, 10))
```

3 key steps to creating a new function:
<ul>
<li> Pick a name  <li>
<li> List inputs/arguments to the function inside `function` <li>
<li> Place the developed code in body of the function <li>
<ul>

Iverall process = only made the function after figuring out how to make it work w/ a simple input = easier to start w/ *working* code + turn it into a function = harder to create a function + then try to make it work.

Check your function with a few different inputs:
```{r}
rescale01(c(-10, 0, 10))
rescale01(c(1, 2, 3, NA, 5))
```
As you write more and more functions you’ll eventually want to convert these informal, interactive tests into formal, automated tests = **unit testing** (learn about in http://r-pkgs.had.co.nz/tests.html)

We can simplify the original example now that we have a function:
```{r}
df$a <- rescale01(df$a)
df$b <- rescale01(df$b)
df$c <- rescale01(df$c)
df$d <- rescale01(df$d)
```

Compared to the original, this code = easier to understand + we’ve eliminated 1 class of copy-and-paste errors. There's still quite a bit of duplication (doing the same thing to multiple column --> eliminated w/ **iteration** (Ch. 21)

Another advantage of functions = if our requirements change, only need to make the change in 1 place

Might discover some of our variables include infinite values + `rescale01()` fails:
```{r}
x <- c(1:10, Inf)
rescale01(x)
```
B/c we’ve extracted the code into a function, we only need to make the fix in 1 place:
```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE) # omit all non-finite values
  (x - rng[1]) / (rng[2] - rng[1])
}
rescale01(x)
```

This is an important part of the **“do not repeat yourself”** (or **DRY**) **principle**. More repetition in code = more places you need to remember to update when things change (they always do) = more likely to create bugs over time.

## 19.2.1 Practice

**Why is TRUE not a parameter to rescale01()? What would happen if x contained a single missing value, and na.rm was FALSE?**

**In the second variant of rescale01(), infinite values are left unchanged. Rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1.**
```{r}
x <- c(1:10, Inf)

#rescale02 <- function(x) {
  x <- for (i in x) {
        if_else(i == 2, 1,
              if_else(i == -3, 0, i))
        } 
  rng <- range(x, na.rm = TRUE, finite = TRUE) # omit all non-finite values
  (x - rng[1]) / (rng[2] - rng[1])
#}

#rescale02(x)
```

**Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? Can you rewrite it to be more expressive or less duplicative?**
```{r}
x <- rnorm(50)
mean(is.na(x))
```
x / sum(x, na.rm = TRUE)

sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)
Follow http://nicercode.github.io/intro/writing-functions.html to write your own functions to compute the variance and skew of a numeric vector.

Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.

What do the following functions do? Why are they useful even though they are so short?

is_directory <- function(x) file.info(x)$isdir
is_readable <- function(x) file.access(x, 4) == 0
Read the complete lyrics to “Little Bunny Foo Foo”. There’s a lot of duplication in this song. Extend the initial piping example to recreate the complete song, and use functions to reduce the duplication.

# 19.3 Functions are for humans and computers

Functions != just for the CPU, but are also for humans = what a function is called, what comments it contains,

Ideally, name of a function = short but clearly evoke what it does (But better to be clear than short b/c RStudio’s autocomplete makes it easy to type long names)

Generally, function names should be verbs + arguments should be nouns, w/ some exceptions:
<ul>
<li> nouns are ok if function computes a very well known noun (mean() = better than compute_mean())</li>
<li> nouns are ok if accessing some property of an object (coef() = better than get_coefficients()). </li>
<li> Good sign a noun might be a better choice = you’re using a very broad verb like “get”, “compute”, “calculate”, or “determine”. </li>
<li> Use best judgement + don’t be afraid to rename a function if you figure out a better name later. </li>
</ul>

If a function name is composed of multiple words, use **snake_case** = each lowercase word is separated by an underscore. **camelCase** = a popular alternative. Important thing = **be consistent**, pick 1 or the other + stick w/ it. 

For a **family** of functions that do similar things, make sure they have consistent names + arguments. Use a common prefix to indicate they're connected (better than a common suffix b/c autocomplete allows you to type the prefix + see all members of a family.)

A good example of this design is `stringr` = functions start w/ `str_`

Use **comments** to explain the “why” of code, but generally avoid comments that explain the “what” or “how”.

If you can’t understand what code does from reading it, think about how to rewrite it to be more clear, like add some intermediate variables w/ useful names?, break out a subcomponent of a large function to name it, etc.

Code can never capture the *reasoning* behind decisions (why you choose 1 approach instead of another? What else did you try that didn’t work?) --> great idea to capture that sort of thinking in a comment.

Another important use of comments = break up a file into easily readable chunks. 

Use long lines of - and = to make it easy to spot the breaks.
```{r}
# Load data --------------------------------------

# Plot data --------------------------------------
```
RStudio provides a keyboard shortcut to create these headers = **Cmd/Ctrl + Shift + R** + will display them in the code navigation drop-down at the bottom-left of the editor

# 19.3.1 Exercises

**Read the source code for each of the following three functions, puzzle out what they do, + brainstorm better names.**
```{r}
f1 <- function(string, prefix) {
  substr(string, 1, nchar(prefix)) == prefix 
}
# takes a string, removes # of chars = length of prefix, checks if the prefix matches given prefix
check_prefix <- function(string, prefix) {
  substr(string, 1, nchar(prefix)) == prefix 
}
```
```{r}
f2 <- function(x) {
  if (length(x) <= 1) return(NULL)
  x[-length(x)]
}
# if a given object's length is less than 1, return NULL
# if not, return all items in x except the last
chop_last <- function(x) {
  if (length(x) <= 1) return(NULL)
  x[-length(x)]
}
```
```{r}
f3 <- function(x, y) {
  rep(y, length.out = length(x))
}
# repeat 7 as many times as the length of x
repeat_y <- function(x,y) {
  rep(y, length.out = length(x))
}
```

# Compare and contrast rnorm() and MASS::mvrnorm(). How could you make them more consistent?
```{r}
#?rnorm
#?MASS::mvrnorm
```
We could rename the second and third arguments, which are mean and standard deviation and are represented as `mean` and `mu` and then `sd` and `Sigma`, repectively.

#Make a case for why norm_r(), norm_d() etc would be better than rnorm(), dnorm(). Make a case for the opposite.

For the 1st situation, it makes it easier to find w/ RStudio's autocomplete. For the latter, it's easier if we know exaclty what function we are trying to perform (density, random generation, quantile, p-values)

# 19.4 Conditional execution

`if` = conditionally execute code.

```{r}
# return a logical vector describing whether or not each element of a vector is named.
has_name <- function(x) {
  nms <- names(x)
  if (is.null(nms)) {
    rep(FALSE, length(x))
  } else {
    !is.na(nms) & nms != ""
  }
}
has_name(c("hi",1,3,4,"bye"))
```
This takes advantage of the standard **return rule**: a function returns the *last value it computed* = either 1 of the 2 branches of the `if` statement.

## 19.4.1 Conditions

The **condition** must evaluate to either TRUE or FALSE. If it’s a vector, you’ll get a warning message; if it’s an NA, you’ll get an error
```{r}
#if (c(TRUE, FALSE)) {}
##> Warning in if (c(TRUE, FALSE)) {: the condition has length > 1 and only the first element will be used
##> NULL

#if (NA) {}
##> Error in if (NA) {: missing value where TRUE/FALSE needed
```
can use `||` (or) and `&&` (and) to combine multiple logical expressions. 

These operators are **short-circuiting** = as soon as `||` sees the 1st `TRUE` it returns `TRUE` *without computing anything else*. 
As soon as `&&` sees the 1st `FALSE` it returns `FALSE`. 

Never use `|` or `&` in an `if` statement b/c these are **vectorised operations** that apply to *multiple* values (that’s why you use them in `filter`). If you *do* have a logical vector, use `any()` or `all()` to **collapse** it to a single value.

Be careful when testing for equality b/c `==` is **vectorised** + it’s easy to get more than 1 output. Either check that `length` is already 1, **collapse** w/ `all` or `any`, or use the non-vectorised `identical` (very strict + always returns either a single `TRUE` or a single `FALSE` +  doesn’t coerce types = be careful when comparing integers and doubles):
```{r}
identical(0L, 0)
```
Also be wary of floating point numbers:
```{r}
x <- sqrt(2) ^ 2
x == 2
x - 2 # expect to be 0
```
Instead use `dplyr::near` for comparisons, as described in comparisons.

And remember, **x == NA doesn’t do anything useful**

## 19.4.2 Multiple conditions

Can chain multiple if statements together. But for a very long series of chained `if`'s, consider rewriting. 1 useful technique = `switch` = evaluate selected code based on position or name.
```{r}
switch_math <- function(x, y, op) {
  switch(op,
         plus = x + y,
         minus = x - y,
         times = x * y,
         divide = x / y,
         stop("Unknown op!")
  )
}
```
Another useful function that can often eliminate long chains of `if` = `cut()` = used to **discretise** continuous variables.

## 19.4.3 Code style

An opening curly brace should never go on its own line + should always be followed by a new line. A closing curly brace should always go on its own line, unless followed by `else`.

It’s ok to drop the curly braces if you have a very short if statement that can fit on one line:
```{r}
y <- 10
x <- if (y < 20) "Too low" else "Too high"

# same thing, long-form
if (y < 20) {
  x <- "Too low" 
} else {
  x <- "Too high"
}
```

# 19.4.4 Exercises

**What’s the difference between if and ifelse()? Carefully read the help and construct three examples that illustrate the key differences.**

The true/false return expressions for ifelse are w/in the parenthesis, not curly brackets.
```{r}
if (9 > 10) { 
  print(10)
} else {
    print(20)
}
ifelse(9 > 10, 10, 20)

if ("maybe" == "maybe") { 
  print("match")
} else {
    print("no")
}
ifelse("maybe`" == "maybe","match", "no")
```

**Write a greeting function that says “good morning”, “good afternoon”, or “good evening”, depending on the time of day. (Hint: use a time argument that defaults to lubridate::now(). That will make it easier to test your function.)**
```{r}
greet <- function(time) {
  if (hour(time) < 11) {
    print("good morning")
  } else if (hour(time) < 16) {
    print("good afternoon")
  } else {
    print("good evening")
  }
}
greet(lubridate::now()+10)
greet(lubridate::now()+18000)
```

**Implement a fizzbuzz function. It takes a single number as input. If the number is divisible by three, it returns “fizz”. If it’s divisible by five it returns “buzz”. If it’s divisible by three and five, it returns “fizzbuzz”. Otherwise, it returns the number. Make sure you first write working code before you create the function.**
```{r}
fizzbuzz <- function(x) {
  if (x %% 3 == 0 && x %% 5 == 0) {
    print("fizzbuzz") 
  } else if (x %% 3 == 0) {
    print("fizz")
  } else if (x %% 5 == 0) {
    print("buzz")
  }
}
fizzbuzz(6)
fizzbuzz(10)
fizzbuzz(15)
```
**How could you use cut() to simplify this set of nested if-else statements?**
```{r}
temp <- 0
if (temp <= 0) {
  "freezing"
} else if (temp <= 10) {
  "cold"
} else if (temp <= 20) {
  "cool"
} else if (temp <= 30) {
  "warm"
} else {
  "hot"
}
seq(10,100,1)
cut(seq(10,100,1),10)
```
How would you change the call to cut() if I’d used < instead of <=? What is the other chief advantage of cut() for this problem? (Hint: what happens if you have many values in temp?)

What happens if you use switch() with numeric values?

What does this switch() call do? What happens if x is “e”?

switch(x, 
  a = ,
  b = "ab",
  c = ,
  d = "cd"
)
Experiment, then carefully read the documentation.

# 19.5 Function arguments

Arguments to a function typically fall into 2 broad sets: 1 set supplies data to compute on + the other supplies arguments that control the details of the computation. For example:
<ul>
<li> In `log()`, data = `x`, and the detail = the `base` of the logarithm. </li>
<li> In `mean()`, data = `x`, and details = how much data to trim from the ends (`trim`) + how to handle missing values (`na.rm`). </li>
<li> In `t.test()`, data are `x` + `y`, and details of the test = `alternative`, `mu`, `paired`, `var.equal`, and `conf.level`. </li>
<li> In `str_c()`, you can supply any # of strings to `...`, + the details of the concatenation are controlled by `sep` + `collapse`. </li>
</ul>

Generally, data arguments should come 1st + detail arguments should go on the end w/ default values (specify a default value in the same way you call a function with a named argument)
```{r}
# Compute confidence interval around mean using normal approximation
mean_ci <- function(x, conf = 0.95) {
  se <- sd(x) / sqrt(length(x))
  alpha <- 1 - conf
  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))
}

x <- runif(100)
mean_ci(x)
mean_ci(x, conf = 0.99)
```
The default value should almost always be the most *common* value. 

The few exceptions to this rule are to do w/ safety (ex: makes sense for `na.rm` to default to `FALSE` b/c missing values are important. Even though `na.rm = TRUE` is what you usually put in code, it’s a bad idea to silently ignore missing values by default)

When you call a function, you typically omit the names of the data arguments, b/c they are used so commonly. If you override the default value of a detail argument, use the full name:
```{r}
# Good
mean(1:10, na.rm = TRUE)

# Bad
mean(x = 1:10, , FALSE)
mean(, TRUE, x = c(1:10, NA))
```
Can refer to an argument by its unique prefix (e.g. mean(x, n = TRUE)), but this is generally best avoided given the possibilities for confusion.

Using whitespace makes it easier to skim the function for the important components.
```{r}
# Good
average <- mean(feet / 12 + inches, na.rm = TRUE)

# Bad
average<-mean(feet/12+inches,na.rm=TRUE)
```

## 19.5.1 Choosing names

The names of the arguments are also important. R doesn’t care, but the readers of your code (including future-you!) will. Generally you should prefer longer, more descriptive names, but there are a handful of very common, very short names. It’s worth memorising these:

x, y, z: vectors.
w: a vector of weights.
df: a data frame.
i, j: numeric indices (typically rows and columns).
n: length, or number of rows.
p: number of columns.
Otherwise, consider matching names of arguments in existing R functions. For example, use na.rm to determine if missing values should be removed.

## 19.5.2 Checking values

As you start to write more functions, you’ll eventually get to the point where you don’t remember exactly how your function works. At this point it’s easy to call your function with invalid inputs. To avoid this problem, it’s often useful to make constraints explicit. For example, imagine you’ve written some functions for computing weighted summary statistics:

wt_mean <- function(x, w) {
  sum(x * w) / sum(w)
}
wt_var <- function(x, w) {
  mu <- wt_mean(x, w)
  sum(w * (x - mu) ^ 2) / sum(w)
}
wt_sd <- function(x, w) {
  sqrt(wt_var(x, w))
}
What happens if x and w are not the same length?

wt_mean(1:6, 1:3)
#> [1] 7.67
In this case, because of R’s vector recycling rules, we don’t get an error.

It’s good practice to check important preconditions, and throw an error (with stop()), if they are not true:

wt_mean <- function(x, w) {
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  sum(w * x) / sum(w)
}
Be careful not to take this too far. There’s a tradeoff between how much time you spend making your function robust, versus how long you spend writing it. For example, if you also added a na.rm argument, I probably wouldn’t check it carefully:

wt_mean <- function(x, w, na.rm = FALSE) {
  if (!is.logical(na.rm)) {
    stop("`na.rm` must be logical")
  }
  if (length(na.rm) != 1) {
    stop("`na.rm` must be length 1")
  }
  if (length(x) != length(w)) {
    stop("`x` and `w` must be the same length", call. = FALSE)
  }
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
This is a lot of extra work for little additional gain. A useful compromise is the built-in stopifnot(): it checks that each argument is TRUE, and produces a generic error message if not.

wt_mean <- function(x, w, na.rm = FALSE) {
  stopifnot(is.logical(na.rm), length(na.rm) == 1)
  stopifnot(length(x) == length(w))
  
  if (na.rm) {
    miss <- is.na(x) | is.na(w)
    x <- x[!miss]
    w <- w[!miss]
  }
  sum(w * x) / sum(w)
}
wt_mean(1:6, 6:1, na.rm = "foo")
#> Error: is.logical(na.rm) is not TRUE
Note that when using stopifnot() you assert what should be true rather than checking for what might be wrong.

## 19.5.3 Dot-dot-dot (…)

Many functions in R take an arbitrary number of inputs:

sum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
#> [1] 55
stringr::str_c("a", "b", "c", "d", "e", "f")
#> [1] "abcdef"
How do these functions work? They rely on a special argument: ... (pronounced dot-dot-dot). This special argument captures any number of arguments that aren’t otherwise matched.

It’s useful because you can then send those ... on to another function. This is a useful catch-all if your function primarily wraps another function. For example, I commonly create these helper functions that wrap around str_c():

commas <- function(...) stringr::str_c(..., collapse = ", ")
commas(letters[1:10])
#> [1] "a, b, c, d, e, f, g, h, i, j"

rule <- function(..., pad = "-") {
  title <- paste0(...)
  width <- getOption("width") - nchar(title) - 5
  cat(title, " ", stringr::str_dup(pad, width), "\n", sep = "")
}
rule("Important output")
#> Important output ------------------------------------------------------
Here ... lets me forward on any arguments that I don’t want to deal with to str_c(). It’s a very convenient technique. But it does come at a price: any misspelled arguments will not raise an error. This makes it easy for typos to go unnoticed:

x <- c(1, 2)
sum(x, na.mr = TRUE)
#> [1] 4
If you just want to capture the values of the ..., use list(...).

19.5.4 Lazy evaluation

Arguments in R are lazily evaluated: they’re not computed until they’re needed. That means if they’re never used, they’re never called. This is an important property of R as a programming language, but is generally not important when you’re writing your own functions for data analysis. You can read more about lazy evaluation at http://adv-r.had.co.nz/Functions.html#lazy-evaluation.

19.5.5 Exercises

What does commas(letters, collapse = "-") do? Why?

It’d be nice if you could supply multiple characters to the pad argument, e.g. rule("Title", pad = "-+"). Why doesn’t this currently work? How could you fix it?

What does the trim argument to mean() do? When might you use it?

The default value for the method argument to cor() is c("pearson", "kendall", "spearman"). What does that mean? What value is used by default?

19.6 Return values

Figuring out what your function should return is usually straightforward: it’s why you created the function in the first place! There are two things you should consider when returning a value:

Does returning early make your function easier to read?

Can you make your function pipeable?

19.6.1 Explicit return statements

The value returned by the function is usually the last statement it evaluates, but you can choose to return early by using return(). I think it’s best to save the use of return() to signal that you can return early with a simpler solution. A common reason to do this is because the inputs are empty:

complicated_function <- function(x, y, z) {
  if (length(x) == 0 || length(y) == 0) {
    return(0)
  }
    
  # Complicated code here
}
Another reason is because you have a if statement with one complex block and one simple block. For example, you might write an if statement like this:

f <- function() {
  if (x) {
    # Do 
    # something
    # that
    # takes
    # many
    # lines
    # to
    # express
  } else {
    # return something short
  }
}
But if the first block is very long, by the time you get to the else, you’ve forgotten the condition. One way to rewrite it is to use an early return for the simple case:


f <- function() {
  if (!x) {
    return(something_short)
  }

  # Do 
  # something
  # that
  # takes
  # many
  # lines
  # to
  # express
}
This tends to make the code easier to understand, because you don’t need quite so much context to understand it.

19.6.2 Writing pipeable functions

If you want to write your own pipeable functions, it’s important to think about the return value. Knowing the return value’s object type will mean that your pipeline will “just work”. For example, with dplyr and tidyr the object type is the data frame.

There are two basic types of pipeable functions: transformations and side-effects. With transformations, an object is passed to the function’s first argument and a modified object is returned. With side-effects, the passed object is not transformed. Instead, the function performs an action on the object, like drawing a plot or saving a file. Side-effects functions should “invisibly” return the first argument, so that while they’re not printed they can still be used in a pipeline. For example, this simple function prints the number of missing values in a data frame:

show_missings <- function(df) {
  n <- sum(is.na(df))
  cat("Missing values: ", n, "\n", sep = "")
  
  invisible(df)
}
If we call it interactively, the invisible() means that the input df doesn’t get printed out:

show_missings(mtcars)
#> Missing values: 0
But it’s still there, it’s just not printed by default:

x <- show_missings(mtcars) 
#> Missing values: 0
class(x)
#> [1] "data.frame"
dim(x)
#> [1] 32 11
And we can still use it in a pipe:

mtcars %>% 
  show_missings() %>% 
  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% 
  show_missings() 
#> Missing values: 0
#> Missing values: 18
19.7 Environment

The last component of a function is its environment. This is not something you need to understand deeply when you first start writing functions. However, it’s important to know a little bit about environments because they are crucial to how functions work. The environment of a function controls how R finds the value associated with a name. For example, take this function:

f <- function(x) {
  x + y
} 
In many programming languages, this would be an error, because y is not defined inside the function. In R, this is valid code because R uses rules called lexical scoping to find the value associated with a name. Since y is not defined inside the function, R will look in the environment where the function was defined:

y <- 100
f(10)
#> [1] 110

y <- 1000
f(10)
#> [1] 1010
This behaviour seems like a recipe for bugs, and indeed you should avoid creating functions like this deliberately, but by and large it doesn’t cause too many problems (especially if you regularly restart R to get to a clean slate).

The advantage of this behaviour is that from a language standpoint it allows R to be very consistent. Every name is looked up using the same set of rules. For f() that includes the behaviour of two things that you might not expect: { and +. This allows you to do devious things like:

`+` <- function(x, y) {
  if (runif(1) < 0.1) {
    sum(x, y)
  } else {
    sum(x, y) * 1.1
  }
}
table(replicate(1000, 1 + 2))
#> 
#>   3 3.3 
#> 100 900
rm(`+`)
This is a common phenomenon in R. R places few limits on your power. You can do many things that you can’t do in other programming languages. You can do many things that 99% of the time are extremely ill-advised (like overriding how addition works!). But this power and flexibility is what makes tools like ggplot2 and dplyr possible. Learning how to make best use of this flexibility is beyond the scope of this book, but you can read about in Advanced R.

