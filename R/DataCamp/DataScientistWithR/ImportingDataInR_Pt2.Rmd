---
title: "ImportingDataInR_Pt2"
author: "Steve Newns"
date: "December 5, 2017"
output: html_document
---

```{r}
library(ggplot2)
```
# Establish a connection

1st step to import data from a SQL db = creating a connection to it via different packages depending on db 

`dbConnect()` creates a connection between an R session + a SQL db  w/ 1st arg = a **DBIdriver object** that specifies how connections are made + how data is mapped between R + the db. For MySQL db's, you can build such a driver w/ ``RMySQL::MySQL()``. If the MySQL db is a remote db hosted on a server, you'll also have to specify the following arguments: `dbname`, `host`, `port`, `user` + `password.` Most of these details have already been provided.


    Load the DBI library, which is already installed on DataCamp's servers.
    Edit the dbConnect() call to connect to the MySQL database. Change the port argument (3306) and user argument ("student").
```{r}
# Load the DBI package
library(DBI)

# Edit dbConnect() call to connect to tweater db
con <- dbConnect(RMySQL::MySQL(), 
                 dbname = "tweater", 
                 host = "courses.csrrinzqubik.us-east-1.rds.amazonaws.com", 
                 port = 3306,
                 user = "student",
                 password = "datacamp")
class(con)
```

# List the database tables

Next step = see what tables db contains w/ `dbListTables()` which requires the connection object as an input + outputs a character vector w/ table names.
```{r}
tables <- dbListTables(con)
str(tables)
```

# Import users

DB contains data on a more tasty version of Twitter, **Tweater** where users can post tweats w/ short recipes for delicious snacks + comment on other tweats. There are 3 tables that have relations among

Importing the data on the users into the R session w/ `dbReadTable()` by passing the connection objec + the table to import. The resulting object is a standard R data frame.

```{r}
users <- dbReadTable(con,"users")
users
```

# Import all tables

Separate `dbReadTable()` calls for each + every table in a db = a lot of code duplication. Use `lapply()` to load in all tables in the database.

```{r}
(table_names <- dbListTables(con))
# apply read table to each table name + specify connection
tables <- lapply(table_names,dbReadTable, conn = con)
tables
```

Now w/ an R version of all data contained in the db, you can dive a little deeper into relations between the different data frames.

# How do the tables relate?

`tweats` table contains a column `user_id` = users that have posted the tweat. Similarly, `comments` contain both a `user_id` + a `tweat_id` col = which user posted a comment on which tweat.

Who posted the tweat on which somebody commented "awesome! thanks!" (comment 1012)?
```{r}
comments <- tables[[1]]
tweats <- tables[[2]]
users <- tables[[3]]

users[tweats[which(tweats$id == comments[which(comments$message == "awesome! thanks!"),"tweat_id"]),"user_id"],"name"]
```

# Query tweater (1)

Often be working w/ huge db's w/ tables w/ millions of rows. To do some analyses on this data, it's possible you only need a fraction of this data = good idea to send SQL queries to your db + only import data you actually need into R.

`dbGetQuery()` = pass connection object + a SQL query in the form of a character string

    Use dbGetQuery() to create a data frame, elisabeth, that selects the tweat_id column from the comments table where elisabeth is the commenter, her user_id is 1
    Print out elisabeth so you can see if you queried the database correctly.
```{r}
elisabeth <- dbGetQuery(con,"SELECT tweat_ID FROM comments WHERE user_id = 1")
elisabeth

elisabeth2 <- dbGetQuery(con,"SELECT a.tweat_ID FROM comments a 
                         LEFT JOIN users b
                          ON a.user_id = b.id
                         WHERE b.name = 'elisabeth'")
elisabeth2
```



    Create a data frame, latest, that selects the post column from the tweats table observations where the date is higher than '2015-09-21'.
    Print out latest.
```{r}
latest <- dbGetQuery(con, "SELECT post FROM tweats WHERE date > '2015-09-21'")
latest
```


    Create an R data frame, specific, that selects the message column from the comments table where the tweat_id is 77 and the user_id is greater than 4.
    Print specific.
```{r}
specific <- dbGetQuery(con, "SELECT message FROM comments WHERE tweat_id = 77 AND user_id > 4")
specific
```


    Create a data frame, short, that selects the id and name columns from the users table where the number of characters in the name is strictly less than 5.
    Print short.
```{r}
short <- dbGetQuery(con, "SELECT id, name FROM users WHERE CHAR_LENGTH(name) < 5")
short
```

```{r}
dbGetQuery(con, "SELECT post, message
  FROM tweats INNER JOIN comments on tweats.id = tweat_id
    WHERE tweat_id = 77")
```

# Send - Fetch - Clear

`dbGetQuery()` = a virtual function from the `DBI` package, but is actually implemented by the `RMySQL` package. Behind the scenes, the following steps are performed:
<ul>
<li> Sending the specified query with `dbSendQuery()`; </li>
<li> Fetching the result of executing the query on the database with `dbFetch()`; </li>
<li> Clearing the result with `dbClearResult()`. </li>
</ul>

Implement the steps above, tedious but gives the ability to fetch the query's result in chunks rather than all at once. (by specifying the `n` argument inside `dbFetch()`.


    Inspect the dbSendQuery() call that has already been coded for you. It selects the comments for the users with an id above 4.
    Use dbFetch() twice. In the first call, import only two records of the query result by setting the n argument to 2. In the second call, import all remaining queries (don't specify n). In both calls, simply print the resulting data frames.
    Clear res with dbClearResult().
```{r}
# Send query to the database + get back a MySQLResult object
res <- dbSendQuery(con, "SELECT * FROM comments WHERE user_id > 4")

# Use dbFetch() twice
dbFetch(res, n = 2) # get 2 records
dbFetch(res) # get all records

# Clear res
dbClearResult(res)
```

In our toy example, chopping up the fetches doesn't make a lot of sense, but make sure to remember this technique when you're struggling with huge databases!

Every time you connect to a db using `dbConnect()`, you're creating a *new* connection to the db you're referencing. `RMySQL` automatically specifies a max # of open connections + closes some connections for you, but still: it's always polite to manually disconnect from the db afterwards via `dbDisconnect()`.

    Using the technique you prefer, build a data frame long_tweats. It selects the post and date columns from the observations in tweats where the character length of the post variable exceeds 40.
    Print long_tweats.
    Disconnect from the database by using dbDisconnect().
```{r}
(long_tweats <- dbGetQuery(con, "SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) > 40"))
dbDisconnect(con)
```

# Import flat files from the web

`utils` functions import flat file data, such as `read.csv()` + `read.delim()`, + are capable of automatically importing from URLs that point to flat files on the web. `readr`

    Load the readr package. It's already installed on DataCamp's servers.
    Use url_csv to read in the .csv file it is pointing to. Use the read_csv() function. The .csv contains column names in the first row. Save the resulting data frame as pools.
    Similarly, use url_delim to read in the online .txt file. Use the read_tsv() function and store the result as potatoes.
    Print pools and potatoes. Looks correct?
```{r}
# Load the readr package
library(readr)

# Import the csv file: pools
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"
pools <- read_csv(url_csv)

# Import the txt file: potatoes
url_delim <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt"
potatoes <- read_tsv(url_delim)

# Print pools and potatoes
pools
potatoes
```

There is a safer alternative to HTTP, HTTPS (https://) connections, (HypterText Transfer Protocol Secure) that you can use w/ the standard importing functions (since R version 3.2.2).

    Take a look at the URL in url_csv. It uses a secure connection, https://.
    Use read.csv() to import the file at url_csv. The .csv file it is referring to contains column names in the first row. Call it pools1.
    Load the readr package. It's already installed on DataCamp's servers.
    Use read_csv() to read in the same .csv file in url_csv. Call it pools2.
    Print out the structure of pools1 and pools2. Looks like the importing went equally well as with a normal http connection!
```{r}
# https URL to the swimming_pools csv file.
url_csv <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv"

# Import the file using read.csv(): pools1
pools1 <- read.csv(url_csv)

# Load the readr package
library(readr)

# Import the file using read_csv(): pools2
pools2 <- read_csv(url_csv)

# Print the structure of pools1 and pools2
str(pools1)
str(pools2)
```

# Downloading Files
## Import Excel files from the web

When you learned about `gdata`, it was already mentioned that `gdata` can handle `.xls` files on the internet. `readxl` can't, at least not yet.

    Load the readxl and gdata packages. They are already installed on DataCamp's servers.
    Import the .xls file located at the URL url_xls using read.xls() from gdata. Store the resulting data frame as excel_gdata.
    You can not use read_excel() directly with a URL. Complete the following instructions to work around this problem:
    Use download.file() to download the .xls file behind the URL and store it locally as "local_latitude.xls".
    Call read_excel() to import the local file, "local_latitude.xls". Name the resulting data frame excel_readxl.
```{r}
# Load the readxl and gdata package
library(readxl)
library(gdata)

# Specification of url: url_xls
url_xls <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls"

# 1) Import the .xls file DIRECLTY with gdata: excel_gdata
excel_gdata <- gdata::read.xls(url_xls)

# 2a)  file behind URL, name it local_latitude.xls
download.file(url_xls, destfile = 'local_latitude.xls')
# 2b) Import the local .xls file with readxl: excel_readxl
excel_readxl <- read_excel('local_latitude.xls')
```

# Downloading any file, secure or not

Can read excel files on the web using `read_excel` by 1st downloading the file w/ `download.file()`.

There's more: with `download.file()` you can download ANY kind of file from the web using HTTP + HTTPS: images, executable files, but also .RData files (very efficient format to store R data)

You can load data from an RData file using `load()`, but this function does not accept a URL string as an argument. 

    Take a look at the URL in url_rdata. It uses a secure connection, https://. This URL points to an RData file containing a data frame with some metrics on different kinds of wine.
    Download the file at url_rdata using download.file(). Call the file "wine_local.RData" in your working directory.
    Load the file you created, wine_local.RData, using the load() function. It takes one argument, the path to the file, which is just the filename in our case. After running this command, the variable wine will automatically be available in your workspace.
    Print out the summary() of the wine dataset.
```{r}
# https URL to wine RData file.
url_rdata <- "https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData"

# Download the wine file to your working directory
download.file(url_rdata, "wine_local.RData")

# Load the wine data into your workspace using load(path) --> wine variable is now available
load("wine_local.RData")

# Print out the summary of the wine data
summary(wine)
```

Another way to load remote RData files is to use `url()` inside `load()`. However, this will not save the RData file to a local file. 

# Reading a text file from the web

Which way of importing data is NOT possible?
<ul>
<li> Importing a .csv file residing on the web using the URL with read.csv(): </li>
<li> Downloading a remote excel and saving it to your working directory using download.file():</li>
<li> Importing a .txt file residing on the web using the URL with read_tsv():</li>
<li> **Using the load() function to load a remote RData file into the workspace with only the URL string:** -> need to be local </li> 
```{r}
read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv")
read_tsv("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt")
download.file("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xlsx", "lat.xlsx")
load("https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData")
```

# HTTP? httr!

Downloading a file from the Internet = sending a **GET request** + receiving the file you asked for. Internally, all previously-discussed functions use a GET request to download files.

`httr` provides `GET()` to execute this GET request w/ a result = a **response object** that provides easy access to the **status code, content-type** and, of course, the actual content.

You can extract content from the request using `content()` function. At the time of writing, there are 3 ways to retrieve this content: as a raw object, character vector, or an R object, such as a list. If you don't tell `content()` how to retrieve the content through `as` argument, it'll try its best to figure out which type is most appropriate based on the content-type.

    Load the httr package. It's already installed on DataCamp's servers.
    Use GET() to get the URL stored in url. Store the result of this GET() call as resp.
    Print the resp object. What information does it contain?
    Get the content of resp using content() and set the as argument to "raw". Assign the resulting vector to raw_content.
    Print the first values in raw_content with head().
```{r}
library(httr)

# Get the url, save response to resp
url <- "http://www.example.com/"
resp <- GET(url)

# Print resp
resp

# Get the raw content of resp: raw_content
raw_content <- content(resp, as = "raw")

# Print the head of raw_content
head(raw_content)
```

The raw content of the response doesn't make a lot of sense. Luckily, `content()` by default, if you don't specify `as` argument, figures out what type of data you're dealing with and parses it for you.

Web content does not limit itself to HTML pages + files stored on remote servers such as DataCamp's Amazon S3 instances. There are many other data formats out there. A very common one is **JSON**, a format very often used by so-called Web APIs, interfaces to web servers w/ which you as a client can communicate to get or store info in more complicated ways.


    Use GET() to get the url that has already been specified in the sample code. Store the response as resp.
    Print resp. What is the content-type?
    Use content() to get the content of resp. Set the as argument to "text". Simply print out the result. What do you see?
    Use content() to get the content of resp, but this time do not specify a second argument. R figures out automatically that you're dealing with a JSON, and converts the JSON to a named R list.
```{r}
# Get the url
url <- "http://www.omdbapi.com/?apikey=ff21610b&t=Annie+Hall&y=&plot=short&r=json"
resp <- GET(url)

# Print resp
resp

# Print content of resp as text
content(resp, as = "text")

# Print content of resp
content(resp)
```

The fact that httr converts the JSON response body automatically to an R list is very convenient. 

# From JSON to R

In the simplest setting, `fromJSON()` can convert character strings that represent JSON data into a nicely structured R list. 

    Load the jsonlite package. It's already installed on DataCamp's servers.
    wine_json represents a JSON. Use fromJSON() to convert it to a list, named wine.
    Display the structure of wine
```{r}
# Load the jsonlite package
library(jsonlite)

# wine_json is a JSON
wine_json <- wine_json <- '{"name":"Chateau Migraine", "year":1997, "alcohol_pct":12.4, "color":"red", "awarded":false}'

# Convert wine_json into a list: wine
wine <- fromJSON(wine_json)

# Print structure of wine
str(wine)
```

# Quandl API

fromJSON() also works if you pass a URL as a character string or the path to a local file that contains JSON data. try out on Quandl API, where you can fetch all sorts of financial and economical data.

    quandl_url represents a URL. Use fromJSON() directly on this URL and store the result in quandl_data.
    Display the structure of quandl_data.
```{r}
# jsonlite is preloaded

# Definition of quandl_url
quandl_url <- "https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz"

# Import Quandl data: quandl_data
quandl_data <- fromJSON(quandl_url)

# Print structure of quandl_data 
str(quandl_data) # matrix
```

# OMDb API

V. easy to interact w/ an API once you know how to formulate requests. To fetch all info = perform a GET() call, ask for the contents with content(), part of the httr package, which uses jsonlite behind the scenes to import the JSON data into R.

jsonlite can handle URLs itself simply passing the request URL to fromJSON() to get your data into R. use this technique to compare the release year of two movies in the Open Movie Database.

    Two URLs are included in the sample code, as well as a fromJSON() call to build sw4. Add a similar call to build sw3.
    Print out the element named Title of both sw4 and sw3. You can use the $ operator. What movies are we dealing with here?
    Write an expression that evaluates to TRUE if sw4 was released later than sw3. This information is stored in the Year element of the named lists.
```{r}
# Definition of the URLs
url_sw4 <- "http://www.omdbapi.com/?apikey=ff21610b&i=tt0076759&r=json"
url_sw3 <- "http://www.omdbapi.com/?apikey=ff21610b&i=tt0121766&r=json"

# Import two URLs with fromJSON(): sw4 and sw3
sw4 <- fromJSON(url_sw4)
sw3 <- fromJSON(url_sw3)

# Print out the Title element of both lists
sw4$Title
sw3$Title

# Is the release year of sw4 later than sw3?
sw4$Year > sw3$Year
```

# JSON practice (1)

JSON is built on 2 structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. It's up to you to change them appropriately and then call jsonlite's fromJSON() function on them each time.


    Change the assignment of json1 such that the R vector after conversion contains the numbers 1 up to 6, in ascending order. Next, call fromJSON() on json1.
    Adapt the code for json2 such that it's converted to a named list with two elements: a, containing the numbers 1, 2 and 3 and b, containing the numbers 4, 5 and 6. Next, call fromJSON() on json2.
```{r}
# jsonlite is already loaded

# Challenge 1
json1 <- '[1, 2, 3, 4, 5, 6]'
fromJSON(json1)

# Challenge 2
json2 <- '{"a": [1, 2, 3], "b": [4, 5, 6]}'
fromJSON(json2)
```


    Remove characters from json1 to build a 2 by 2 matrix containing only 1, 2, 3 and 4. Call fromJSON() on json1.
    Add characters to json2 such that the data frame in which the json is converted contains an additional observation in the last row. For this observations, a equals 5 and b equals 6. Call fromJSON() one last time, on json2.

```{r}
# Challenge 1
#json1 <- '[[1, 2], [3, 4], [5, 6]]'
json1 <- '[[1, 2], [3, 4]]'
fromJSON(json1)

# Challenge 2
#json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}]'
json2 <- '[{"a": 1, "b": 2}, {"a": 3, "b": 4}, {"a": 5, "b": 6}]'
fromJSON(json2)
```

See that different JSON data structures will lead to different data structures in R

# toJSON()

Apart from converting JSON to R with fromJSON(), you can also use toJSON() to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class json, which is basically a character string representing that JSON.

For this exercise, you will be working with a .csv file containing information on the amount of desalinated water that is produced around the world. As you'll see, it contains a lot of missing values. This data can be found on the URL that is specified in the sample code

    Use a function of the utils package to import the .csv file directly from the URL specified in url_csv. Save the resulting data frame as water. Make sure that strings are not imported as factors.
    Convert the data frame water to a JSON. Call the resulting object water_json.
    Print out water_json.
```{r}
# URL pointing to the .csv file
url_csv <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv"

# Import the .csv file located at url_csv
water <- read.csv(url_csv,stringsAsFactors = F)

# Convert the data file according to the requirements
water_json <- toJSON(water)

# Print out water_json
water_json
```


JSON printed out isn't easy to read.

# Minify and prettify

JSONs can come in different formats. Take these 2 JSONs, that are in fact exactly the same: the first one is in a minified format, the second one is in a pretty format with indentation, whitespace and new lines:
```{r}
# Mini
'{"a":1,"b":2,"c":{"x":5,"y":6}}'

# Pretty
'{
  "a": 1,
  "b": 2,
  "c": {
    "x": 5,
    "y": 6
  }
}'
```
Unless you're a CPU, you surely prefer the 2nd version. However, the standard form that toJSON() returns, is the minified version, as it is more concise. You can adapt this behavior by setting the pretty argument inside toJSON() to TRUE. If you already have a JSON string, you can use prettify() or minify() to make the JSON pretty or as concise as possible.

    Convert the mtcars dataset, which is available in R by default, to a pretty JSON. Call the resulting JSON pretty_json.
    Print out pretty_json. Can you understand the output easily?
    Convert pretty_json to a minimal version using minify(). Store this version under a new variable, mini_json.
    Print out mini_json. Which version do you prefer, the pretty one or the minified one?
```{R}
# jsonlite is already loaded

# Convert mtcars to a pretty JSON: pretty_json
pretty_json <- toJSON(mtcars, pretty = T)

# Print pretty_json
pretty_json

# Minify pretty_json: mini_json
mini_json <- minify(pretty_json)

# Print mini_json
mini_json
```

# Import SAS data with haven

`haven` = an extremely easy-to-use package to import data from three software packages: SAS, STATA and SPSS. Depending on the software, you use different functions:

    SAS: read_sas()
    STATA: read_dta() (or read_stata(), which are identical)
    SPSS: read_sav() or read_por(), depending on the file type.

All these functions take one key argument: the path to your local file. In fact, you can even pass a URL; haven will then automatically download the file for you before importing it.

You'll be working with data on the age, gender, income, and purchase level (0 = low, 1 = high) of 36 individuals (Source: SAS). The information is stored in a SAS file, sales.sas7bdat, which is available in your current working directory. You can also download the data here.

    Load the haven package; it's already installed on DataCamp's servers.
    Import the data file "sales.sas7bdat". Call the imported data frame sales.
    Display the structure of sales with str(). Some columns represent categorical variables, so they should be factors.
```{r}
library(haven)

# Import sales.sas7bdat: sales
sales <- read_sas("sales.sas7bdat")

# Display the structure of sales
str(sales)
```

# Import STATA data with haven

Next up are STATA data files; you can use read_dta() for these.

When inspecting the result of the read_dta() call, you will notice that one column will be imported as a labelled vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it's best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you'll need haven's as_factor() function.

In this exercise, you will work with data on yearly import and export numbers of sugar, both in USD and in weight. The data can be found at: http://assets.datacamp.com/production/course_1478/datasets/trade.dta

    Import the data file directly from the URL using read_dta(), and store it as sugar.
    Print out the structure of sugar. The Date column has class labelled.
    Convert the values in the Date column of sugar to dates, using as.Date(as_factor(___)).
    Print out the structure of sugar once more. Looks better now?
```{r}
# Import the data from the URL: sugar
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")

# Structure of sugar
str(sugar)

# Convert values in Date column to dates
sugar$Date <- as.Date(haven::as_factor(sugar$Date))

# Structure of sugar again
str(sugar)
```

# What does the graphic tell?

A plot can be very useful to explore the relationship between two variables. If you pass the plot() function two arguments, the first one will be plotted on the x-axis, the second one will be plotted on the y-axis.

The sugar trading data is again available at http://assets.datacamp.com/production/course_1478/datasets/trade.dta.

After you've imported the data frame, you should plot two of its variables, Import against Weight_I, and describe their relationship! haven is already loaded in your R session, so you can start importing straight away.
```{r}
sugar <- read_dta("http://assets.datacamp.com/production/course_1478/datasets/trade.dta")
plot(sugar$Import,sugar$Weight_I)
```

The import figures in USD and the import figures in weight are rather positively correlated. This of course makes sense: the more sugar is traded, the higher the weight that's traded. 

# Import SPSS data with haven

The haven package can also import data files from SPSS. Again, importing the data is pretty straightforward. Depending on the SPSS data file you're working with, you'll need either read_sav() - for .sav files - or read_por() - for .por files.

In this exercise, you will work with data on four of the Big Five personality traits for 434 persons (Source: University of Bath). The Big Five is a psychological concept including, originally, five dimensions of personality to classify human personality. The SPSS dataset is called person.sav and is available in your working directory.

    Use read_sav() to import the SPSS data in "person.sav". Name the imported data frame traits.
    traits contains several missing values, or NAs. Run summary() on it to find out how many NAs are contained in each variable.
    Print out a subset of those individuals that scored high on Extroversion and on Agreeableness, i.e. scoring higher than 40 on each of these two categories. You can use subset() for this.
```{r}
# Import person.sav: traits
traits <- read_sav("person.sav")

# Summarize traits
summary(traits)

# Print out a subset
subset(traits, Extroversion > 40 & Agreeableness > 40)
```

# Factorize, round two

In the last exercise you learned how to import a data file using the command read_sav(). With SPSS data files, it can also happen that some of the variables you import have the labelled class. This is done to keep all the labelling information that was originally present in the .sav and .por files. It's advised to coerce (or change) these variables to factors or other standard R classes.

The data for this exercise involves information on employees and their demographic and economic attributes (Source: QRiE). The data can be found on the following URL:

http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav

    Import the SPSS data straight from the URL and store the resulting data frame as work.
    Display the summary of the GENDER column of work. This information doesn't give you a lot of useful information, right?
    Convert the GENDER column in work to a factor, the class to denote categorical variables in R. Use as_factor().
    Once again display the summary of the GENDER column. This time, the printout makes much more sense.
```{r}
# Import SPSS data from the URL: work
work <- read_sav("http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav")

# Display summary of work$GENDER
summary(work$GENDER) # doesn't give much info

# Convert work$GENDER to a factor
work$GENDER <- as_factor(work$GENDER)

# Display summary of work$GENDER again
summary(work$GENDER) 
```

as_factor() function has more arguments you can specify, have a look at its documentation to discover more!

# foreign
The foreign package offers a simple function to import and read STATA data: read.dta().

In this exercise, you will import data on the US presidential elections in the year 2000. The data in florida.dta contains the total numbers of votes for each of the four candidates as well as the total number of votes per election area in the state of Florida (Source: Florida Department of State). The file is available in your working directory, you can download it here if you want to experiment some more.
Instructions
100xp

    Load the foreign package; it's already installed on DataCamp's servers.
    Import the data on the elections in Florida, "florida.dta", and name the resulting data frame florida. Use read.dta() without specifying extra arguments.
    Check out the last 6 observations of florida with tail()
```{r}
library(foreign)

# Import florida.dta and name the resulting data frame florida
florida <- read.dta("florida.dta")

# Check tail() of florida
tail(florida)
```

Data can be very diverse, going from character vectors to categorical variables, dates and more. It's in these cases that the additional arguments of read.dta() will come in handy.

The arguments you will use most often are convert.dates, convert.factors, missing.type and convert.underscore. Their meaning is pretty straightforward, as Filip explained in the video. It's all about correctly converting STATA data to standard R data structures. Type ?read.dta to find out about about the default values.

The dataset for this exercise contains socio-economic measures and access to education for different individuals (Source: World Bank). This data is available as edequality.dta, which is located in the worldbank folder in your working directory

    Specify the path to the file using file.path(). Call it path. Remember the "edequality.dta" file is located in the "worldbank" folder.
    Use the path variable to import the data file in three different ways; each time show its structure with str():
    edu_equal_1: By passing only the file path to read.dta().
    edu_equal_2: By passing the file path, and setting convert.factors to FALSE.
    edu_equal_3: By passing the file path, and setting convert.underscore to TRUE.
```{r}
# Specify the file path using file.path(): path
path <- file.path("worldbank","edequality.dta")

# Create and print structure of edu_equal_1
edu_equal_1 <- read.dta(path)
str(edu_equal_1)

# Create and print structure of edu_equal_2 
edu_equal_2 <- read.dta(path, convert.factors = F) # leave factors as char or int
str(edu_equal_2)

# Create and print structure of edu_equal_3
edu_equal_3 <- read.dta(path, convert.underscore = T) # converts underscores to periods
str(edu_equal_3)
```

Can you tell the difference between the different versions of read.dta()? For this data, the first version (where you simply specified the file path) will be most useful to work with. Head over to the next exercise to see if you actually understand your data! 

# Do you know your data?

The previous exercise dealt about socio-economic indicators and access to education of different individuals. The edu_equal_1 dataset that you've built is already available in the workspace. Now that you have it in R, it's pretty easy to get some basic insights.

For example, you can ask yourself how many observations (e.g. how many people) have an age higher than 40 and are literate? When you call

str(edu_equal_1)

You'll see that age is an integer, and literate is a factor, with the levels "yes" and "no". The following expression thus answers the question:

nrow(subset(edu_equal_1, age > 40 & literate == "yes"))

Up to you to answer a similar question now:

How many observations/individuals from Bulgaria have an income above 1000?
```{r}
nrow(subset(edu_equal_1, ethnicity_head == "Bulgaria" & income > 1000))
```

# Import SPSS data with foreign (1)

All great things come in pairs. Where foreign provided read.dta() to read Stata data, there's also read.spss() to read SPSS data files. To get a data frame, make sure to set to.data.frame = TRUE inside read.spss().

In this exercise, you'll be working with socio-economic variables from different countries (Source: Quantative Data Analysis in Education). The SPSS data is in a file called international.sav, which is in your working directory. You can also download it here if you want to play around with it some more.

    Import the data file "international.sav" and have R convert it to a data frame. Store this data frame as demo.
    Create a boxplot of the gdp variable of demo.
```{r}
# Import international.sav as a data frame: demo
demo <- read.spss("international.sav", to.data.frame = T)

# Create boxplot of gdp variable of demo
boxplot(demo$gdp)
```

# Excursion: Correlation

If you're familiar with statistics, you'll have heard about Pearson's Correlation. It is a measurement to evaluate the linear dependency between two variables, say X and Y. It can range from -1 to 1; if it's close to 1 it means that there is a strong positive association between the variables. If X is high, also Y tends to be high. If it's close to -1, there is a strong negative association: If X is high, Y tends to be low. When the Pearson correlation between two variables is 0, these variables are possibly independent: there is no association between X and Y

.

You can calculate the correlation between two vectors with the cor() function. Take this code for example, that computes the correlation between the columns height and width of a fictional data frame size:

cor(size$height, size$width)

The data you've worked with in the previous exercise, international.sav, is again available in your working directory. It's now up to import it and undertake the correct calculations to answer the following question:

What is the correlation coefficient for the two numerical variables gdp and f_illit (female illiteracy rate)?
```{r}
demo <- read.spss("international.sav", to.data.frame = T)
head(demo)
cor(demo$gdp,demo$f_illit)
```

The value of the correlation cofficient is equal to -0.4476856, to be precise. That indicates a negative association among GDP and female illiteracy. but it is rather weak.

# Import SPSS data with foreign (2)

In the previous exercise, you used the to.data.frame argument inside read.spss(). There are many other ways in which to customize the way your SPSS data is imported.

In this exercise you will experiment with another argument, use.value.labels. It specifies whether variables with value labels should be converted into R factors with levels that are named accordingly. The argument is TRUE by default which means that so called labelled variables inside SPSS are converted to factors inside R.

You'll again be working with the international.sav data, which is available in your current working directory.

    Import the data file "international.sav" as a data frame, demo_1.
    Print the first few rows of demo_1 using the head() function.
    Import the data file "international.sav" as a data frame, demo_2, but this time in a way such that variables with value labels are not converted to R factors.
    Again, print the first few rows of demo_2. Can you tell the difference between the two data frames?
```{r}
# foreign is already loaded

# Import international.sav as demo_1
demo_1 <- read.spss("international.sav", to.data.frame = T)

# Print out the head of demo_1
head(demo_1)

# Import international.sav as demo_2
demo_2 <- read.spss("international.sav", to.data.frame = T, use.value.labels = F)

# Print out the head of demo_2
head(demo_2)
```
