Subscribe to the edX subreddit
Download the edX mobile app from the Apple App Store Download the edX mobile app from Google Play
film
aggregate(Pct.Dom~BudgetCategory,film,mean)
BudCat.model <- aov(film$Pct.Dom,film$BudgetCategory)
summary(BudCat.model)
BudCat.model <- aov(film$Pct.Dom,film$BudgetCategory)
BudCat.model <- aov(film$Pct.Dom~film$BudgetCategory)
summary(BudCat.model)
TukeyHSD(BudCat.model)
SS(t) = 5949.1
SS(b) = 2387.7
SS(w) = SS(t) - SS(b)
SS(t) = 5949.1
SS(b) = 2387.7
SS(w) = SS(t) - SS(b)
SS.t = 5949.1
SS.b = 2387.7
SS.w = SS.t - SS.b
unanswered
SS.w
MS.b = SS.b / 2
MS.w = SS.w / 42
MS.b = SS.b / 2
MS.w = SS.w / 42
MS.b = SS.b / 2
MS.w = SS.w / 42
f = MS.b / MS.w
f
A local police department has divided the city into three sections, and each is patrolled by a different set of six (6) officers.  The police chief wants to determine if officers are biased in the number of disorderly conduct tickets that they give out in each section.
sec1 <- c(8,4,6,8,6,4)
sec2 <- c(3,7,0,2,7,5)
sec3 <- c(1,2,7,6,5,0)
grand.mu <- mean(sec1,sec3,sec3)
grand.mu <- mean(sec1+sec3+sec3)
grand.mu
mean(8,4,6,8,6,4,3,7,0,2,7,5,1,2,7,6,5,0)
grand.mu <- mean(sec1,sec3,sec3)
mean(sec1)
mean(mean(sec1),mean(sec2),mean(sec3))
grand.mu <- mean(mean(sec1),mean(sec2),mean(sec3))
var.sec1 <- sec1 - grand.mu
sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
var.sec1 <- (sec1 - grand.mu)^2
var.sec2 <- (sec2 - grand.mu)^2
var.sec3 <- (sec3 - grand.mu)^2
sum(var.sec3,var.sec2,var.sec3)
(sec1 - grand.mu)^2
(sec2 - grand.mu)^2
(sec3 - grand.mu)^2
(sec1 - grand.mu)
(sec2 - grand.mu)
(sec3 - grand.mu)
sum(var.sec3)+sum(var.sec2)+sum(var.sec3)
grand.mu
sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
(sec1 - grand.mu)
(sec1 - grand.mu)^2
(sec2 - grand.mu)
sec1 <- c(8,4,6,8,6,4)
sec2 <- c(3,7,0,2,7,5)
sec3 <- c(1,2,7,6,5,0)
'3a. Which of the following is the alternative hypothesis for this test?'
# HA: Police officers in at least 1 section of the city give out a different number
#   of ￼tickets on average.
'2c. What are the degrees of freedom for this test?'
# Numerator Degree of Freedom = groups - 1 = k - 1 = 3 - 1 = 2
# Denominator Degree of Freedom = sample - groups = N - k = 18 - 3 = 15
'3c. What is SSTotal for this problem? (Round to 1 decimal place.)'
grand.mu <- mean(mean(sec1),mean(sec2),mean(sec3))
sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
var.sec1 <- (sec1 - grand.mu)^2
var.sec2 <- (sec2 - grand.mu)^2
var.sec3 <- (sec3 - grand.mu)^2
sum(var.sec3,var.sec2,var.sec3)
(sec3 - grand.mu)
grand.mu <- mean(c(sec1,sec2,sec3)
)
grand.mu
grand.mu <- mean(c(sec1,sec2,sec3))
sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
var.sec1 <- (sec1 - grand.mu)^2
var.sec2 <- (sec2 - grand.mu)^2
var.sec3 <- (sec3 - grand.mu)^2
sum(var.sec3,var.sec2,var.sec3)
grand.mu <- mean(c(sec1,sec2,sec3))
SS.t <- sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
SS.t
sec1.mu <- mean(sec1)
sec2.mu <- mean(sec2)
sec3.mu <- mean(sec3)
sec1.mu
sec2.mu
sec3.mu
SS.w <- sum((sec1 - sec1.mu)^2,(sec2 - sec2.mu)^2,(sec3 - sec3.mu)^2)
SS.w
'******Course Week 5: Hypothesis Testing (More Than Two Group Means)****
Question 3
A local police department has divided the city into 3 sections, and each is
patrolled by a different set of 6 officers. The police chief wants to determine
if officers are biased in the number of disorderly conduct tickets they give out
in each section.
H
ere are the number of tickets given by the officers in each section in the last week:
Section 1 	Section 2 	Section 3
8 	        3         	1
4 	        7 	      	2
6 	        0 	      	7
8 	        2 	      	6
6 	        7 	      	5
4 	        5 	      	0'
sec1 <- c(8,4,6,8,6,4)
sec2 <- c(3,7,0,2,7,5)
sec3 <- c(1,2,7,6,5,0)
'3a. Which of the following is the alternative hypothesis for this test?'
# HA: Police officers in at least 1 section of the city give out a different number
#   of ￼tickets on average.
'2c. What are the degrees of freedom for this test?'
# Numerator Degree of Freedom = groups - 1 = k - 1 = 3 - 1 = 2
# Denominator Degree of Freedom = sample - groups = N - k = 18 - 3 = 15
'3c. What is SSTotal for this problem? (Round to 1 decimal place.)'
grand.mu <- mean(c(sec1,sec2,sec3))
SS.t <- sum((sec1 - grand.mu)^2,(sec2 - grand.mu)^2,(sec3 - grand.mu)^2)
'3d. What is MSBetween for this problem? (Round to 1 decimal place.)'
sec1.mu <- mean(sec1)
sec2.mu <- mean(sec2)
sec3.mu <- mean(sec3)
SS.w <- sum((sec1 - sec1.mu)^2,(sec2 - sec2.mu)^2,(sec3 - sec3.mu)^2)
SS.b = SS.t - SS.w
SS.b
sec1.mu
(sec1 - sec1.mu)
(sec2 - sec2.mu)
SS.w <- sum((sec1 - sec1.mu)^2,(sec2 - sec2.mu)^2,(sec3 - sec3.mu)^2)
SS.b = SS.t - SS.w
SS.b
MS.b <- SS.b / 2
MS.w <- SS.w / 15
MS.b
MS.w
f <- MS.b / MS.w
f
MS.b
MS.w <- SS.w / 15
MS.w
f
f.crit <- 3.6823
f > f.crit
SS.t <- 2147
SS.b <- 782
SS.w <- SS.t - SS.b
SS.w
MS.w <- SS.w / 34
MS.w
1365/34
MS.b <- SS.b / 2
f <- MS.b / MS.w
f
MS.w <- round(SS.w / 34,2) # 40.14706 = 40.15
f <- MS.b / MS.w # 9.739194 = 9.739194
f
MS.b <- round(SS.b / 2,2)
f <- MS.b / MS.w # 9.739194 = 9.739194
f
bonferroni <- (3 * (3-1)) / 2)
bonferroni <- (3 * (3-1)) / 2
0.05 / bonferroni
round(0.05 / bonferroni,3)
'4h. Here are the results from your post-hoc analysis. These are the p-values from the actual t-tests. They are not adjusted, so be sure to compare them to the significance level you calculated above.
d
)
library(SDSFoundations)
res <- TempskiResilience
head(res[res$QoL==10,])
head(res)
res$QoL[:10]
res$QoL[0:10]
sum(res$QoL[0:10] > 5)
(res$QoL[0:10] > 5)
(res$MS.QoL[0:10] > 5)
sum(res$MS.QoL[0:10] > 5)
names(res)
head(res
)
clin.sci <- res[res$Group == 'Clinical Sciences']
clin.sci <- res[res$Group == 'Clinical Sciences',]
cor(clin.sci$BDI,clin.sci$QoL)
cor.test(clin.sci$BDI,clin.sci$QoL) # -0.3746403
install.packages('psych')
library(psych)
corr(clin.sci$BDI,clin.sci$QoL) # -0.3746403
corr.test(clin.sci$BDI,clin.sci$QoL) # -0.3746403
?corr.test
vars <- c('BDI','QoL')
corr.test(clin.sci[,vars]) # -0.3746403
corr.test(clin.sci[,vars])$r
corr.test(clin.sci[,vars])$r
corr.test(clin.sci[,vars])$p
corr.test(clin.sci[,vars])$t
clin.sci.model <- lm(QoL~BDI, clin.sci)
summary(clin.sci.model)
names(clin.sci)
vars <- c('DREEM.S.SP','DREEM.A.SP','Resilience', 'BDI', 'Age')
corr.test(clin.sci[,vars])$r
corr.test(clin.sci[,vars])$r
corr.test(clin.sci[,vars],use = 'pairwise.complete.obs')$r
names(clin.sci)
vars <- c('MS.QoL','DREEM.S.SP','DREEM.A.SP','Resilience', 'BDI', 'Age')
corr.test(clin.sci[,vars],)$r
vars <- c('MS.QoL','DREEM.S.SP','DREEM.A.SP','Resilience', 'BDI', 'Age')
corr.test(clin.sci[,vars])$r
clin.sci.model2 <- lm(MS.QoL ~ DREEM.S.SP + DREEM.A.SP + Resilience + BDI + Age, clin.sci)
summary(clin.sci.model2)
confint(clin.sci.model)
confint(clin.sci.model2) # confident intervals
plot(clin.sci.model, which = 1)
plot(clin.sci.model)
plot(clin.sci.model, which = 1)
# have linearity and homoscedasticity
# observations w/ numeric labels are the row #'s of outliers --> 1055, 871, 1107
#cooks distance plot
cutoff <- 4/(clin.sci.model$df) # cutoff of influence for cook's distance is 4 / dF in the LM model
plot(clin.sci.model, which = 4, cook.levels=cutoff)
plot(clin.sci.model, which = 4, cook.levels = cutoff, id.n = 5)
plot(clin.sci.model2, which = 1)
cutoff <- 4/(clin.sci.model2$df) # cutoff of influence for cook's distance is 4 / dF in the LM model
plot(clin.sci.model2, which = 4, cook.levels = cutoff, id.n = 5)
vif(clin.sci.model2)
library(car)
vif(clin.sci.model2)
library(car)
install.packages('car')
library(car)
vif(clin.sci.model2)
1/vif(clin.sci.model2)
plot(clin.sci.model2, which = 1)
lmBeta(clin.sci.model2)
round(pCorr(clin.sci.model2), 4)
cor(clin.sci$BDI,clin.sci$QoL) # -0.3746403
cor(clin.sci$BDI,clin.sci$QoL,use = "pairwise.complete.obs") # -0.3746403
cor(clin.sci$BDI,clin.sci$QoL,use = "pairwise.complete.obs") # -0.3746403
cor(clin.sci[,vars]) # -0.3746403
cor(clin.sci[,vars],use = "pairwise.complete.obs") # -0.3746403
vars <- c('BDI','QoL')
round(corr.test(clin.sci[,vars]),3)
vars <- c('BDI','QoL')
round(cor(clin.sci[,vars]),3)
cor(clin.sci[,vars])
corr.test(clin.sci[,vars])$t
round(corr.test(clin.sci[,vars])$t,3)
summary(clin.sci.model)$r
summary(clin.sci.model)
summary(clin.sci.model2)
lmBeta(clin.sci.model2)
round(lmBeta(clin.sci.model2),3)
round(pCorr(clin.sci.model2),3)
round(100*pCorr(clin.sci.model2),3)
round(100*pCorr(clin.sci.model2),2)
summary(clin.sci.model2
)
library(SDSFoundations)
res <- TempskiResilience
res <- TempskiResilience
table(res$Group)
bas.sci <- res[res$Group == 'Basic Sciences']
bas.sci <- res[res$Group == 'Basic Sciences',]
names(bas.sci)
whoModel <- lm(QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
library(car)
vif(whoModel)
1/vif(whoModel)
plot(whoModel, which = 1)
cutoff <- 4/(whoModel$df)
plot(whoModel, which = 4, cook.levels = cutoff, id.n = 5)
lmBeta(whoModel)
round(pCorr(whoModel), 4)
round(100*pCorr(whoModel),2) # 8.23
corr.test(bas.sci[,vars])$r
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
corr.test(bas.sci[,vars])$r
cor(bas.sci[,vars])
cor(bas.sci[,vars])$r
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(bas.sci[,vars])
vars <- c('QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(res[,vars])
whoModel <- lm(MS.QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
cor(res[,vars])
corr.test(res[,vars])$r
vars <- c('MS.QoL', 'WHOQOL.PH', 'WHOQOL.PSY', 'WHOQOL.SOC', 'WHOQOL.ENV')
corr.test(res[,vars])$r
corr.test(res[,vars],use = "pairwise.complete.obs)$r
corr.test(res[,vars], use = "pairwise.complete.obs')$r
corr.test(res[,vars], use = "pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
corr.test(res[,vars], use = 'pairwise.complete.obs')
corr.test(res[,vars], use = 'pairwise.complete.obs')$r
vif(whoModel)
1/vif(whoModel)
whoModel
whoModel <- lm(MS.QoL ~ WHOQOL.PH + WHOQOL.PSY + WHOQOL.SOC + WHOQOL.ENV, bas.sci)
summary(whoModel)
vif(whoModel)
plot(whoModel, which = 1)
cutoff <- 4/(whoModel$df)
plot(whoModel, which = 4, cook.levels = cutoff, id.n = 5)
lmBeta(whoModel)
function (model)
round(100*pCorr(whoModel),2) # 8.23
round(100*pCorr(whoModel),2) # 8.23
whoModel$t
whoModel
res <- TempskiResilience
clin.sci <- res[res$Group == 'Clinical Sciences',]
clin.sci
summary(lm(BDI~Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Gender+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Gender+Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Female+State.Anxiety+Trait.anxiety,clin.sci))
summary(lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci))
model1 <- summary(lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci))
summary(model1)
summary(model1)
model1 <- lm(BDI~Female+Age+State.Anxiety+Trait.anxiety,clin.sci)
summary(model1)
lmBeta(model1)
round(lmBeta(model1),3)
round(100*pCorr(model1),2)
SS = 1848.76
df <- 20-2
SS = 1848.76
SS = 1848.76
df <- 20-2
25592/2646
-23.4325 / 12.74
22.245*421
8.32*0.1528
480.78 / 1848.76
480.78 / (1848.76+480.78)
480.78 / (1848.76 - 480.78)
1848.76 / (1848.76 - 480.78)
1848.76 / (1848.76 + 480.78)
install.packages('rattle')
install.packages('RGTK2')
install.packages('RGtk2')
partial <- c(10, 20, NA, 30)
mean(partial)
mean(partial, na.rm = T)
dbinom(x = 4, size = 20, prob = 1/6)
dbinom(1,20,1/2)
dbinom(1,100,1/2)
pbinom(q = 4, size = 20, prob = 1/6)
dbinom(x = 4, size = 20, prob = 1/6)
qbinom(p = 0.75, size = 20, prob = 1/6)
pbinom(q = 3, size = 20, prob = 1/6) #76.87%
pbinom(q = 3, size = 20, prob = 1/6) #56.65%
rbinom( n = 100, size = 20, prob = 1/6 )
dnorm( x = 1, mean = 1, sd = 0.1 )
?dnorm
normal.a <- rnorm(1000, mean = 0, sd = 1)
library(ggplot2)
ggplot + geom_histogram(normal.a)
ggplot + geom_histogram(aes(normal.a))
ggplot() + geom_histogram(aes(normal.a))
?rchisq
chisq.a <- rchisq(1000, df = 3)
library(ggplot2)
ggplot() + geom_histogram(aes(chisq.a))
chi.sq.3 <- (normal.a)^2 + (normal.a)^2 + (normal.a)^2
ggplot() + geom_histogram(aes(chi.sq.3))
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
normal.d / sqrt(scaled.chisq.3)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() + geom_histogram(aes(t.3))
scaled.chisq.3 <- chi.sq.3/3
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() + geom_histogram(aes(t.3))
ggplot() + geom_histogram(aes(t.3), bins = 4)
ggplot() + geom_histogram(aes(t.3), bins = 10)
normal.b <- rnorm(1000)
normal.3 <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.a)^2 + (normal.a)^2
ggplot() + geom_histogram(aes(chi.sq.3))
# create t-distribution from scaled chi-squared
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() + geom_histogram(aes(t.3), bins = 10)
normal.a <- rnorm(1000, mean = 0, sd = 1)
library(ggplot2)
ggplot() + geom_histogram(aes(normal.a))
# generate 1000 values in a chi-squared distribution w/ dF = 3
chisq.a <- rchisq(1000, df = 3)
ggplot() + geom_histogram(aes(chisq.a))
# way #2 --> sum of squares
# square values of 3 distributions and sum
normal.b <- rnorm(1000)
normal.3 <- rnorm(1000)
chi.sq.3 <- (normal.a)^2 + (normal.a)^2 + (normal.a)^2
ggplot() + geom_histogram(aes(chi.sq.3))
# create t-distribution from scaled chi-squared
scaled.chisq.3 <- chi.sq.3/3
normal.d <- rnorm(1000)
t.3 <- normal.d / sqrt(scaled.chisq.3)
ggplot() + geom_histogram(aes(t.3), bins = 10)
chi.sq.20 <- rchisq( 1000, 20)
scaled.chi.sq.20 <- chi.sq.20 / 20
F.3.20 <- scaled.chi.sq.3 / scaled.chi.sq.20
ggplot() + geom_histogram(aes(F.3.20))
F.3.20 <- scaled.chi.sq.3 / scaled.chi.sq.20
chi.sq.20 <- rchisq( 1000, 20)
scaled.chi.sq.20 <- chi.sq.20 / 20
F.3.20 <- scaled.chisq.3 / scaled.chi.sq.20
ggplot() + geom_histogram(aes(F.3.20))
?binom.test
binom.test(x = 62, n = 100, p = .5 )
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/R/Udacity/EDAWithR')
library(tidyverse)
library(ggplot2)
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
library(gridExtra) # multiple plots on 1 graph
summary(facebook$mobile_likes)
d_count)) +
coord_cartesian(ylim = c(0,1000))
# females seem to have a higher median of friends (higher on "average"), and tops of boxes seems higher
# different isn't very large in average # of friends
# check middle 50% (box)
ggplot(subset(facebook, !is.na(gender))) +
geom_boxplot(aes(gender, friend_count)) +
coord_cartesian(ylim = c(0,250))
# medians seem more similar now
# check quartiles (75% of females have friend counts < 244)
# or 25% of female users have > 244 friends
by(facebook$friend_count, facebook$gender, summary)
# males have smaller box = less variable
# on average, who initiated more friendships?
ggplot(subset(facebook, !is.na(gender))) +
geom_boxplot(aes(gender, friendships_initiated))
ggplot(subset(facebook, !is.na(gender))) +
geom_boxplot(aes(gender, friendships_initiated)) +
coord_cartesian(ylim = c(0,200))
by(facebook$friendships_initiated, facebook$gender, summary)
# females barely
'******************************************************************************************************'
'LOGICAL'
'******************************************************************************************************'
# often want to convert variables w/ many 0 values into a binary variable
# helpful to find out if a user has actually used a certain feature instead of how many times they
#   actually used it
summary(facebook$mobile_likes)
# see median is 4 but looking at the mean + max, it tells us we have a lot of 0 values in the dataset
table(facebook$mobile_likes == 0)
table(facebook$mobile_likes == 0)
summary(facebook$mobile_likes > 0)
facebook <- facebook %>%
mutate(mobile_check_in, ifelse(mobile_likes > 0,TRUE,FALSE))
head(facebook)
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE))
head(facebook)
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
library(forcats)
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
parse_factor(mobile_check_in)
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
parse_factor(mobile_check_in)
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE))
facebook %>%  parse_factor(mobile_check_in)
parse_factor(facebook$mobile_check_in)
parse_factor(facebook$mobile_check_in, c(0,1))
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
parse_factor(mobile_check_in, c(0,1))
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
mutate(mobile_check_in = as.factor(mobile_check_in))
head(facebook)
as.factor(mobile_check_in)
as.factor(facebook$mobile_check_in)
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
mutate(as.factor(mobile_check_in))
glimpse(facebook)
summary(facebook$mobile_check_in)
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
facebook <- facebook %>%
mutate(mobile_check_in = ifelse(mobile_likes > 0,TRUE,FALSE)) %>%
mutate(factor(mobile_check_in))
glimpse(facebook)
table(facebook$mobile_check_in)
facebook$mobile_check_in/nrow(facebook)*100
sum(facebook$mobile_check_in)
sum(facebook$mobile_check_in)/nrow(facebook)*100
