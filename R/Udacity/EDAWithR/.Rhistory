}
# build data frame
schedule <- data.frame(rbind(row1,row2,row3,row4,row5,row6,row7,row8,row9,row10,row11,row12,row13,row14,row15,row16,row17),
row.names = NULL)
# rename data frame columns
colnames(schedule) <- headers
# remove tickets column
schedule <- schedule[,1:4]
# convert from factor
#schedule$WK <- as.integer(schedule$WK)
schedule$DATE <- as.character(schedule$DATE)
schedule$OPPONENT <- as.character(schedule$OPPONENT)
schedule$`TIME (ET)` <- as.character(schedule$`TIME (ET)`)
# fix BYE WEEK
schedule[10,3] <- 'BYE WEEK'
schedule[10,4] <- NA
# split out date field
schedule <- schedule %>% separate(DATE, into = c('DAYOFWEEK', 'DATE'), sep = ',')
schedule
schedule %>%
sapply(gsub, pattern='@', replacement='') %>%
sapply(gsub, pattern='vs', replacement='')
schedule %>%
aapply(gsub, pattern='@', replacement='') %>%
aapply(gsub, pattern='vs', replacement='')
schedule %>%
apply(gsub, pattern='@', replacement='') %>%
apply(gsub, pattern='vs', replacement='')
schedule %>%
apply(FUN = gsub, pattern='@', replacement='') %>%
apply(FUN = gsub, pattern='vs', replacement='')
schedule
schedule %>%
apply(gsub, pattern='@', replacement='') %>%
apply(gsub, pattern='vs', replacement='')
schedule %>%
apply(gsub, pattern='@', replacement='')
schedule %>%
lapply(gsub, pattern='@', replacement='')
schedule %>%
ldply(gsub, pattern='@', replacement='')
library(plyr)
schedule %>%
ldply(gsub, pattern='@', replacement='')
schedule[] <- schedule %>%
lapply(gsub, pattern='@', replacement='') %>%
lapply(gsub, pattern='vs', replacement='')
schedule
separate(schedule, `TIME (ET)``, into = c("TIME", "CHANNEL"), sep = " (?=[^ ]+$)")
)
separate(schedule, `TIME (ET)`, into = c("TIME", "CHANNEL"), sep = " (?=[^ ]+$)")
separate(schedule, `TIME (ET)`, into = c("TIME", "CHANNEL"), sep = " (?=[^ ]+$)")
length(schedule)
schedule
schedule <- schedule %>%
separate(`TIME (ET)`, into = c("TIME", "CHANNEL"), sep = " (?=[^ ]+$)")
schedule <- schedule[,1:length(schedule)]
schedule
library(rtweet) # get tweets
library(tidyverse)
library(magick)
library(stringr)
library(kableExtra)
library(knitr)
library(text2vec) #sentiment analysis
library(tm) #text mining (get_nrc_sentiment)
library(syuzhet) # nrc-word emotion associate lexicon
library(rvest) # html scraping
# Specifying url for site to be scraped
url <- 'http://www.espn.com/nfl/team/schedule/_/name/phi/philadelphia-eagles'
# Reading HTML code from the website
webpage <- read_html(url)
# get html from specific CSS selector from webpage
date_data_html <- html_nodes(webpage,'tr td')
# convert this data structure to text
date_data_text <- html_text(date_data_html)
# limit to just regular season
reg_season <- date_data_text[27:length(date_data_text)]
# get column headings from vector
headers <- reg_season[2:6]
# loops to get all game data into rows for each week
n = 7
i = 1
while (n < 52) {
assign(paste('row', as.character(i), sep = ''), reg_season[n:(n+4)])
n <- n + 5
i <- i + 1
}
# BYE week causes some issues
assign(paste('row', as.character(10), sep = ''), reg_season[52:53])
n = 54
i = 11
while (n < length(reg_season)) {
assign(paste('row', as.character(i), sep = ''), reg_season[n:(n+4)])
n <- n + 5
i <- i + 1
}
# build data frame
schedule <- data.frame(rbind(row1,row2,row3,row4,row5,row6,row7,row8,row9,row10,row11,row12,row13,row14,row15,row16,row17),
row.names = NULL)
# rename data frame columns
colnames(schedule) <- headers
# remove tickets column
schedule <- schedule[,1:4]
# convert from factor
#schedule$WK <- as.integer(schedule$WK)
schedule$DATE <- as.character(schedule$DATE)
schedule$OPPONENT <- as.character(schedule$OPPONENT)
schedule$`TIME (ET)` <- as.character(schedule$`TIME (ET)`)
# fix BYE WEEK
schedule[10,3] <- 'BYE WEEK'
schedule[10,4] <- NA
# split out date field
schedule <- schedule %>% separate(DATE, into = c('DAYOFWEEK', 'DATE'), sep = ',')
# remove @ symbol and 'vs' from opponent
schedule[] <- schedule %>%
lapply(gsub, pattern='@', replacement='') %>%
lapply(gsub, pattern='vs', replacement='')
schedule <- schedule %>%
separate(`TIME (ET)`, into = c("TIME", "CHANNEL"), sep = " (?=[^ ]+$)")
schedule <- schedule[,1:(length(schedule)-1)]
schedule
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
# calculate expected frequencies if null (probability of all suites are equal) were true
n = 200
expected.i <- n * probabilities
# expect 50 of each
observed.i <- c(35,51,64,50)
observed.i - expected.i
# don't want negatives, so square the deviations
(observed.i - expected.i)^2
((observed.i - expected.i)^2)/expected.i
#goodness of fit test statistic
chi.sq <- sum(((observed.i - expected.i)^2)/expected.i)
# calculate the 95th percentile of a chi-squared distribution w/ k - 1 = 3 dF
qchisq(.95,3)
# want an exact p-value for observed X2 value
pchisq(8.44,3, lower.tail = F)
# another way to get p-value
1 - pchisq(8.44,3)
# do all this w/ 1 function from lsr package
library(lsr)
load('cards.Rdata')
summary(suit.choice)
goodnessOfFitTest(suit.choice)
# new null
nullProbs <- c(clubs = .2, diamonds = .3, hearts = .3, spades = .2)
goodnessOfFitTest(suit.choice, p = nullProbs)
'*******************************************************************************************************'
load('chapek9.Rdata')
summary(chapek9)
# need a more detailed description of the dat
# look at the choices broken down by species via??? cross-tabulation
# Since data are stored in a data frame, use xtabs()
# produce contingency table
xtabs(~ choice + species, chapek9)
# chi-squared association test
library(lsr)
associationTest(~ choice + species, chapek9)
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/Stats/LSR')
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
# calculate expected frequencies if null (probability of all suites are equal) were true
n = 200
expected.i <- n * probabilities
# expect 50 of each
observed.i <- c(35,51,64,50)
observed.i - expected.i
# don't want negatives, so square the deviations
(observed.i - expected.i)^2
((observed.i - expected.i)^2)/expected.i
#goodness of fit test statistic
chi.sq <- sum(((observed.i - expected.i)^2)/expected.i)
# calculate the 95th percentile of a chi-squared distribution w/ k - 1 = 3 dF
qchisq(.95,3)
# want an exact p-value for observed X2 value
pchisq(8.44,3, lower.tail = F)
# another way to get p-value
1 - pchisq(8.44,3)
# do all this w/ 1 function from lsr package
library(lsr)
load('cards.Rdata')
summary(suit.choice)
goodnessOfFitTest(suit.choice)
# new null
nullProbs <- c(clubs = .2, diamonds = .3, hearts = .3, spades = .2)
goodnessOfFitTest(suit.choice, p = nullProbs)
'*******************************************************************************************************'
load('chapek9.Rdata')
summary(chapek9)
# need a more detailed description of the dat
# look at the choices broken down by species via??? cross-tabulation
# Since data are stored in a data frame, use xtabs()
# produce contingency table
xtabs(~ choice + species, chapek9)
# chi-squared association test
library(lsr)
associationTest(~ choice + species, chapek9)
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/Stats/LSR')
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/Stats/LSR/sample-master')
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
# calculate expected frequencies if null (probability of all suites are equal) were true
n = 200
expected.i <- n * probabilities
# expect 50 of each
observed.i <- c(35,51,64,50)
observed.i - expected.i
# don't want negatives, so square the deviations
(observed.i - expected.i)^2
((observed.i - expected.i)^2)/expected.i
#goodness of fit test statistic
chi.sq <- sum(((observed.i - expected.i)^2)/expected.i)
# calculate the 95th percentile of a chi-squared distribution w/ k - 1 = 3 dF
qchisq(.95,3)
# want an exact p-value for observed X2 value
pchisq(8.44,3, lower.tail = F)
# another way to get p-value
1 - pchisq(8.44,3)
# do all this w/ 1 function from lsr package
library(lsr)
load('cards.Rdata')
summary(suit.choice)
goodnessOfFitTest(suit.choice)
# new null
nullProbs <- c(clubs = .2, diamonds = .3, hearts = .3, spades = .2)
goodnessOfFitTest(suit.choice, p = nullProbs)
'*******************************************************************************************************'
load('chapek9.Rdata')
summary(chapek9)
# need a more detailed description of the dat
# look at the choices broken down by species via??? cross-tabulation
# Since data are stored in a data frame, use xtabs()
# produce contingency table
xtabs(~ choice + species, chapek9)
# chi-squared association test
library(lsr)
associationTest(~ choice + species, chapek9)
observed.i
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/Stats/LSR/sample-master')
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spades = .25)
# calculate expected frequencies if null (probability of all suites are equal) were true
n = 200
expected.i <- n * probabilities
# expect 50 of each
observed.i <- c(35,51,64,50)
observed.i - expected.i
# don't want negatives, so square the deviations
(observed.i - expected.i)^2
((observed.i - expected.i)^2)/expected.i
#goodness of fit test statistic
chi.sq <- sum(((observed.i - expected.i)^2)/expected.i)
# calculate the 95th percentile of a chi-squared distribution w/ k - 1 = 3 dF
qchisq(.95,3)
# want an exact p-value for observed X2 value
pchisq(8.44,3, lower.tail = F)
# another way to get p-value
1 - pchisq(8.44,3)
# do all this w/ 1 function from lsr package
library(lsr)
load('cards.Rdata')
summary(suit.choice)
goodnessOfFitTest(suit.choice)
# new null
nullProbs <- c(clubs = .2, diamonds = .3, hearts = .3, spades = .2)
goodnessOfFitTest(suit.choice, p = nullProbs)
'*******************************************************************************************************'
load('chapek9.Rdata')
summary(chapek9)
# need a more detailed description of the dat
# look at the choices broken down by species via??? cross-tabulation
# Since data are stored in a data frame, use xtabs()
# produce contingency table
xtabs(~ choice + species, chapek9)
# chi-squared association test
library(lsr)
associationTest(~ choice + species, chapek9)
'***************************************************************************************************'
# base R chi-sq goodness of git
observed.i
chi.sq(observed.i)
chisq.test(observed.i)
chisq.test(observed.i, nullProbs)
chisq.test(observed.i, p = nullProbs)
summary(chapek9)
chisq.test(chapek9)
xtabs(~ choice + species, chapek9)
chisq.test(xtabs(~ choice + species, chapek9))
cramersV(chapek9)
cramersV(xtabs(~ choice + species, chapek9))
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/R/Udacity/EDAWithR)
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/R/Udacity/EDAWithR')
list.files()
# load in FB tab data w/ read.csv but indicate the data is seperated by tab
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
head(facebook)
library(tidyverse)
glimpse(facebook)
# all ints but gender which is a factor
dim(facebook)
# 99k users --> variables describe user demos + their behavior (what they are doing on FB + what they use)
# histograms of user birthdays in 2 ways
library(ggplot2)
ggplot(facebook) + geom_histogram(aes(x = dob_day))
qplot(facebook$dob_day)
# break out the days of the month in the graph
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1)
# Majority of people born in 1st of month
# would expect similar counts among all days, except the 31st (as not all months have 31 days)
# break out histogram by dob_month into 12 histograms, one for each month --> i.e. break out by month
# do this w/ facet_wrap which breaks out plot by levels of a categorical variable
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month)
# now break out  by dob_month into 12 months into 3 rows SPECIFIED
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month, 3)
# now break out by dob_month into 12 months into 3 cols
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month, ncol = 3)
# days of birth are similar among all months, except Jan, with an abnormal amount of births on Jan 1
# possibly due to default values from Facebook
# facet_grid is similar, but we split by facet_grid(variableOnVerticalAxis ~ variableOnHorizontalAxis)
# facet_grid = better for 2+ variabels, facet_wrap = better for 1 variable
# create histogram of friend counts
ggplot(facebook) + geom_histogram(aes(x = friend_count))
qplot(x = friend_count, data = facebook)
# see long-tailed data w/ some having users well over 1000
# want to adjust code + plot to focus on bulk of user friend counts
qplot(x = friend_count, data = facebook, xlim = c(0,1000))
ggplot(facebook) +
geom_histogram(aes(x = friend_count)) +
scale_x_continuous(limits = c(0, 1000))
# try new bin widths
ggplot(facebook) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50))
qplot(x = friend_count, data = facebook, xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50))
# split these by genderggplot(facebook) +
ggplot(facebook) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
qplot(x = friend_count, data = facebook, xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
# see the NA's
ggplot(subset(facebook, is.na(gender))) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
ggplot(subset(facebook, !is.na(gender))) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
qplot(x = friend_count, data = subset(facebook, !is.na(gender)),
xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
table(facebook$gender)
?by
facebook %>%
by(gender, mean)
facebook %>%
by(facebook$gender, mean)
by(facebook$friend_count facebook$gender, mean)
by(facebook$friend_count, facebook$gender, mean)
by(facebook$friend_count, facebook$gender, mean)
by(facebook$friend_count, facebook$gender, summary)
ggplot(facebook) +
geom_histogram(aes(x = tenure), binwidth = 25)
ggplot(facebook) +
geom_histogram(aes(x = tenure))
ggplot(facebook) +
geom_histogram(aes(x = tenure), color = 'black')
?I
ggplot(facebook) +
geom_histogram(aes(x = tenure), color = 'black', fill = I('#099DD9'))
setwd('C:/Users/Nimz/Dropbox/DataScienceMasters/R/Udacity/EDAWithR')
# check wd for files
list.files()
# load in FB tab data w/ read.csv but indicate the data is seperated by tab
facebook <- read.csv('pseudo_facebook.tsv', sep = '\t')
head(facebook)
library(tidyverse)
glimpse(facebook)
# all ints but gender which is a factor
dim(facebook)
# 99k users --> variables describe user demos + their behavior (what they are doing on FB + what they use)
# histograms of user birthdays in 2 ways
library(ggplot2)
ggplot(facebook) + geom_histogram(aes(x = dob_day))
qplot(facebook$dob_day)
# break out the days of the month in the graph
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1)
# Majority of people born in 1st of month
# would expect similar counts among all days, except the 31st (as not all months have 31 days)
# break out histogram by dob_month into 12 histograms, one for each month --> i.e. break out by month
# do this w/ facet_wrap which breaks out plot by levels of a categorical variable
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month)
# now break out  by dob_month into 12 months into 3 rows SPECIFIED
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month, 3)
# now break out by dob_month into 12 months into 3 cols
ggplot(facebook) + geom_histogram(aes(x = dob_day), binwidth = 1) +
facet_wrap(~ dob_month, ncol = 3)
# days of birth are similar among all months, except Jan, with an abnormal amount of births on Jan 1
# possibly due to default values from Facebook
# facet_grid is similar, but we split by facet_grid(variableOnVerticalAxis ~ variableOnHorizontalAxis)
# facet_grid = better for 2+ variabels, facet_wrap = better for 1 variable
# create histogram of friend counts
ggplot(facebook) + geom_histogram(aes(x = friend_count))
qplot(x = friend_count, data = facebook)
# see long-tailed data w/ some having users well over 1000
# want to adjust code + plot to focus on bulk of user friend counts
qplot(x = friend_count, data = facebook, xlim = c(0,1000))
ggplot(facebook) +
geom_histogram(aes(x = friend_count)) +
scale_x_continuous(limits = c(0, 1000))
# try new bin widths
ggplot(facebook) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50))
qplot(x = friend_count, data = facebook, xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50))
# split these by genderggplot(facebook) +
ggplot(facebook) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
qplot(x = friend_count, data = facebook, xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
# see the NA's + remove them by subsetting the dataset to those records that are not NA
ggplot(subset(facebook, !is.na(gender))) +
geom_histogram(aes(x = friend_count), binwidth = 25) +
scale_x_continuous(limits = c(0, 1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
qplot(x = friend_count, data = subset(facebook, !is.na(gender)),
xlim = c(0,1000), binwidth = 25) +
scale_x_continuous(limits = c(0,1000), breaks = seq(0,1000,50)) +
facet_wrap(~ gender)
# can't really tell which gender has more friends on average
# get a table to see if there are more males or females
table(facebook$gender)
# a bit more males
# apply a function (mean friends) to the facebook dataset that is split by factors of gender
by(facebook$friend_count, facebook$gender, mean)
# females have more friends on average
# see more summary stats
by(facebook$friend_count, facebook$gender, summary)
# should use median bc these distributions are skewed, but median is still higher for female (resistant to extremes)
# to better understand users a bit more --> examine tenure (how many days using FB)
ggplot(facebook) +
geom_histogram(aes(x = tenure), color = 'black', fill = I('#099DD9'))
qplot(x = tenure, data = facebook,  color = 'black', fill = I('#099DD9'))
qplot(x = tenure, data = facebook,  color = 'black', fill = I('#099DD9'), bindwidth = 30)
qplot(x = tenure, data = facebook,  color = I('black'), fill = I('#099DD9'), bindwidth = 30)
ggplot(facebook) +
geom_histogram(aes(x = tenure), color = 'black', fill = I('#099DD9'), binwidth = 30)
qplot(x = tenure, data = facebook,  color = I('black'), fill = I('#099DD9'), bindwidth = 30)
qplot(x = tenure, data = facebook, bindwidth = 30, color = I('black'), fill = I('#099DD9'))
ggplot(facebook) +
geom_histogram(aes(x = tenure), color = 'black', fill = I('#099DD9'), binwidth = 30)
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), color = 'black', fill = I('#099DD9'))
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9'))
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9'))
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9')) +
scale_x_continuous(1, limits = c(0,7))
#qplot(x = tenure/365, data = facebook, bindwidth = 0.25, color = I('black'), fill = I('#099DD9'))
#qplot(x = tenure/365, data = facebook, bindwidth = 0.25, color = I('black'), fill = I('#099DD9'))
qplot(x = tenure/365, data = facebook, bindwidth = 0.25, color = I('black'), fill = I('#099DD9'))
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9')) +
scale_x_continuous(1, limits = c(0,7))
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9')) +
scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7))
ggplot(facebook) +
geom_histogram(aes(x = tenure/365), binwidth = 0.25, color = 'black', fill = I('#099DD9')) +
scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7)) +
xlab('Number of Years on Facebook') +
ylab('Count')
ggplot(facebook) +
geom_histogram(aes(x = age), binwidth = 2, color = 'black', fill = I('#099DD9')) +
#scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7)) +
xlab('Age') +
ylab('Count')
ggplot(facebook) +
geom_histogram(aes(x = age), binwidth = 5, color = 'black', fill = I('#099DD9')) +
#scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7)) +
xlab('Age') +
ylab('Count')
ggplot(facebook) +
geom_histogram(aes(x = age), binwidth = 10, color = 'black', fill = I('#099DD9')) +
#scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7)) +
xlab('Age') +
ylab('Count')
#looks like rollercoaster, most under age of 30
#
ggplot(facebook) +
geom_histogram(aes(x = age), binwidth = 1, color = 'black', fill = I('#099DD9')) +
#scale_x_continuous(breaks = seq(1,7,1), limits = c(0,7)) +
xlab('Age') +
ylab('Count')
ggplot(facebook) +
geom_histogram(aes(x = age), binwidth = 1, color = 'black', fill = I('#099DD9')) +
scale_x_continuous(breaks = seq(0,113,5)) +
xlab('Age') +
ylab('Count')
