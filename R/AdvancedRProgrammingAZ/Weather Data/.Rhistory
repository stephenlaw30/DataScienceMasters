spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
spotifyEndPoint <- oauth_endpoint(NULL,
"https://accounts.spotify.com/authorize",
"https://accounts.spotify.com/api/token")
spotifyKey <- "ab0f8dd5358b49668d5da5c7b2368b43"
spotifySecret <- "56681b43de334d8a9a9a352222d5c02e"
spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
?oauth2.0_token
spotifyToken <- ?oauth2.0_token(spotifyEndPoint, spotifyApp)
(150*617)+(16*238)
source('C:/Users/snewns/Dropbox/NewLearn/Edx/AnalyticsEdge/Unit8_LinearOptimization/8_1_AirlineRevenueMgmt.R')
$96,358 - $86,883
96358 - 86883
one of 0.20 (0.08 means there is an 8% chance a user who searches for best LTE network will click on the ad for AT&T if it
0.2*25
budget has dropps from 100 to 4, and the remaining displays of Query 3 has dropped to 0. So there are no more displays of Query 3 remaining that we can use.
So, our decisions are which teams should play each other each week and we model this with binary decision variables. The first constraint is that each team
15*7
(1030+92)/(1030+92+126+227)
Mount Sinai Hospital in Toronto uses integer optimization to schedule their ORs. Hospitals have a limited number of operating rooms,
of what they wanted. We want to maximize this percentage value *for every department*, so that is why we sum over all departments
weekly department OR requirements. Our last set of constraints has to do with departmental targets. We want to make sure we don't give any department more
Our objective value would be a max profitability of 269.9246814, more than 5X that of the greedy approach, with 7 hotels are selected in the solution?
I22x1,22 â‰¤ 1.2** to say the index of SR 1 at brick 1 plus the index of SR at brick 2 etc. We should have a similar constraint in our model for every SR (1 - 4).
administrative staff could adjust the constraints depending on the importance of the teacher preferences versus parent preferences.
set.seed(200)
kmc <- kmeans(housesNorm, centers = 10, iter.max = 1000)
Pt. 2 - UNDERSTANDING RETAIL CONSUMERS
**Clustering** can be used for **market segmentation**, the idea of dividing airline passengers into small, more similar
groups, and then designing a marketing strategy specifically for each group. We will see how this idea can be applied to
retail consumer data with data collected over 2 years from source files provided by dunnhumby, a customer science company
based in the UK, for a group of 2.5K households. Each row represents a unique household with the following variables:
* NumVisits       = the number of times the household visited the retailer
* AvgProdCount    = the average number of products purchased per transaction
* AvgDiscount     = the average discount per transaction from coupon usage (in %) - NOTE: Do not divide by 100!
* AvgSalesValue   = the average sales value per transaction
* MorningPct      = the percentage of visits in the morning (8am - 1:59pm)
* AfternoonPct    = the percentage of visits in the afternoon (2pm - 7:59pm)
Note that some visits can occur outside of morning and afternoon hours (visits from 8pm - 7:59am are possible)'
houses <- read.csv("Households.csv")
head(houses)
nrow(subset(houses, houses$MorningPct == 100))
nrow(subset(houses, houses$AfternoonPct == 100))
'4 households have logged transactions at the retailer only in the morning and 13 households have logged transactions at
the retailer only in the afternoon.
Of households that spend over $150 per transaction on average, the minimum average discount per transaction is 15.64%'
min(houses$AvgDiscount[houses$AvgSalesValue > 150])
'Of households who have an average discount per transaction greater than 25%, the minimum average sales value per
transaction is 50.1175'
min(houses$AvgSalesValue[houses$AvgDiscount > 25])
'In the dataset, a proportion of 0.0592 or 5.92% of households visited the retailer at least 300 times'
nrow(subset(houses, houses$NumVisits >= 300))/nrow(houses)
'When clustering data, it is often important to normalize the variables so that they are all on the same scale. If you
clustered this dataset without normalizing, we would expect to NumVisits dominate in the distance calculations, because
it is on the largest scale.
Normalize all variables'
library(caret)
preproc = preProcess(houses)
housesNorm = predict(preproc, houses)
'Remember that for each variable, the normalization process subtracts the mean and divides by the standard deviation. So,
in this normalized dataset, all variables should have mean = 0 and standard deviation = 1.
The maximum value of NumVisits in the normalized dataset is 10.28281 and the the minimum value of AfternoonPct in the
normalized dataset is -3.228427.
Create a dendrogram of the normalized data'
set.seed(200)
distances <- dist(housesNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
'Based on the dendrogram, 2, 3, or 5 clusters would be appropriate for this problem. 4 or 6 clusters have very little
"wiggle room", which means the additional clusters are not very distinct from existing clusters. That is, when moving
from 3 clusters to 4 clusters, the additional cluster is *very similar to an existing one* (as well as when moving from 5
clusters to 6 clusters).
Run the k-means clustering algorithm on the normalized dataset, selecting 10 clusters. Right before using kmeans(), set
the seed to 200 again.'
set.seed(200)
kmc <- kmeans(housesNorm, centers = 10, iter.max = 1000)
kmc
str(kmc)
table(kmc$cluster)
order(table(kmc$cluster))
sort(table(kmc$cluster))
healthyClusters <- kmc$cluster
healthyClusters
kmc
str(kmc)
cluster1 <- subset(housesNorm, kmc == 1)
cluster1 <- subset(housesNorm, kmc$cluster == 1)
cluster1 <- subset(housesNorm, kmc$cluster == 1)
cluster2 <- subset(housesNorm, kmc$cluster == 2)
cluster3 <- subset(housesNorm, kmc$cluster == 3)
cluster4 <- subset(housesNorm, kmc$cluster == 4)
cluster5 <- subset(housesNorm, kmc$cluster == 5)
cluster6 <- subset(housesNorm, kmc$cluster == 6)
cluster7 <- subset(housesNorm, kmc$cluster == 7)
cluster8 <- subset(housesNorm, kmc$cluster == 8)
cluster9 <- subset(housesNorm, kmc$cluster == 9)
cluster10 <- subset(housesNorm, kmc$cluster == 10)
cluster1
tail(sort(colMeans(cluster2)))
head(cluster1)
head(cluster1)Submit You have used 0 of 1 attempt Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Show Answer
head(cluster1)
cluster2
head(cluster2)
head(cluster3)
head(cluster4)
head(cluster5)
head(cluster6)
head(cluster7)
head(cluster4) #
head(cluster7)
head(cluster8)
head(cluster9)
head(cluster10)
head(cluster4) #
head(cluster7) #
kmc
str(kmc)
head(kmc)
head(kmc) #check 'centers'
head(kmc) #check 'centers'
set.seed(5000)
kmc <- kmeans(housesNorm, centers = 5, iter.max = 1000)
sort(table(kmc$cluster))
str(kmc)
head(kmc)
rm(list = ls(all = TRUE))
energy <- read.csv("energy.csv")
energy <- read.csv("energy.csv")
table(energy$STATE,energy$GenTotalRenewable)
sort(table(energy$STATE,energy$GenTotalRenewable))
order(table(energy$STATE,energy$GenTotalRenewable))
sort(table(energy$GenTotalRenewable,energy$STATE))
table(energy$GenTotalRenewable,energy$STATE)
table(energy$STATE,energy$GenTotalRenewable)
summary(energy$STATE,energy$GenTotalRenewable)
summary(energy$GenTotalRenewable)
table(energy$STATE,mean(energy$GenTotalRenewable))
table(energy$STATE,mean(energy$GenTotalRenewable)
energy[energy$STATE == 'AZ',]
energy[energy$STATE == 'AZ',]
sum(energy$GenTotalRenewable[energy$STATE == 'AZ'])
sum(energy$GenTotalRenewable[energy$STATE == 'CA'])
sum(energy$GenTotalRenewable[energy$STATE == 'ID'])
sum(energy$GenTotalRenewable[energy$STATE == 'MA'])
max(energy$GenTotalRenewable[energy$STATE == 'AZ'])
max(energy$GenTotalRenewable[energy$STATE == 'CA'])
max(energy$GenTotalRenewable[energy$STATE == 'ID'])
max(energy$GenTotalRenewable[energy$STATE == 'MA'])
tapply(energy$GenTotalRenewable,energy$STATE, max)
sort(tapply(energy$GenTotalRenewable,energy$STATE, max))
which.max(energy$GenTotalRenewable[energy$STATE == 'ID'])
energy[which.max(energy$GenTotalRenewable[energy$STATE == 'ID'])]
energy[which.max(energy$GenTotalRenewable[energy$STATE == 'ID']),]
energy$GenTotalRenewable[energy$STATE == 'ID']
which.max(energy$GenTotalRenewable[energy$STATE == 'ID']
)
idaho <- subset(energy, energy$STATE == 'ID')
idaho
energy[which.max(idaho$GenTotalRenewable,]
energy[which.max(idaho$GenTotalRenewable)
]
idaho$GenTotalRenewable
which.max(idaho$GenTotalRenewable)
idaho[which.max(idaho$GenTotalRenewable)]
idaho[which.max(idaho$GenTotalRenewable),]
energy$AllSourcesCO2[energy$presidential.results == 0]
mean(energy$AllSourcesCO2[energy$presidential.results == 0])
mean(energy$AllSourcesCO2[energy$presidential.results == 0], na.rm = TRUE)
mean(energy$AllSourcesCO2[energy$presidential.results == 1], na.rm = TRUE)
mean(energy$AllSourcesNOx[energy$presidential.results == 0], na.rm = TRUE)
mean(energy$AllSourcesNOx[energy$presidential.results == 1], na.rm = TRUE)
cor(energy$AllSourcesCO2,energy$EPriceIndustrial, use="complete") #use = 'complete' handles NA's
cor(energy$EPriceIndustrial,energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial ,energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesSO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesNOx, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesSO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesResidential, energy$AllSourcesNOx, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesCommercial, energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
library(ggplot2)
library(ggplot2)
ggplot(data = energy, aes(x = State, y = EPriceTotal)) + geom_boxplot()
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot()
tapply(energy$EPriceTotal, energy$STATE, mean)
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
tapply(energy$EPriceTotal, energy$STATE, mean)
sort(tapply(energy$EPriceTotal, energy$STATE, mean))
cor(energy$STATE[energy$STATE == 'WY'],max(mean(energy$GenTotal)))
set.seed(144)
spl = sample(1:nrow(energy), size = 0.7*nrow(energy))
train = energy[spl,]
test = energy[-spl,]
model1 <- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import,
data = train, family = "binomial")
summary(model1)
logModelPredictions <- predict(model1, test, type = "response")
table(test$GenSolarBinary, logModelPredictions > 0.5)
154+18/nrow(test)
(154+18)/nrow(test) #accuracy of
table(test$GenSolarBinary[test$presidential.results == 0], logModelPredictions > 0.5)
republicanLogModelPredictions <- predict(model1, test[test$presidential.results == 0], type = "response")
republicanLogModelPredictions <- predict(model1, test[test$presidential.results == 0,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 0], republicanLogModelPredictions > 0.5)
democratLogModelPredictions <- predict(model1, test[test$presidential.results == 1,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 0], democratLogModelPredictions > 0.5)
democratLogModelPredictions <- predict(model1, test[test$presidential.results == 1,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 1], democratLogModelPredictions > 0.5)
(90+2)/nrow(test[test$presidential.results == 0])
(90+2)/nrow(test[test$presidential.results == 0,])
table(test$GenSolarBinary[test$presidential.results == 1], democratLogModelPredictions > 0.5)
(64+16)/nrow(test[test$presidential.results == 1,])
train.limited <- train[,c('CumlRegulatory','CumlFinancial','presidential.results','Total.salary','Import')]
test.limited <- test[,c('CumlRegulatory','CumlFinancial','presidential.results','Total.salary','Import')]
library(caret)
preProcess(train.limited)
library(caret)
reproc = preProcess(train.limited)
train.limited.norm = predict(preproc, train.limited)
library(caret)
preproc = preProcess(train.limited)
train.limited.norm = predict(preproc, train.limited)
preproc2 = preProcess(test.limited)
test.limited.norm = predict(preproc2, test.limited)
set.seed(144)
kmc.train.norm <- kmeans(train.limited, centers = 2, iter.max = 1000)
str(kmc.train.norm)
head(kmc.train.norm)
install.packages("flexclust")
library(flexclust)
?flexclust
?as.kcca
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
cluster.train.norm
str(kmc.train.norm)
head(kmc.train.norm)
train.cluster1 <- subset(train.limited.norm, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train.limited.norm, kmc.train.norm$cluster == 2)
summary(train.cluster1)
summary(train.cluster2)
train.limited.norm
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
summary(train.cluster1)
summary(train.cluster2)
cluster.train.norm
model2 <- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import,
data = train.cluster1, family = "binomial")
summary(model2)
test.cluster1 <- subset(test, kmc.train.norm$cluster == 1)
test.cluster2 <- subset(test, kmc.train.norm$cluster == 2)
logModelPredictions2 <- predict(model2, test, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
test.cluster1
logModelPredictions2
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+25)/nrow(test.cluster1)
logModelPredictions2 <- predict(model2, test.limited.norm, type = "response")
table(test.limited.norm$GenSolarBinary, logModelPredictions2 > 0.5)
?cluster.test
logModelPredictions2 <- predict(model2, test.limited.norm, type = "response")
table(test.limited.norm$GenSolarBinary, logModelPredictions2 > 0.5)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+25)/nrow(test.cluster1)
test.cluster1
nrow(test.cluster1)
(126+25)/nrow(test.cluster1)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+13)(126+13+18+25)
(126+13)/(126+13+18+25)
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
cluster.test.norm
kcm.test.norm.kcaa
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
kcm.test.norm.kcaa
kmc.train.norm.kcaa
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
as.kcca
?as.kcca
set.seed(144)
kmc.test.norm <- kmeans(test.limited, centers = 2, iter.max = 1000)
str(kmc.test.norm)
head(kmc.test.norm)
kmc.train.norm.kcaa <- ?as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
kmc.test.norm
str(kmc.test.norm)
head(kmc.test.norm)
kcm.test.norm.kcaa <- as.kcca(kmc.test.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm$cluster == 2)
head(kmc.test.norm)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kmc.test.norm$cluster == 1)
test.cluster2 <- subset(test, kmc.test.norm$cluster == 2)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
2+22/nrow(test.cluster1)
(40*20) + (40*4) + (35*32) + (35*0)
library(SDSFoundations)
animaldata <- AnimalData
install.packages("C:/Users/snewns/Dropbox/DataScienceMasters/Stats/UT/SDSFoundations_1.1.zip", repos = NULL, type = "win.binary")
library(SDSFoundations)
animaldata <- AnimalData
table(animaldata$Age.Intake)
str(animaldata)
table(animaldata$Age.Intake)
adultAnimals <- subset(animaldata, Age.Intake >= 1)
adultAnimals
table(adultAnimals$Age.Intake)
table(adultAnimals$Animal.Type)
adultDogs <- subset(adultDogs, Animal.Type == 'Dog')
adultDogs <- subset(adultAnimals, Animal.Type == 'Dog')
adultCats <- subset(adultAnimals, Animal.Type == 'Cat')
ggplot(adultAnimals) + geom_histogram(aes(x = adultDogs))
ggplot(adultDogs) + geom_histogram(aes(x = Weight))
library(ggplot2)
ggplot(adultDogs) + geom_histogram(aes(x = Weight))
ggplot(adultCats) + geom_histogram(aes(x = Weight))
mean(adultCats$Weight)
sd(adultCats$Weight)
(13 - 8.6)/1.9
mean(adultAnimals$Weight)
sd(adultAnimals$Weight)
(13 - 30.29539) / 23.6356
zScore.cat <- 2.3
1-pnorm(zScore.cat)
fivenum(adultDogs$Weight)
fivenum(adultCats$Weight)
mean(adultDogs$Weight)
sd(adultDogs$Weight)
(13 - 35.67) / 23.47
zScore.dog <- -0.97
1-pnorm(zScore.dog)
deliverable3 <- sapply(weather, min) #<-- DELIVERABLE 2
setwd("C:/Users/Nimz/Dropbox/DataScienceMasters/R/AdvancedRProgrammingAZ/Weather Data")
'Need:
* table with annual averages of each observed metric for every city
* table showing how much temperatures fluctuate each month from min to max %, with min temperature as the base
* table showing annual minimums of each observed metric for every city
* table showing annual maximums of each observed metric for every city
* table showing in which months the annual maximimums of each metric were observed in every city'
chicago_cel <- read.csv("Chicago-C.csv", row.names = 1) #takes row names from 1st col in CSV
chicago_far <- read.csv("Chicago-F.csv", row.names = 1)
str(chicago_far)
newYork_cel <- read.csv("NewYork-C.csv", row.names = 1)
newYork_far <- read.csv("NewYork-F.csv", row.names = 1)
houston_cel <- read.csv("Houston-C.csv", row.names = 1)
houston_far <- read.csv("Houston-F.csv", row.names = 1)
sanFran_cel <- read.csv("SanFrancisco-C.csv", row.names = 1)
sanFran_far <- read.csv("SanFrancisco-F.csv", row.names = 1)
#Chi, Hour, SF measured from 1980-2010, NY from 1960-1990, so all 30-yr periods
#convert data frames to matrix
chicago_far <- as.matrix(chicago_far)
newYork_far <- as.matrix(newYork_far)
houston_far <- as.matrix(houston_far)
sanFran_far <- as.matrix(sanFran_far)
#put all matrices into a list
weather <- list(chicago_far, newYork_far, houston_far, sanFran_far)
weather
#give list indices proper names
weather <- list(chicago = chicago_far, newYork = newYork_far, houston = houston_far, sanFran = sanFran_far)
#access houston as part/subset of the list
weather[3]
#access houston matrix component only, not the list part
weather[[3]]
# OR
weather$houston
#get mean of rows in Houston
apply(weather$houston, 1, mean)
#get max of rows in Houston
apply(weather$houston, 1, max)
#get mean of cols in Houston
apply(weather$houston, 2, mean)
#get mean of cols in Houston
apply(weather$houston, 2, max)
#apply - to use on matrix rows or cols
#tapply - to use on a vector to extract subgroups and apply function to them
#by  - same concept as GROUP BY in SQL, for data frames
#eapply - use on an environment
#lapply - to apply function to all elements of a list
#sapply - version of lapply to simplify results so that it's not presented as a list and as a vector or matric
#vapply - pre-specified return values (V)
#replicate - run a function severalt imes, usually used with generation of random variables
#mapply - multivariate version of sapply so arguments can be recycled
#rapply - recursive version of lapply
#1) table with annual averages of each observed metric for every city
apply(weather$chicago, 1, mean)
apply(weather$newYork, 1, mean)
apply(weather$houston, 1, mean)
apply(weather$sanFran, 1, mean)
#same as above w/ loops
output <- NULL
for (i in 1:5){
output[i] <- mean(chicago_far[i,])
}
output
names(output) <- rownames(chicago_far)
output
#almost anything we can make with a loop can be made with a function from the apply family
#***************lapply - returns list from a list or vector***************
#transport Chicago rows + cols w/ transpose function t()
t(chicago_far)
#tranpose all matrices in weather
lapply(weather, t)
#ex 2
rbind(chicago_far, newRow = 1:12)
#list, function, optional parameter
lapply(weather, rbind, newRow = 1:12)
#ex 3 - get list of names vectors for each city with means of the rows for each city
lapply(weather, rowMeans)
#combine lapply w/ []
weather[[1]][1,1]
weather$chicago[1,1]
#extract avg high temp in jan for all cities
lapply(weather, "[", 1, 1)
#by default, lapply iterates over all components of "weather", so it goes weather[[1]], weather[[2]], weather[[3]]....
#our "[" function would be the *second* set of []'s --> [1,1], and we pass in 1,1 as optional parameters
#get 1st row for each city
lapply(weather, "[", 1,)
#get just march data
lapply(weather, "[", , 3)
#apply/add own functions
lapply(weather, function(x) x[1,]) #extract component x from weather + apply custom function to component to get 1st row from matrix
lapply(weather, function(x) x[,1]) #extract component x from weather + apply custom function to component to get Jan col from matrix
lapply(weather, function(x) x[5,]) #extract component x from weather + apply custom function to component to get 5th row from matrix
lapply(weather, function(x) x[,12]) #extract component x from weather + apply custom function to component to get Dec col from matrix
lapply(weather, function(x) x[,1]) #extract component x from weather + apply custom function to component to get Jan col from matrix
#get avg high temp and subtract avg low temp
lapply(weather, function(z) z[1,] - z[2,])
#get relative change in temp
lapply(weather, function(z) round((z[1,] - z[2,])/z[2,],4)*100)
#see chicago fluctuates almost 78% from avg. high to avg. low temps in January throughout the 30 years
#sapply is a simplified lapply that returns a vector or a matrix (wrapper of lapply)
sapply(weather, function(z) z[1,] - z[2,])
sapply(weather, function(z) round((z[1,] - z[2,])/z[2,],4)*100)
#see we get a matrices
#get avg. high temp for july
lapply(weather, function(z) z[1,7]) #lapply(weather, "[", 1, 7)
sapply(weather, function(z) z[1,7]) #sapply(weather, "[", 1, 7)
#get a named vector from sapply
#get avg. high temp for q4
lapply(weather, function(z) z[1,10:12])
sapply(weather, function(z) z[1,10:12])
deliverable1 <- round(sapply(weather, rowMeans),2)  #<-- DELIVERABLE 1
deliverable2 <- sapply(weather, function(z) round((z[1,] - z[2,])/z[2,],4)*100) #<-- DELIVERABLE 2
deliverable3 <- sapply(weather, min) #<-- DELIVERABLE 2
deliverable3
deliverable4 <- sapply(weather, max) #<-- DELIVERABLE 4
apply(Chicago, 1, max)
apply(chicago_far, 1, max)
sapply(weather, function(z) apply(z,1,max))
sapply(weather, function(z) apply(z,1,min))
chicago_far
sapply(weather, function(z) apply(z,2,max)) #<- deliverable 4
sapply(weather, function(z) apply(z,2,which.max)) #<- deliverable 4
sapply(weather, function(z) apply(z,1,which.max)) #<- deliverable 4
sapply(weather, function(z) apply(z,1,z[which.max])) #<- deliverable 5 --> need names
sapply(weather, function(z) apply(z,1,z[,which.max])) #<- deliverable 5 --> need names
chicago_far
sapply(weather, function(z) z[,apply(z,1,which.max])) #<- deliverable 5 --> need names
sapply(weather, function(z) z[,apply(z,1,which.max)])) #<- deliverable 5 --> need names
sapply(weather, function(z) z[,apply(z,1,which.max)]) #<- deliverable 5 --> need names
sapply(weather, function(z) apply(z,1,names(which.max)) #<- deliverable 5 --> need names
)
sapply(weather, function(z) apply(z,1,names(which.max)))
sapply(weather, function(z) names(apply(z,1,(which.max)) #<- deliverable 5 --> need names
)
)
apply(z,1,(which.max)
sapply(weather, function(z) (apply(z,1,(which.max)))) #<- deliverable 5 --> need names
sapply(weather, function(z) apply(z,1,which.max)) #<- deliverable 5 --> need names
sapply(weather, function(z) names(apply(z,1,which.max)))
sapply(weather, function(z) apply(names(z),1,which.max)) #<- deliverable 5 --> need names
sapply(weather, function(z) apply(z,1, function(x) names(which.max(x)))) #<-- deliverable 3
