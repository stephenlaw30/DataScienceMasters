predicted[i] <- as.integer(fitted(model)+0.5)
}
#calculate accuracy/fraction of correct predictions by taking sum of all predicted records that match outcome
# and dividing by # all records
acc <- sum(predicted == cc[,11]) / nrow(cc)
}
#check accuracy for 1 neighbor up to 20 neighbors
acc_20 <- rep(0,20)
for (j in 1:length(acc_20)) {
acc_20[j] <- check_acc(j)
}
#check
acc_20
library(kknn)
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
check_acc <- function(x) {
predicted <- rep(9,nrow(cc)) #empty vector to hold predictions of each record's outcome
for (i in 1:nrow(cc)) {
#create model + include data[-i] to remove row i when finding its nearest neighbors
# - if not removed, it will be its own neatest neighbor
model <- kknn(R1~.,      # formula to train on
cc[-i,],  # data to train on = data set w/ current row removed
cc[i,],  # data to test on = data set only w/ current row
k = x,     # number of neighbors = input variable
scale = T, # scale the data for us
cross = 10) # number of folds
#record whether prediciton is at least 0.5 (round to 1) or less than 0.5 (round to 0)
#not given integers by model output
predicted[i] <- as.integer(fitted(model)+0.5)
}
#calculate accuracy/fraction of correct predictions by taking sum of all predicted records that match outcome
# and dividing by # all records
acc <- sum(predicted == cc[,11]) / nrow(cc)
}
acc_20 <- rep(0,20)
for (j in 1:length(acc_20)) {
acc_20[j] <- check_acc(j)
}
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
#need data in a matrix for vanilladot simple linear kernel
cc_m <- as.matrix(cc)
library(lme4)
library(geepack)
library(survival)
library(kernlab)
set.seed(1)
#call kvsm w/ vanilladot simle linear kernel
model <- ksvm(V11~.,                  #predict V11 outcome variable by all others
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 100,                #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
model <- ksvm(V11~.,                  #predict V11 outcome variable by all others
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 100,                #lambda
scaled = T,             #have ksvm scale the data
cross = 10)
model <- ksvm(V11~.,                  #predict V11 outcome variable by all others
cc,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 100,                #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
cc_m
model <- ksvm(V11~.,                  #predict V11 outcome variable by all others
cc_m,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 100,                #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
#need data in a matrix for vanilladot simple linear kernel
cc_m <- as.matrix(cc)
library(lme4)
library(geepack)
library(survival)
library(kernlab)
set.seed(1)
#call kvsm w/ vanilladot simle linear kernel
model <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_m,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 100,                #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
model
a <- colSums(model@xmatrix[[1]]*model@coef[[1]])
a0 <- -model@b
a
a0
predictedScaled <- rep(NA,nrow(cc_m))
for (i in 1:nrow(cc_m)) {
#if predicted value is above classifier's value, predict 1, if below, predict 0
if (sum(a*(cc_m[1,1:10] - model@scaling$x.scale$`scaled:center`)/model@scaling$x.scale$`scaled:scale`) + a >= 0) {
predictedScaled[i] <- 1
}
if (sum(a*(cc_m[i,1:10] - model@scaling$x.scale$`scaled:center`)/model@scaling$x.scale$`scaled:scale`) + a < 0) {
predictedScaled[i] <- 0
}
}
pred <- predict(model,cc_m[,1:10])
pred
sum(pred == cc_m[,11]) / nrow(cc_m)
sum(predictedScaled == cc_m[,11]) / nrow(cc_m)
1 - model@error
library(kernlab)
library(kknn)
#load data
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
set.seed(1)
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
?
?sampl
?sample
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- data[mask_train,]
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
mask_train
cc_train <- data[-mask_train,]
data
cc_train <- cc[-mask_train,]
cc_train <- cc[mask_train,]
remainingRows <- cc[-mask_train,]
mask_val
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.2))
cc_val <- remainingRows[mask_val]
cc_test <- remainingRows[-mask_val]
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
set.seed(1)
'**********SPLITTING**************'
#split into 60-20-20 for training, validation, and testing
#create a "mask" using sample() --> set of row indices, such as (1,4,5,8) for those respective rows
#60% training via a random sample out of specified #'s to choose from (# of rows in dataset)
#   with size specified as the 60% of total rows given
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.2))
cc_val <- remainingRows[mask_val,]
cc_test <- remainingRows[-mask_val,]
acc <- rep(0,29)
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
set.seed(1)
'**********SPLITTING**************'
#split into 60-20-20 for training, validation, and testing
#create a "mask" using sample() --> set of row indices, such as (1,4,5,8) for those respective rows
#60% training via a random sample out of specified #'s to choose from (# of rows in dataset)
#   with size specified as the 60% of total rows given
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.5))
cc_val <- remainingRows[mask_val,]
cc_test <- remainingRows[-mask_val,]
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
acc <- rep(0,29)
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in c_amounts) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
a <- colSums(model@xmatrix[[1]]*model@coef[[1]])
a0 <- -model@b
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
acc[i] <- 1 - modelTrain@error #sum(predictions == cc_val[,11]) / nrow(cc_val)
}
acc
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in c_amounts) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
acc[i] <- 1 - modelTrain@error #sum(predictions == cc_val[,11]) / nrow(cc_val)
}
acc
acc <- rep(0,29)
acc
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in 1:9) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
acc[i] <- 1 - modelTrain@error #sum(predictions == cc_val[,11]) / nrow(cc_val)
}
#check accuracy values for each c value
acc[1:9]
cat('The best SVM model is model number ',which.max(acc[1:9]), ', with an validation set correctness/accuracy of ',
max(acc[1:9]), ', with a C/lambda value of ', c_amounts[which.max(acc[1:9])], ', ')
cat('The best SVM model is model number', which.max(acc[1:9]), ', with an validation set correctness/accuracy of ',
max(acc[1:9]), ', with a C/lambda value of ', c_amounts[which.max(acc[1:9])], ', ')
cat('The best SVM model is model number', which.max(acc[1:9]), ', with an validation set correctness/accuracy of',
max(acc[1:9]), ', with a C/lambda value of', c_amounts[which.max(acc[1:9])], ', ')
cat('The best SVM model is model number', which.max(acc[1:9]), 'with an validation set correctness/accuracy of',
max(acc[1:9]), 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
setwd('C:/Users/NEWNSS/Dropbox/DataScienceMasters/GATech/Modeling')
cc <- read.table('credit_card_data-headers.txt', header = T, sep = '\t')
head(cc)
set.seed(1)
'**********SPLITTING**************'
#split into 60-20-20 for training, validation, and testing
#create a "mask" using sample() --> set of row indices, such as (1,4,5,8) for those respective rows
#60% training via a random sample out of specified #'s to choose from (# of rows in dataset)
#   with size specified as the 60% of total rows given
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.5))
cc_val <- remainingRows[mask_val,]
cc_test <- remainingRows[-mask_val,]
#pick best 9 SVM models + best 20 KNN models via storing all results in 1 vector
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in 1:9) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
acc[i] <- 1 - modelTrain@error #sum(predictions == cc_val[,11]) / nrow(cc_val)
}
#check accuracy values for each c value
acc[1:9]
set.seed(1)
'**********SPLITTING**************'
#split into 60-20-20 for training, validation, and testing
#create a "mask" using sample() --> set of row indices, such as (1,4,5,8) for those respective rows
#60% training via a random sample out of specified #'s to choose from (# of rows in dataset)
#   with size specified as the 60% of total rows given
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.5))
cc_val <- remainingRows[mask_val,]
cc_test <- remainingRows[-mask_val,]
#pick best 9 SVM models + best 20 KNN models via storing all results in 1 vector
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in 1:9) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
acc[i] <- 1 - modelTrain@error #sum(predictions == cc_val[,11]) / nrow(cc_val)
}
#check accuracy values for each c value
acc[1:9]
cat('The best SVM model is model number', which.max(acc[1:9]), 'with an validation set correctness/accuracy of',
max(acc[1:9]), 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 0.01,       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
set.seed(1)
'**********SPLITTING**************'
#split into 60-20-20 for training, validation, and testing
#create a "mask" using sample() --> set of row indices, such as (1,4,5,8) for those respective rows
#60% training via a random sample out of specified #'s to choose from (# of rows in dataset)
#   with size specified as the 60% of total rows given
mask_train <- sample(nrow(cc), size = floor(nrow(cc) * 0.6))
#create set via random rows from above
cc_train <- cc[mask_train,]
#20% validation + 20% test
remainingRows <- cc[-mask_train,]
mask_val <- sample(nrow(remainingRows), size = floor(nrow(remainingRows) * 0.5))
cc_val <- remainingRows[mask_val,]
cc_test <- remainingRows[-mask_val,]
#pick best 9 SVM models + best 20 KNN models via storing all results in 1 vector
acc <- rep(0,29)
'*********TRAIN SVM**************'
#9 values of C to test
c_amounts <- c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000)
#fit model w/ training set for each value of c
for (i in 1:9) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
#acc[i] <- 1 - modelTrain@error
acc[i] <- sum(predictions == cc_val[,11]) / nrow(cc_val)
}
#check accuracy values for each c value
acc[1:9]
cat('The best SVM model is model number', which.max(acc[1:9]), 'with an validation set correctness/accuracy of',
max(acc[1:9]), 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
set.seed(1)
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 0.01,       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
cat('Best model perfomance on test set:', sum(predictions == cc_test[,11])/nrow(cc_test))
modelTrain
for (i in 1:9) {
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = c_amounts[i],       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
#check preditions from model against validation set
predictions <- predict(modelTrain,cc_val)
#store in vector our accuracy of current c value model
#acc[i] <- 1 - modelTrain@error
acc[i] <- sum(predictions == cc_val[,11]) / nrow(cc_val)
}
acc[1:9]
cat('The best SVM model is model number', which.max(acc[1:9]), 'with an validation set correctness/accuracy of',
max(acc[1:9]), 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
set.seed(1)
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 0.01,       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
modelTrain
cat('Best model perfomance on test set:', sum(predictions == cc_test[,11])/nrow(cc_test))
cat('Best model perfomance on test set:', sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test))
cat('The best SVM model is model number', which.max(acc[1:9]), 'with an validation set correctness/accuracy of',
max(acc[1:9]), 'with a C/lambda value of', c_amounts[which.max(acc[1:9]
)])
for (k in 1:20) {
#create model + use train and validation sets in this version
modelTrain <- kknn(R1~.,      # formula to train on
cc_train,  # data to train on = data set w/ current row removed
cc_val,    # data to test on = data set only w/ current row
k = k,     # number of neighbors = input variable
scale = T) # scale the data for us
#record whether prediciton is >= 0.5 (round to 1) or < 0.5 (round to 0) b/c model doesn't give int results
predicted <- as.integer(fitted(modelTrain)+0.5)
#store current model accuracy in acc vector
acc[k+9] <- sum(predicted == cc_val[,11]) / nrow(cc_val)
}
acc[10:29]
cat('The best KNN model is the model with k =', which.max(acc[10:29]), 'folds with an validation set
correctness/accuracy of',max(acc[10:29])
)
cat('The best KNN model is the model with k =', which.max(acc[10:29]),
'folds with an validation set correctness/accuracy of',max(acc[10:29]))
set.seed(1)
modelTrain <- kknn(R1~.,      # formula to train on
cc_train,  # data to train on = data set w/ current row removed
cc_val,    # data to test on = data set only w/ current row
k = 16,     # number of neighbors = input variable
scale = T) # scale the data for us
cat('Best model perfomance on test set:', sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test))
#get predictions
predicted <- as.integer(fitted(modelTrain)+0.5)
cat('Best model perfomance on test set:', sum(predicted == cc_test[,11])/nrow(cc_test))
modelTrain <- kknn(R1~.,      # formula to train on
cc_train,  # data to train on = data set w/ current row removed
cc_val,    # data to test on = data set only w/ current row
k = which.max(acc[10:29]),     # number of neighbors = input variable
scale = T) # scale the data for us
predicted <- as.integer(fitted(modelTrain)+0.5)
cat('Best model perfomance on test set:', sum(predicted == cc_test[,11])/nrow(cc_test))
modelTrain <- kknn(R1~.,      # formula to train on
cc_train,  # data to train on = data set w/ current row removed
cc_test,    # data to test on = data set only w/ current row
k = which.max(acc[10:29]),     # number of neighbors = input variable
scale = T) # scale the data for us
#get predictions
predicted <- as.integer(fitted(modelTrain)+0.5)
cat('Best model perfomance on test set:', sum(predicted == cc_test[,11])/nrow(cc_test))
cat('The best KNN model is the model with k =', which.max(acc[10:29]),
'folds with an validation set correctness/accuracy of',max(acc[10:29]))
cat('Best model perfomance on test set:', best_knn_model_acc)
best_knn_model_acc <- sum(predicted == cc_test[,11])/nrow(cc_test)
cat('Best model perfomance on test set:', best_knn_model_acc)
best_svm_model_acc <- sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
#test best model on test set
#cat('Best model perfomance on test set:', 1 - modelTrain@error)
cat('Best model perfomance on test set:', best_svm_model_acc)
best_svm_model_acc <- sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test))
sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
predict(modelTrain,cc_test)
modelTrain <- ksvm(R1~.,                  #predict V11 outcome variable by all others
cc_train,
type = 'C-svc',         #use C classification
kernel = 'vanilladot',  #simple linear kernel
C = 0.01,       #lambda
scaled = T,             #have ksvm scale the data
cross = 10)             #number of folds
sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
best_svm_model_acc <- sum(predict(modelTrain,cc_test) == cc_test[,11])/nrow(cc_test)
cat('Best model perfomance on test set:', sbest_svm_model_acc)
best_svm_model_acc
cat('Best model perfomance on test set:', best_svm_model_acc)
if (which.max(acc) <= 9) {
#if SVM model was best, evaluate SVM method on the test set to find estimated quality
cat('The best overall model was SVM model', which.max(acc[1:9]), 'with test set correctness/accuracy of',
best_svm_model_acc, 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
} else {
#if KNN model was best, evaluate KNN method on the test set to find estimated quality
cat('The best overall model was the KNN model k =', which.max(acc[10:29]),
'folds with an validation set correctness/accuracy of',best_knn_model_acc)
}
acc
if (best_knn_model_acc < best_svm_model_acc) {
#if SVM model was best, evaluate SVM method on the test set to find estimated quality
cat('The best overall model was SVM model', which.max(acc[1:9]), 'with test set correctness/accuracy of',
best_svm_model_acc, 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
} else {
#if KNN model was best, evaluate KNN method on the test set to find estimated quality
cat('The best overall model was the KNN model k =', which.max(acc[10:29]),
'folds with an validation set correctness/accuracy of',best_knn_model_acc)
}
if (which.max(acc) <= 9) {
#if SVM model was best, evaluate SVM method on the test set to find estimated quality
cat('The best overall model was SVM model', which.max(acc[1:9]), 'with test set correctness/accuracy of',
best_svm_model_acc, 'with a C/lambda value of', c_amounts[which.max(acc[1:9])])
} else {
#if KNN model was best, evaluate KNN method on the test set to find estimated quality
cat('The best overall model was the KNN model k =', which.max(acc[10:29]),
'folds with a test set correctness/accuracy of',best_knn_model_acc)
}
