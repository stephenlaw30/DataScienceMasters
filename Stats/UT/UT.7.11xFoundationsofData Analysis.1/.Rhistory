spotifyKey <- ab0f8dd5358b49668d5da5c7b2368b43
spotifyKey <- "ab0f8dd5358b49668d5da5c7b2368b43"
spotifySecret <- "56681b43de334d8a9a9a352222d5c02e"
spotifyKey <- "ab0f8dd5358b49668d5da5c7b2368b43"
spotifySecret
spotifyKey
spotifySecret
spotifyApp
spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
spotifyEndPoint <- oauth_endpoint(NULL,
"https://accounts.spotify.com/authorize",
"https://accounts.spotify.com/api/token")
spotifyKey <- "ab0f8dd5358b49668d5da5c7b2368b43"
spotifySecret <- "56681b43de334d8a9a9a352222d5c02e"
spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
spotifyApp <- oauth_app("R_Analysis", spotifyKey, spotifySecret)
spotifyToken <- oauth2.0_token(spotifyEndPoint, spotifyApp)
?oauth2.0_token
spotifyToken <- ?oauth2.0_token(spotifyEndPoint, spotifyApp)
(150*617)+(16*238)
source('C:/Users/snewns/Dropbox/NewLearn/Edx/AnalyticsEdge/Unit8_LinearOptimization/8_1_AirlineRevenueMgmt.R')
$96,358 - $86,883
96358 - 86883
one of 0.20 (0.08 means there is an 8% chance a user who searches for best LTE network will click on the ad for AT&T if it
0.2*25
budget has dropps from 100 to 4, and the remaining displays of Query 3 has dropped to 0. So there are no more displays of Query 3 remaining that we can use.
So, our decisions are which teams should play each other each week and we model this with binary decision variables. The first constraint is that each team
15*7
(1030+92)/(1030+92+126+227)
Mount Sinai Hospital in Toronto uses integer optimization to schedule their ORs. Hospitals have a limited number of operating rooms,
of what they wanted. We want to maximize this percentage value *for every department*, so that is why we sum over all departments
weekly department OR requirements. Our last set of constraints has to do with departmental targets. We want to make sure we don't give any department more
Our objective value would be a max profitability of 269.9246814, more than 5X that of the greedy approach, with 7 hotels are selected in the solution?
I22x1,22 â‰¤ 1.2** to say the index of SR 1 at brick 1 plus the index of SR at brick 2 etc. We should have a similar constraint in our model for every SR (1 - 4).
administrative staff could adjust the constraints depending on the importance of the teacher preferences versus parent preferences.
set.seed(200)
kmc <- kmeans(housesNorm, centers = 10, iter.max = 1000)
Pt. 2 - UNDERSTANDING RETAIL CONSUMERS
**Clustering** can be used for **market segmentation**, the idea of dividing airline passengers into small, more similar
groups, and then designing a marketing strategy specifically for each group. We will see how this idea can be applied to
retail consumer data with data collected over 2 years from source files provided by dunnhumby, a customer science company
based in the UK, for a group of 2.5K households. Each row represents a unique household with the following variables:
* NumVisits       = the number of times the household visited the retailer
* AvgProdCount    = the average number of products purchased per transaction
* AvgDiscount     = the average discount per transaction from coupon usage (in %) - NOTE: Do not divide by 100!
* AvgSalesValue   = the average sales value per transaction
* MorningPct      = the percentage of visits in the morning (8am - 1:59pm)
* AfternoonPct    = the percentage of visits in the afternoon (2pm - 7:59pm)
Note that some visits can occur outside of morning and afternoon hours (visits from 8pm - 7:59am are possible)'
houses <- read.csv("Households.csv")
head(houses)
nrow(subset(houses, houses$MorningPct == 100))
nrow(subset(houses, houses$AfternoonPct == 100))
'4 households have logged transactions at the retailer only in the morning and 13 households have logged transactions at
the retailer only in the afternoon.
Of households that spend over $150 per transaction on average, the minimum average discount per transaction is 15.64%'
min(houses$AvgDiscount[houses$AvgSalesValue > 150])
'Of households who have an average discount per transaction greater than 25%, the minimum average sales value per
transaction is 50.1175'
min(houses$AvgSalesValue[houses$AvgDiscount > 25])
'In the dataset, a proportion of 0.0592 or 5.92% of households visited the retailer at least 300 times'
nrow(subset(houses, houses$NumVisits >= 300))/nrow(houses)
'When clustering data, it is often important to normalize the variables so that they are all on the same scale. If you
clustered this dataset without normalizing, we would expect to NumVisits dominate in the distance calculations, because
it is on the largest scale.
Normalize all variables'
library(caret)
preproc = preProcess(houses)
housesNorm = predict(preproc, houses)
'Remember that for each variable, the normalization process subtracts the mean and divides by the standard deviation. So,
in this normalized dataset, all variables should have mean = 0 and standard deviation = 1.
The maximum value of NumVisits in the normalized dataset is 10.28281 and the the minimum value of AfternoonPct in the
normalized dataset is -3.228427.
Create a dendrogram of the normalized data'
set.seed(200)
distances <- dist(housesNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
'Based on the dendrogram, 2, 3, or 5 clusters would be appropriate for this problem. 4 or 6 clusters have very little
"wiggle room", which means the additional clusters are not very distinct from existing clusters. That is, when moving
from 3 clusters to 4 clusters, the additional cluster is *very similar to an existing one* (as well as when moving from 5
clusters to 6 clusters).
Run the k-means clustering algorithm on the normalized dataset, selecting 10 clusters. Right before using kmeans(), set
the seed to 200 again.'
set.seed(200)
kmc <- kmeans(housesNorm, centers = 10, iter.max = 1000)
kmc
str(kmc)
table(kmc$cluster)
order(table(kmc$cluster))
sort(table(kmc$cluster))
healthyClusters <- kmc$cluster
healthyClusters
kmc
str(kmc)
cluster1 <- subset(housesNorm, kmc == 1)
cluster1 <- subset(housesNorm, kmc$cluster == 1)
cluster1 <- subset(housesNorm, kmc$cluster == 1)
cluster2 <- subset(housesNorm, kmc$cluster == 2)
cluster3 <- subset(housesNorm, kmc$cluster == 3)
cluster4 <- subset(housesNorm, kmc$cluster == 4)
cluster5 <- subset(housesNorm, kmc$cluster == 5)
cluster6 <- subset(housesNorm, kmc$cluster == 6)
cluster7 <- subset(housesNorm, kmc$cluster == 7)
cluster8 <- subset(housesNorm, kmc$cluster == 8)
cluster9 <- subset(housesNorm, kmc$cluster == 9)
cluster10 <- subset(housesNorm, kmc$cluster == 10)
cluster1
tail(sort(colMeans(cluster2)))
head(cluster1)
head(cluster1)Submit You have used 0 of 1 attempt Some problems have options such as save, reset, hints, or show answer. These options follow the Submit button.
Show Answer
head(cluster1)
cluster2
head(cluster2)
head(cluster3)
head(cluster4)
head(cluster5)
head(cluster6)
head(cluster7)
head(cluster4) #
head(cluster7)
head(cluster8)
head(cluster9)
head(cluster10)
head(cluster4) #
head(cluster7) #
kmc
str(kmc)
head(kmc)
head(kmc) #check 'centers'
head(kmc) #check 'centers'
set.seed(5000)
kmc <- kmeans(housesNorm, centers = 5, iter.max = 1000)
sort(table(kmc$cluster))
str(kmc)
head(kmc)
rm(list = ls(all = TRUE))
energy <- read.csv("energy.csv")
energy <- read.csv("energy.csv")
table(energy$STATE,energy$GenTotalRenewable)
sort(table(energy$STATE,energy$GenTotalRenewable))
order(table(energy$STATE,energy$GenTotalRenewable))
sort(table(energy$GenTotalRenewable,energy$STATE))
table(energy$GenTotalRenewable,energy$STATE)
table(energy$STATE,energy$GenTotalRenewable)
summary(energy$STATE,energy$GenTotalRenewable)
summary(energy$GenTotalRenewable)
table(energy$STATE,mean(energy$GenTotalRenewable))
table(energy$STATE,mean(energy$GenTotalRenewable)
energy[energy$STATE == 'AZ',]
energy[energy$STATE == 'AZ',]
sum(energy$GenTotalRenewable[energy$STATE == 'AZ'])
sum(energy$GenTotalRenewable[energy$STATE == 'CA'])
sum(energy$GenTotalRenewable[energy$STATE == 'ID'])
sum(energy$GenTotalRenewable[energy$STATE == 'MA'])
max(energy$GenTotalRenewable[energy$STATE == 'AZ'])
max(energy$GenTotalRenewable[energy$STATE == 'CA'])
max(energy$GenTotalRenewable[energy$STATE == 'ID'])
max(energy$GenTotalRenewable[energy$STATE == 'MA'])
tapply(energy$GenTotalRenewable,energy$STATE, max)
sort(tapply(energy$GenTotalRenewable,energy$STATE, max))
which.max(energy$GenTotalRenewable[energy$STATE == 'ID'])
energy[which.max(energy$GenTotalRenewable[energy$STATE == 'ID'])]
energy[which.max(energy$GenTotalRenewable[energy$STATE == 'ID']),]
energy$GenTotalRenewable[energy$STATE == 'ID']
which.max(energy$GenTotalRenewable[energy$STATE == 'ID']
)
idaho <- subset(energy, energy$STATE == 'ID')
idaho
energy[which.max(idaho$GenTotalRenewable,]
energy[which.max(idaho$GenTotalRenewable)
]
idaho$GenTotalRenewable
which.max(idaho$GenTotalRenewable)
idaho[which.max(idaho$GenTotalRenewable)]
idaho[which.max(idaho$GenTotalRenewable),]
energy$AllSourcesCO2[energy$presidential.results == 0]
mean(energy$AllSourcesCO2[energy$presidential.results == 0])
mean(energy$AllSourcesCO2[energy$presidential.results == 0], na.rm = TRUE)
mean(energy$AllSourcesCO2[energy$presidential.results == 1], na.rm = TRUE)
mean(energy$AllSourcesNOx[energy$presidential.results == 0], na.rm = TRUE)
mean(energy$AllSourcesNOx[energy$presidential.results == 1], na.rm = TRUE)
cor(energy$AllSourcesCO2,energy$EPriceIndustrial, use="complete") #use = 'complete' handles NA's
cor(energy$EPriceIndustrial,energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial ,energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesSO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesNOx, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesIndustrial, energy$AllSourcesSO2, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesResidential, energy$AllSourcesNOx, use="complete") #use = 'complete' handles NA's
cor(energy$EsalesCommercial, energy$AllSourcesCO2, use="complete") #use = 'complete' handles NA's
library(ggplot2)
library(ggplot2)
ggplot(data = energy, aes(x = State, y = EPriceTotal)) + geom_boxplot()
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot()
tapply(energy$EPriceTotal, energy$STATE, mean)
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(data = energy, aes(x = STATE, y = EPriceTotal)) + geom_boxplot() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
tapply(energy$EPriceTotal, energy$STATE, mean)
sort(tapply(energy$EPriceTotal, energy$STATE, mean))
cor(energy$STATE[energy$STATE == 'WY'],max(mean(energy$GenTotal)))
set.seed(144)
spl = sample(1:nrow(energy), size = 0.7*nrow(energy))
train = energy[spl,]
test = energy[-spl,]
model1 <- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import,
data = train, family = "binomial")
summary(model1)
logModelPredictions <- predict(model1, test, type = "response")
table(test$GenSolarBinary, logModelPredictions > 0.5)
154+18/nrow(test)
(154+18)/nrow(test) #accuracy of
table(test$GenSolarBinary[test$presidential.results == 0], logModelPredictions > 0.5)
republicanLogModelPredictions <- predict(model1, test[test$presidential.results == 0], type = "response")
republicanLogModelPredictions <- predict(model1, test[test$presidential.results == 0,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 0], republicanLogModelPredictions > 0.5)
democratLogModelPredictions <- predict(model1, test[test$presidential.results == 1,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 0], democratLogModelPredictions > 0.5)
democratLogModelPredictions <- predict(model1, test[test$presidential.results == 1,], type = "response")
table(test$GenSolarBinary[test$presidential.results == 1], democratLogModelPredictions > 0.5)
(90+2)/nrow(test[test$presidential.results == 0])
(90+2)/nrow(test[test$presidential.results == 0,])
table(test$GenSolarBinary[test$presidential.results == 1], democratLogModelPredictions > 0.5)
(64+16)/nrow(test[test$presidential.results == 1,])
train.limited <- train[,c('CumlRegulatory','CumlFinancial','presidential.results','Total.salary','Import')]
test.limited <- test[,c('CumlRegulatory','CumlFinancial','presidential.results','Total.salary','Import')]
library(caret)
preProcess(train.limited)
library(caret)
reproc = preProcess(train.limited)
train.limited.norm = predict(preproc, train.limited)
library(caret)
preproc = preProcess(train.limited)
train.limited.norm = predict(preproc, train.limited)
preproc2 = preProcess(test.limited)
test.limited.norm = predict(preproc2, test.limited)
set.seed(144)
kmc.train.norm <- kmeans(train.limited, centers = 2, iter.max = 1000)
str(kmc.train.norm)
head(kmc.train.norm)
install.packages("flexclust")
library(flexclust)
?flexclust
?as.kcca
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
cluster.train.norm
str(kmc.train.norm)
head(kmc.train.norm)
train.cluster1 <- subset(train.limited.norm, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train.limited.norm, kmc.train.norm$cluster == 2)
summary(train.cluster1)
summary(train.cluster2)
train.limited.norm
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
summary(train.cluster1)
summary(train.cluster2)
cluster.train.norm
model2 <- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import,
data = train.cluster1, family = "binomial")
summary(model2)
test.cluster1 <- subset(test, kmc.train.norm$cluster == 1)
test.cluster2 <- subset(test, kmc.train.norm$cluster == 2)
logModelPredictions2 <- predict(model2, test, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
test.cluster1
logModelPredictions2
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+25)/nrow(test.cluster1)
logModelPredictions2 <- predict(model2, test.limited.norm, type = "response")
table(test.limited.norm$GenSolarBinary, logModelPredictions2 > 0.5)
?cluster.test
logModelPredictions2 <- predict(model2, test.limited.norm, type = "response")
table(test.limited.norm$GenSolarBinary, logModelPredictions2 > 0.5)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+25)/nrow(test.cluster1)
test.cluster1
nrow(test.cluster1)
(126+25)/nrow(test.cluster1)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
(126+13)(126+13+18+25)
(126+13)/(126+13+18+25)
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
cluster.test.norm
kcm.test.norm.kcaa
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
kcm.test.norm.kcaa
kmc.train.norm.kcaa
kmc.train.norm.kcaa <- as.kcca(kmc.train.norm, train.limited.norm)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
as.kcca
?as.kcca
set.seed(144)
kmc.test.norm <- kmeans(test.limited, centers = 2, iter.max = 1000)
str(kmc.test.norm)
head(kmc.test.norm)
kmc.train.norm.kcaa <- ?as.kcca(kmc.train.norm, train.limited.norm)
cluster.train.norm <- predict(kmc.train.norm.kcaa)
kcm.test.norm.kcaa <- as.kcca(kmc.train.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
kmc.test.norm
str(kmc.test.norm)
head(kmc.test.norm)
kcm.test.norm.kcaa <- as.kcca(kmc.test.norm, test.limited.norm)
cluster.test.norm <- predict(kcm.test.norm.kcaa)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm.kcaa$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm.kcaa$cluster == 2)
test.cluster1 <- subset(test, kcm.test.norm$cluster == 1)
test.cluster2 <- subset(test, kcm.test.norm$cluster == 2)
head(kmc.test.norm)
train.cluster1 <- subset(train, kmc.train.norm$cluster == 1)
train.cluster2 <- subset(train, kmc.train.norm$cluster == 2)
test.cluster1 <- subset(test, kmc.test.norm$cluster == 1)
test.cluster2 <- subset(test, kmc.test.norm$cluster == 2)
logModelPredictions2 <- predict(model2, test.cluster1, type = "response")
table(test.cluster1$GenSolarBinary, logModelPredictions2 > 0.5)
2+22/nrow(test.cluster1)
(40*20) + (40*4) + (35*32) + (35*0)
library(SDSFoundations)
animaldata <- AnimalData
install.packages("C:/Users/snewns/Dropbox/DataScienceMasters/Stats/UT/SDSFoundations_1.1.zip", repos = NULL, type = "win.binary")
library(SDSFoundations)
animaldata <- AnimalData
table(animaldata$Age.Intake)
str(animaldata)
table(animaldata$Age.Intake)
adultAnimals <- subset(animaldata, Age.Intake >= 1)
adultAnimals
table(adultAnimals$Age.Intake)
table(adultAnimals$Animal.Type)
adultDogs <- subset(adultDogs, Animal.Type == 'Dog')
adultDogs <- subset(adultAnimals, Animal.Type == 'Dog')
adultCats <- subset(adultAnimals, Animal.Type == 'Cat')
ggplot(adultAnimals) + geom_histogram(aes(x = adultDogs))
ggplot(adultDogs) + geom_histogram(aes(x = Weight))
library(ggplot2)
ggplot(adultDogs) + geom_histogram(aes(x = Weight))
ggplot(adultCats) + geom_histogram(aes(x = Weight))
mean(adultCats$Weight)
sd(adultCats$Weight)
(13 - 8.6)/1.9
mean(adultAnimals$Weight)
sd(adultAnimals$Weight)
(13 - 30.29539) / 23.6356
zScore.cat <- 2.3
1-pnorm(zScore.cat)
fivenum(adultDogs$Weight)
fivenum(adultCats$Weight)
mean(adultDogs$Weight)
sd(adultDogs$Weight)
(13 - 35.67) / 23.47
zScore.dog <- -0.97
1-pnorm(zScore.dog)
(4.5 - 6.7) / 1.1
(5.38 - 6.7) / 1.1
(8.79 - 6.7) / 1.1
'Lab 3: Professional Bull Riding
Over 1,200 bull riders from around the world are members of the Professional Bull Riders (PBR), who compete in more
than 300 PBR-affiliated bull riding events per year. In the American tradition, the rider must stay atop the
bucking bull for a full 8 seconds. This data set includes info about the top-ranked bull riders for 2013. Rankings
are based on a system which awards points for qualified rides at events throughout the season.
**Primary Research Question** - For the 2013 season, is there a linear relationship between how often a rider
placed in the Top 10 and the number of times he stayed on his bull for a full 8 seconds?'
setwd('C:/Users/snewns/Dropbox/DataScienceMasters/Stats/UT/UT.7.11xFoundationsofData Analysis.1')
library(SDSFoundations)
bull <- read.csv("BullRiders.csv")
dim(bull) #58 obs of 14 vars
str(bull)
head(bull,10)
head(bull,15)
'Of the first 10 riders in the dataset, 3 have been pro for 10 years or more. Of the top 15 riders so far in 2015,
0 rides were completed by the rider with the fewest buck-outs in 2014, Kaique Pacheco.
**Top10_13** tells us how many times the rider has placed in the Top 10 at the end of the 2013 season and Rides13
tells us the number of times a rider stayed on his bull for the full 8 seconds in 2013. Both are numerical.
We should be using correlation for the analysis because we want to explore a linear relationship between 2
quantitative variables. We should generate a scatterplot of these 2 variables before we continue our analysis because
we want to confirm that the relationship is linear.
Create a subset of the data which contains only riders that have participated in at least one event in 2013.'
rider13 <- subset(bull, Events13 > 0)
#rider13 <- bull[bull$Events13  > 0 ,]
'Create a scatterplot of the two variables of interest.'
hist(rider13$Rides13)
summary(rider13$Rides13)
hist(rider13$Top10_13)
summary(rider13$Top10_13)
fivenum(rider13$Rides13)
fivenum(rider13$Top10_13)
library(ggplot2)
ggplot(data = rider13) + geom_point(aes(x = Top10_13, y = Rides13))
'Check to see that the relationship is linear, which it is. Plot a line of best fit as a guide.'
ggplot(data = rider13) + geom_point(aes(x = Top10_13, y = Rides13)) +
geom_smooth(aes(x = Top10_13, y = Rides13), method = "lm", se = FALSE)
plot(bull$Events12, bull$BuckOuts12)
abline(lm(bull$Events12~bull$BuckOuts12)) #linear model of event as a function of buckouts
ggplot(data = rider13) + geom_point(aes(x = Top10_13, y = Rides13)) +
geom_smooth(aes(x = Top10_13, y = Rides13), method = "lm", se = FALSE)
rider13[which(rider13$Top10_13 == 2 & rider13$Rides13 == 22)
]
rider13[which(rider13$Top10_13 == 2 & rider13$Rides13 == 22)]
rider13[which(rider13$Top10_13 == 2 & rider13$Rides13 == 22),]
setwd('C:/Users/snewns/Dropbox/DataScienceMasters/Stats/UT/UT.7.11xFoundationsofData Analysis.1')
library(SDSFoundations)
bull <- read.csv("BullRiders.csv")
bull
new_bull12 <- subset(bull, Events12 > 0)
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12))
library(ggplot2)
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12))
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12), stat = 'indentity')
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12), binwidth = 20000)
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12), binwidth = 200000)
ggplot(new_bull12) + geom_histogram(aes(x = Earnings12), binwidth = 50000)
summary(new_bull12$Earnings12)
fivenum(new_bull12$Earnings12)
cor(new_bull12[,c('Earnings12','RidePer12','CupPoints12')])
ggplot(data = new_bull12) + geom_point(aes(x = RidePer12, y = Earnings12)) +
geom_smooth(aes(x = RidePer12, y = Earnings12), method = "lm", se = FALSE)
ggplot(data = new_bull12) + geom_point(aes(x = CupPoints12., y = Earnings12)) +
geom_smooth(aes(x = CupPoints12., y = Earnings12), method = "lm", se = FALSE)
ggplot(data = new_bull12) + geom_point(aes(x = RidePer12, y = Earnings12)) +
geom_smooth(aes(x = RidePer12, y = Earnings12), method = "lm", se = FALSE)
ggplot(data = new_bull12) + geom_point(aes(x = CupPoints12, y = Earnings12)) +
geom_smooth(aes(x = CupPoints12, y = Earnings12), method = "lm", se = FALSE)
ggplot(data = new_bull12) + geom_point(aes(x = RidePer12, y = Earnings12)) +
geom_smooth(aes(x = RidePer12, y = Earnings12), method = "lm", se = FALSE)
ggplot(data = new_bull12) + geom_point(aes(x = CupPoints12, y = Earnings12)) +
geom_smooth(aes(x = CupPoints12, y = Earnings12), method = "lm", se = FALSE)
new_bull12[which(new_bull12$Earnings12 == max(new_bull12$Earnings12)),]
noOutlier <- new_bull12[new_bull12$Earnings12 < 1000000,]
noOutlier <- new_bull12[new_bull12$Earnings12 < 1000000,]
cor(noOutlier[,c('Earnings12','RidePer12','CupPoints12')])
ggplot(data = noOutlier) + geom_point(aes(x = RidePer12, y = Earnings12)) +
geom_smooth(aes(x = RidePer12, y = Earnings12), method = "lm", se = FALSE)
ggplot(data = noOutlier) + geom_point(aes(x = CupPoints12, y = Earnings12)) +
geom_smooth(aes(x = CupPoints12, y = Earnings12), method = "lm", se = FALSE)
cor(new_bull12[,c('Earnings12','RidePer12','CupPoints12')])
new_bull <- subset(bull, Rides14 > 0)
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
setwd('C:/Users/snewns/Dropbox/DataScienceMasters/Stats/UT/UT.7.11xFoundationsofData Analysis.1')
library(SDSFoundations)
bull <- read.csv("BullRiders.csv")
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
'Subset the dataset for riders that had at least 1 ride in the 2014 season, new_bull.'
new_bull <- subset(bull, Rides14 > 0)
'Create a new variable for the average number of rides per event for each bull rider in the new_bull dataset'
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
'Make a histogram of "rides per event" and find the 5-number summary for it.'
library(ggplot2)
ggplot(new_bull) + geom_histogram(aes(x = RidesPerEvent14))
ggplot(new_bull) + geom_histogram(aes(x = RidesPerEvent14), binwidth = 0.25)
fivenum(new_bull$RidesPerEvent14)
new_bull
fivenum(new_bull$RidesPerEvent14)
library(SDSFoundations)
bull <- read.csv("BullRiders.csv")
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
'Subset the dataset for riders that had at least 1 ride in the 2014 season, new_bull.'
new_bull <- subset(bull, Rides14 > 0)
'Create a new variable for the average number of rides per event for each bull rider in the new_bull dataset'
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
'Make a histogram of "rides per event" and find the 5-number summary for it.'
library(ggplot2)
ggplot(new_bull) + geom_histogram(aes(x = RidesPerEvent14), binwidth = 0.25)
fivenum(new_bull$RidesPerEvent14)
fivenum(new_bull$RidesPerEvent14, na.rm = TRUE)
new_bull$RidesPerEvent14
RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
new_bull$RidesPerEvent14 <- new_bull$Rides14/new_bull$Events14
library(ggplot2)
ggplot(new_bull) + geom_histogram(aes(x = RidesPerEvent14), binwidth = 0.25)
fivenum(new_bull$RidesPerEvent14)
ggplot(data = ggplot2) + geom_point(aes(x = RidesPerEvent14, y = Rank14)) +
geom_smooth(aes(x = RidesPerEvent14, y = Rank14), method = "lm", se = FALSE)
library(ggplot2)
ggplot(new_bull) + geom_histogram(aes(x = RidesPerEvent14), binwidth = 0.25)
fivenum(new_bull$RidesPerEvent14)
' Create a scatterplot of "rides per event" and yearly ranking (Rank14) and add a line of best fit.
Which of the following best describes the relationship between these two variables?'
ggplot(data = ggplot2) + geom_point(aes(x = RidesPerEvent14, y = Rank14)) +
geom_smooth(aes(x = RidesPerEvent14, y = Rank14), method = "lm", se = FALSE)
ggplot(data = new_bull) + geom_point(aes(x = RidesPerEvent14, y = Rank14)) +
geom_smooth(aes(x = RidesPerEvent14, y = Rank14), method = "lm", se = FALSE)
cor(new_bull$RidesPerEvent14,new_bull$Rank14)
0.75^2
0.75**2
1 - 0.75**2
