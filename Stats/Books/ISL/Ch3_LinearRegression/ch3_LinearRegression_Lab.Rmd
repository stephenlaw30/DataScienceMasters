---
title: "Ch3_LinearRegression"
author: "Steve Newns"
date: "November 4, 2017"
output:
  html_document: default
  github_document: default
  word_document: default
---

# Ch. 3 Lab: Linear Regression

```{r setup, include=FALSE}
library(MASS) # data sets + functions
library(ISLR) # data from book
library(ggplot2)
library(tidyverse)
```

`Boston` data set = `medv` (median house value) for 506 neighborhoods around Boston predicted using 13 predictors 

variable         | description
---------------- | -----------
`crim`           | per capita crime rate by town.
`zn`             | proportion of residential land zoned for lots over 25,000 sq.ft.
`indus`          | proportion of non-retail business acres per town.
`chas`           | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
`nox`            | nitrogen oxides concentration (parts per 10 million).
`rm`             | average number of rooms per dwelling.
`age`            | proportion of owner-occupied units built prior to 1940.
`dis`            | weighted mean of distances to five Boston employment centres.
`rad`            | index of accessibility to radial highways..
`tax`            | full-value property-tax rate per \$10,000.
`ptratio`        | pupil-teacher ratio by town..
`black`          | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
`lstat`          | lower status of the population (percent).
`medv`           | median value of owner-occupied homes in \$1000s.

```{r}
#fix(Boston)
glimpse(Boston)
```

One can see they are all numerical, so this makes the regression a bit easier.

We start with a simple linear regression model predicting `medv` from `lstat`

```{r}
model1 <- lm(medv ~ lstat, data = Boston)
summary(model1)
```

According to this regression output, about 54.4% of the variability in median value of occupied homes (in $10,000's) is explained by the variability in percentage of lower status of the population. We can also see that a one-unit increase in `lstats` results in a decrease of about $9,500 in terms of the median value of occupied homes, *on average*. We can see that our t value and F-statistic are quite large, and our p-value is very small, indicating the `lstat` is a significant predictor of `medv` in this simple mode.

We can check the confidence interval for the coefficient estimates as well.
```{r}
confint(model1)
```

95% of the intervals of this form would contain the true coefficient values.

We can also produce confidence intervals and prediction intervals for the prediction of `medv` for given values of `lstat` with `predict()`.

```{r}
# CI for 3 lstat values = 5, 10, 15
predict(model1, data.frame(lstat = c(5,10 ,15)), interval ="confidence")
```

The 95 % confidence interval for an a `lstat` value = 10 is (24.47, 25.63), and the 95 % prediction interval is (12.828, 37.28). 

The confidence and prediction intervals are centered around the same point (predicted `medv` value of 25.05 when lstat = 10), but the prediction intervals are wider.

95% of intervals of this form would contain the true value of `medv` for `lstat` values of 5, 10, and 15.

Now to plot the data points along with the least squares regression line from our model.

```{r}
plot( Boston$lstat,Boston$medv, pch = "+")
abline(model1, lwd = 3, col = "red")
```

We can see some non-linearity in the lower values of `lstat` that give higher values of `medv`.

For the diagnostic plots to check:
<ol>
<li> Non-linearity of the Data </li>
<li> Correlation of Error Terms </li>
<li> Non-constant Variance of Error Terms (heteroskedasticity) </li>
<li> Outliers </li> 
<li> High Leverage Points </li>
<li> Collinearity </li>
</ol>

Use `par()`to view the 4 plots in a 2x2 space.
```{r}
par(mfrow = c(2,2))
plot(model1)
```

A linear regression model assumes a straight-line relationship between predictors and a response. If the true relationship is far from linear, conclusions drawn from the model fit are suspect and prediction accuracy of the model can be significantly reduced.

In plotting the residuals vs. the fit (predicted) values in a **residuals plot** (top left), we'd ideally see no discernible pattern, which would indicate a possible problem with some aspect of the linear model. The red, smooth line is fit to the residuals to make it easier to ID any trends. We see a slight U-shape, which provides a mediocre indication of non-linearity in the data and a possibly pattern in the residuals. We could use non-linear transformations of the predictors (log X, âˆšX, and X2) to deal with this issue if needed.

The normal probability, or QQ plot (top-right) also indicated we do not have a linear relationship, as the data points veer off of the line at the tails, especially at the higher tail (right-hand side).



Alternatively, residuals can be computed from a model with `residuals()` or `rstudent()` (**studentized residuals**, which can be used to plot residuals against fitted values.
```{r}
plot(predict(model1), residuals (model1)) # residuals vs. fitted/predicted
abline(h = 0, lwd = 2, col = "red")
plot(predict(model1), rstudent(model1)) # studentized residuals vs. fitted/predicted
abline(h = 0, lwd = 2, col = "red")
```

In either case, we again see evidence of non-linearity like in the first plot (in the 2x2 grid above)

**Leverage statistics** can be computed for any number of predictors with `hatvalues()` 
```{r}
plot(hatvalues(model1))
```

We can also get the index for the largest leverage statistic
```{r}
which.max(hatvalues(model1))
```