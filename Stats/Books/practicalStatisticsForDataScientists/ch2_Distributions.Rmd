---
title: "ch2: Data + Sampling Distributions"
author: "Steve Newns"
date: "May 10, 2018"
output: html_document
---

## Data + Sampling Distributions 

Proliferation of data of varying quality + relevance reinforces need for **sampling** as a tool to work efficiently w/ a variety of data + to **minimize bias**. Even w/ big data projects, predictive models = typically developed + piloted w/ samples + they're used in tests of various sorts (e.g., pricing, web treatments)

**Population** = *unknown* distribution, only thing available = sample of said population + the ***sample's *empirical* distribution** retrived via a sampling procedure

* traditional stats = population parametes that **require strong assumptions**, modern stats = sample statistics where **such assumptions are not needed**

DS should focus on sampling procedures + data *actually at hand*, sometimes from **a physical process that can be modeled** (flipping coins = binomial distribution)

* ANY real-life binomial can be modeled effectively via a coin flip w/ *modified specified success probabilities*

Can then gain additional insights via understanding the population.

### Random sampling + sample bias

Population (`N`) = large, defined but sometimes *theoretical or imaginary* set of data, and sample (`n`) = its subset

* **Random sampling** = drawing elements into a sample at random where each available member = equal chance of being drawn in each draw (results in simple random sample), *with or without replacement*
* **Stratified sampling** = dividing population into **strata** + randomly sampling from each strata.
* **Simple random sample** = random sampling *without stratifying the population*
* **Sample bias** = misrepresents the population.
* **SELF-SELECTION SAMPLING BIAS** = reviews of restaurants, hotels, cafes, etc. on social media = prone to bias b/c people submitting them = NOT randomly selected + have taken initiative to write = leads to self-selection bias
  * people motivated to write reviews may be those w/ poor experiences, have an association w/ establishment, or simply be a different type of person from those who do NOT write reviews. 
  * While self-selection samples can be unreliable indicators of true state of affairs, they *may be more reliable in simply comparing 1 establishment to a similar one b/c the same self-selection bias might apply to each*

Data *quality* often matters more than *quantity* when making estimates/models based on sample = involves **completeness, consistency of format, cleanliness, + accuracy of individual DPs**, + statistics adds notion of **representativeness**

* Classic example = Literary Digest (1936) polled subscriber base = 10M+, Gallup poll = 2k w/ difference in selection of those polled.
* LD = little attention to method of selection = polled relatively high socioeconomic status (own subscribers = owned luxuries like
phones + automobiles, so appeared in marketers’ lists) = sample bias
* **sample was different in some *meaningful nonrandom way* from the larger population it was meant to represent**
  * **nonrandom* =  important b/c **hardly any sample will be *exactly* representative of a population** but sample ***bias* occurs when difference = meaningful** + can be expected to continue for other samples drawn in the same way as the 1st

#### Bias

*Statistical* **bias** = measurement/sampling errors that are **systematic** + produced by measurement/sampling process

* Important distinction is made between errors due to *random chance* vs. due to bias.
  * Ex: Consider physical process of a gun shooting @ a target = will not hit absolute center of the target every time, or even much at all. 
  * **Unbiased process** = produces error, but it's ***random* + does not tend strongly in any direction**
  
Bias comes in *different forms* = may be *observable or invisible*

If result *suggests* bias (by reference to **benchmark** or **actual values**) = often an indicator that statistical/ML model has been misspecified or an important predictor was let out

#### Random selection

To avoid sample bias, use more scientifically-chosen methods to get more representative samples, heart of which lies in random sampling = not always easy as *proper definitions of populations are key but are difficult*

* generate a representative profile of customers via a pilot survey that survey needs to be representative but is labor intensive.
  * 1) ***Define* WHO a customer is** = might select all customer records where purchase amount > 0. Include all past customers, or refunds? Internal test purchases? Resellers? Both billing agent + customer?
* 2) Must specify a sampling *procedure* = “select 100 customers @ random,” or where a sampling from a **flow** (e.g., real-time transactions or web visitors), *timing considerations may be important* (10 AM weekday visitor may be different from 10 PM weekend)
  * **Stratified sampling** = population divided into **strata** + random samples are taken from each stratum. 
  * Political pollsters might seek to learn electoral preferences of whites, blacks, + Hispanics = SRS would yield too few blacks +  Hispanics, so those strata could be **overweighted** in stratified sampling to yield equivalent sample sizes.

#### Size v. Quality

In era of big data = sometimes surprising if *smaller is better* + ***time + effort* spent on random sampling not only will reduce bias, but also allow greater attention to EDA + data quality** 

* Ex: Missing data + outliers may contain useful info + it might be too expensive to track down + evaluate these w/in millions of records, but doing so in a sample of several thousand records may be feasible. 
* Data plotting + manual inspection bog down w/too much data.

*So when are massive amounts of data needed?*: Classic scenario = when data is not only big, but **sparse** as well

* Google search queries: columns = terms, rows = individual queries, cell = 0/1 if query contains term w/ goal = ddetermine best
predicted search destination for a given query. 
  * 150K+ words in English language + Google processes > 1 trillion queries/year = yields a huge matrix + vast majority of entries = 0
* This = true big data problem: *only when enormous quantities of data are accumulated can effective search results be returned for most queries + more data accumulates = better the results* 
* Not problem for popular search terms = effective data found fairly quickly for handful of extremely popular topics trending @ particular time
* Real value of modern search tech = ability to return detailed + useful results for a huge *variety* of search queries, including those that occur only w/ v. small a frequency,
* # of actual pertinent records (exact search query, or something very similar, appears, together w/ info on what link people ultimately clicked on) might need only be in the thousands to be effective. 
* However, many trillions of DPs = needed to obtain these pertinent records (random sampling will not help).

#### Sample Mean versus Population Mean

Mean of a sample from a population = $\bar{x}$ whereas ${\mu}$ = mean of a population.

**Info about samples = *observed*, + info about large populations = often *inferred* from smaller samples** = keep the 2 things separate in symbology

### Selection Bias

**Selection bias** = practice of selectively choosing data — consciously or unconsciously — in a way that leads to a conclusion that is misleading or ephemeral.


**Bias** = systematic error
**Data snooping** = extensive hunting through data in search of something interesting.
**Vast search effect** = Bias or nonreproducibility resulting from repeated data modeling, or modeling data w/ large #'s of predictor variables, $p$

If you specify a hypothesis + conduct well-designed experiment to test it = can have high confidence in conclusion

Often, one looks @ available data + tries to discern patterns, *but is pattern = real, or just a product of data snooping?* (“If you torture the data long enough, sooner or later it will confess")

Difference between a phenomenon verified when testing a hypothesis via experiment vs. a phenomenon discovered by perusing available
data can be illuminated w/ thought experiment.

* Someone can flip a coin on H on next 10 tosses, you challenge her (equivalent of an experiment) + she proceeds to toss 10 H
* Clearly you ascribe some special talent to her b/c $p(10 H straight by chance)$ = 1/1000
* Do this w/ 20k people = chance *someone* gets 10 H = extremely high (> 99%, or (1-$p$(no 10 H)$)
* Clearly, selecting, **after the fact**, the person(s) who gets 10 H does not indicate any special talent = most likely luck.

Since **repeated review of large data sets = a key value proposition in DS, selection bias is something to worry about**

Form of selection bias particular concern to DS = **vast search effect** = repeatedly run different models + ask different questions w/ large data set = bound to find something interesting but is the result truly something interesting, or is it the chance outlier?

Can guard against w/ **holdout set(s)** against which to validate performance, + use of **target shuffling = a permutation test, in 
essence, to test the validity of predictive associations that a data mining model suggests**

*Other typical forms of selection bias in statistics*: Nonrandom sampling (see sampling bias), cherry-picking data, selection of time intervals that accentuate a partiular statistical effect, stopping an experiment when the results look “interesting.”

#### Regression to the Mean

**Regression to the mean** = phenomenon involving successive measurements on a given variable: extreme observations tend to be followed by more central ones, so attaching special focus + meaning to extreme value can lead to a form of selection bias.

Sports = “rookie of the year, sophomore slump” phenomenon = Among athletes who begin career in a given season (rookie class), 1 always performs better generally does not do as well in his 2nd year. Why not?

In nearly all major sports, at least those played w/ a ball or puck, 2 elements play a role in overall performance: Skill + Luck

Regression to the mean = consequence of a particular form of selection bias = select rookie w/ best performance, then skill + good luck = probably contributing., + in next season, skill will still be there but, in most cases, luck will not, so performance regresses

Phenomenon was 1st IDed by Francis Galton in 1886 in connection w/ genetic tendencies (children of extremely tall men tend not to be as tall as their father)

***WARNING***: Regression to the mean =  *distinct* from linear regression statistical modeling method = linear relationship is estimated between predictors + outcome

***KEY IDEAS*** Specifying a hypothesis + *collecting data following randomization + random sampling principles* ensures against bias

* All other forms of data analysis run risk of bias resulting from data collection/analysis process (repeated running of models in data mining, data snooping in research, after-the-fact selection of interesting events)

###Sampling Distribution of a Statistic

**Sampling distribution** = distribution of some sample statistic *over many samples drawn from same population* + much of
classical statistics = concerned w/ *making* inferences from (small) samples to (very large) populations*.

* **Sample statistic**  = metric calculated for a sample of data drawn from a larger population.
* **Data distribution** = frequency distribution of individual values in a data set.
* **Sampling distribution** = frequency distribution of a sample statistic over many samples or resamples.
* **CLT** = Tendency of sampling distribution to take on a normal shape as sample size rises.
* **Standard error** = variability (StdDev) of a sample statistic over many samples (*NOT* SD = variability of *individual data values*)

Typically samples = drawn w/ goal of measuring something (w/ a *sample statistic*) or modeling something (w/ a statistical or ML model).

**Since estimate/model is based on a sample, it might be in error = different in a different sample** = therefore interested in how different it might be w/ a key concern= **sampling variability** 

W/ lots of data, could draw additional samples + observe distribution of a sample stat directly

*Typically, will calculate estimate/model using as much data as is easily available, so option of drawing additional samples from population is not readily available*

***WARNING***: Important to distinguish between distribution of the individual DPs (data distribution) + distribution of a sample statistic (sampling distribution)

Distribution of a sample statistic = likely to be more regular + bell-shaped than distribution of data itself + larger a sample statistic is based on = more this is true + larger the sample, narrower the distribution of the sample statistic.

Ex: Annual income for loan applicants to Lending Club  w/ 3 samples of data: a sample of k values, a sample of 1k means of 5 values, + a sample of 1k means of 20 values w/ histogram of each sample:

```{r,message=F,warning=F}
library(tidyverse)
library(ggplot2)

loans <- read.csv('../../../../PracticalStatsForDataScientists_50/data/loans_income.csv')
```

```{r}
smpl <- data.frame(income=sample(loans$x,1000),type="data_dist")
smplMean_5 <- data.frame(income=tapply(sample(loans$x,1000*5),rep(1:1000, rep(5,1000)), FUN = mean),type="mean_of_5")
smplMean_20 <- data.frame(income=tapply(sample(loans$x,1000*20),rep(1:1000, rep(20,1000)), FUN = mean),type="mean_of_20")

income <- rbind(smpl,smplMean_5,smplMean_20) %>%
  mutate(type = factor(type,levels=c("data_dist","mean_of_5","mean_of_20"),labels=c('Data','Mean of 5','Mean of 20')))

income %>%
  ggplot(aes(x=income)) +
  geom_histogram(bins=40) +
  facet_grid(type ~ .)
```


####CLT

CLT says that means drawn from multiple samples will resemble bell-shaped normal curve even if source population is not normally distributed, provided sample size $n$ = large enough + departure of the data from normality = not too great

CLT allows normal-approximation formulas like **t-distribution** to be used in calculating sampling distributions for inference (**CI's + hypothesis tests**).

CLT = lot of attention in traditional stats texts b/c it underlies machinery of hypothesis tests + CI's so DS's should be aware of this role, but, since formal hypothesis tests + CI's play a small role in DS, + the **bootstrap** = available in *ANY* case, CLT is not so central in DS

#### Standard Error

Std Error =  single metric that sums up variability in sampling distribution for a statistic + can be estimated using a statistic based on the SD $s$ or $\sigma$ of the sample values, + the sample size $n$: $SE = \frac {s} {\sqrt{n}}$

**As $n$ increases, SE decreases** = *square-root of n rule*: in order to reduce SE by a factor of 2, $n$ must be increased by a factor of 4.

Validity of SE formula arises from the CLT (don’t *need* to rely on CLT to understand SE)

* 1. Collect a # of brand new samples from population.
* 2. For each new sample, calculate the statistic 
* 3. Calculate SD of the statistics computed in step 2 + use it as estimate of SE

In practice, approach of collecting new samples to estimate SE = typically not feasible (+ statistically very wasteful). Fortunately, it turns out it is not necessary to draw brand new samples + can use **bootstrap resamples** = modern statistics standard way to estimate SE + can be used for virtually any stat + does not rely on CLT or other distributional assumptions.

**REMEMBER**: SD = measures variability of individual DPs + SE = measures variability of a sample metric

###Bootstrap

Bootstrap = easy + effective way to estimate sampling distribution of a stat or of model parameters via drawing additional samples, *w/ replacement* from the sample itself + recalculate the statistic/model for each resample

* **does not necessarily involve any assumptions about the data or the sample stat *being normally distributed***

**Bootstrap sample** = taken w/ replacement from an *observed* data set.
**Resampling** = process of taking repeated samples from *observed* data --> includes both bootstrap + permutation (shuffling) procedures

imagine bootstrap as replicating original sample thousands/millions of times to get hypothetical population that embodies all knowledge from original sample + is just larger + then can draw samples from this hypothetical population w/ purpose of estimatinga sampling distribution

In practice = not necessary to *actually* replicate sample huge # of time + can simply replace each observation after each draw to  effectively create an infinite population in which probability of an element being drawn remains unchanged from draw to draw

Algorithm for a bootstrap resampling of the mean from sample of size $n$:

* 1. Draw a sample value, record it, replace it.
* 2. Repeat $n$ times.
* 3. Record mean of the $n$ resampled values.
* 4. Repeat steps 1–3 $R$ times.
* 5. Use the $R$ results to:
  * a. Calculate their SD (estimates sample mean SE).
  * b. Produce a histogram or boxplot.
  * c. Find a CI.

$R$ = # of iterations of the bootstrap = set somewhat arbitrarily: more iterations = more accurate SE estimate or CI estimate

Result = set of sample stats/estimated model parameters which can be examined to see how variable they are.

`boot` package combines these steps in 1 function


```{r,message=F,warning=F}
library(boot)

stat_fun <- function(x,idx) {
  # Compute median for given sample specified by index = idx
  median(x[idx])
}
(boot_obj <- boot(loans$x, R = 1000, statistic=stat_fun))
```

Original estimate of median = \$62k + bootstrap distribution indicates this estimate has a bias of about –\$79 + a SE of \$221

Bootstrap can be used w/ multivariate data, where rows = sampled as units + a model might then be run on the bootstrapped data, for example, to estimate **stability**/variability of model parameters, or to improve predictive power. 

**Bootstrap aggregating** (**bagging**) = W/ classification + regression trees (**CART** or **decision trees**), running multiple trees on bootstrap samples + then averaging their predictions (or, w/ classification, taking a majority vote) generally performs better than using a single tree

Repeated resampling of the bootstrap = conceptually simple (Julian Simon, an economist + demographer, published a compendium of resampling examples, including the bootstrap, in 1969 text *Basic Research Methods in Social Science (Random House)*) but also computationally intensive + was not a feasible before widespread availability of CPU power. 

Technique gained its name + took off w/ publication of several journal articles + a book by Stanford statistician Bradley Efron in  late 1970s + early 1980s, + was particularly popular among researchers who use stats but are not statisticians + for use w/ metrics or models where mathematical approximations = not readily available.

**Sampling distribution of the mean = well established since 1908 but sampling distribution of many other metrics has not** Bootstrap can be used for sample size determination + we experiment w/ different values for $n$ to see how the sampling distribution is affected

***WARNING***: Bootstrap does *NOT* compensate for small sample size + does not create new data, nor does it fill in holes in an existing data set = merely informs us about how lots of additional samples would behave when drawn from a population like our original sample.

#### Resampling versus Bootstrapping
Sometimes "resampling" = used synonymously w/ "bootstrapping", but more often, **resampling** also includes **permutation procedures**  where multiple samples are combined + sampling may be done w/out replacement. I

***KEY IDEAS***: Bootstrap = powerful tool for assessing variability of a sample stat + can be applied in similar fashion in a wide variety of circumstances, w/out extensive study of mathematical approximations to sampling distributions + also allows us to estimate sampling distributions for stats where no mathematical approximation has been developed.

* When applied to predictive models, aggregating multiple bootstrap sample predictions (bagging) can outperform use of a single model.

### CI's

* Freq. tables, histograms, boxplots, +stdErr's = all ways to understand *potential* error in a sample estimate + so are **CI's** = the % of CI's, constructed in *SAME WAY* from *SAME POPULATION* that're expected to contain statistic of interest.

Natural human aversion to uncertainty; many, while acknowledging uncertainty, nonetheless place undue faith in estimates when presented as a single # (**point estimate**) + presenting estimates as a *range* = 1 way to counteract this tendency. 

CI's do this in a manner grounded in statistical sampling principles. CI's always come with a **coverage level**, expressed as a (high) % = 90% or 95%. 1 way to think of a **90% CI: interval that encloses the central 90% of the bootstrap sampling distribution of a sample statistic** or, more generally, **an x% CI around a sample estimate should, on average, contain similar sample estimates x% of the time *when a similar sampling procedure is followed***

Given sample of size $n$ + a sample stat of interest, the algorithm for a **bootstrap CI** is as follows:
* 1. Draw random sample of size $n$ *with replacement* from the data (a **resample**)
* 2. Record the stat of interest for the resample.
* 3. Repeat steps 1–2 many ($R$) times.
* 4. For an x% CI, trim `[(1 – [x/100]) / 2]`% of the $R$ resample results from either end of the distribution.
* 5. Trimmed points = the endpoints of an x% bootstrap CI

Figure 2-9 shows a a 90% confidence interval for the mean annual income of loan
applicants, based on a sample of 20 for which the mean was $57,573.
Figure 2-9. Bootstrap confidence interval for the annual income of loan applicants, based on a sample
of 20

Bootstrap = general tool that can be used to generate CIs for most stats or model parameters. Stat textbooks + software, w/ roots in over a half-century of CPU-less statistical analysis, will also reference CIs generated by formulas, especially the t-distribution

***NOTE***: Of course, we are *really* interested in for a sample result = *“what is the probability the *true* value lies w/in a certain interval?”*, but this is not really the question a CI answers, but it ends up being how most people interpret the answer.

The probability question associated w/ a CI starts out w/ phrase **“Given a sampling procedure + a population, what is the probability that…”** To go in the opposite direction, **“Given a sample result, what is the probability that (something is true about the population),”** involves more complex calculations + deeper imponderables.

The % associated w/ a CI = **level of confidence** + higher level of confidence = wider interval, + also smaller sample = wider interval (i.e. more uncertainty). Both make sense: the more confident you want to be, + the less data you have, the wider you must make a CI to be sufficiently assured of capturing the true value.

***NOTE***: For a DS, a CI= a tool to get an idea of how variable a sample result might be. **DSs would use this info** not to publish a scholarly paper or submit a result to a regulatory agency (as a researcher might), but **most likely to communicate the potential error in an estimate, +, perhaps, learn whether a larger sample is needed.**

***KEY IDEAS**: CIs = typical way to present estimates as an interval range, + the more data you have, the less variable a sample estimate will be.

* The lower the level of confidence you can tolerate = the narrower the CI will be.
* Bootstrap = effective way to construct confidence intervals.
* Botstrap approach to CIs = see **Introductory Statistics and Analytics: A Resampling Perspective by Peter Bruce (Wiley, 2014)** or
**Statistics by Robin Lock** and four other Lock family members (Wiley, 2012).
* Engineers, who have a need to understand precision of measurements, use CIs perhaps more than most disciplines, + **Modern Engineering Statistics by Tom Ryan (Wiley, 2007)** discusses CIs 
  * It also reviews a tool that is just as useful + gets less attention: **prediction intervals** around a single value, as opposed to a mean or other summary statistic.
a mean or other summary statistic

### Normal Distribution

Gaussian/Bell-shaped	normal =	iconic in	traditional	stats

* iconic but perhaps	overrated: George	W.	Cobb (statistician	noted	for	contribution	to	philosophy	of	teaching	intro stats,	argued	in	a	November	2015	editorial	in the	*American	Statistician*	that	the	“standard	intro	course,	which	puts	the	normal	at	its center,	had	outlived	the	usefulness	of	its	centrality"
* common	misconception	= called "normal" b/c	most	data	follows its	distribution
* Most	vars	used	in	typical	SD	project (+ most	raw	data	as	a	whole) = *NOT*	normally	distributed:
* Utility	of	the Gaussian	derives	from	the	fact	that	many *stats*	=	normally	distributed	*in	their	sampling	distribution.*	
* **Even	so,	assumptions	of	normality =	generally	a	last	resort,	used	when	empirical	probability	distributions,	or	bootstrap	distributions, are	not	available**

Gaussian	=	Carl	Friedrich	Gauss, German	mathematician	from	late	18th +	early	19th	century.	

* Another	name =	the	“error”	distribution.	
* Statistically	speaking,	**error** =the	difference	between	an	actual	value	+	a	statistical	estimate (SD = based	on	errors	from the	mean	of	the	data)

The	fact that	distributions	of	sample	stats	are	often	Gaussian has	made	it	a powerful	tool	in	the	development	of	mathematical	formulas	that	approximate	those distributions

* essential	to	historical	development	of	stats b/c it	permitted mathematical	approximation	of	uncertainty	+	variability
* raw	data	= typically	not	normal, errors	often	are,	as	are	averages	+	totals	in large	samples. 

68% of data in Gaussian distribution = w/in 1 SD, 95% w/in 2 SD, 99.7% w/in 3 SD

#### Standard Normal + QQ Plots

**Standard  	normal** =	units	on	the	x	are expressed	in	terms	of	SDs	from	mean.	

* To	compare	data to	a	standard	normal	distribution,	**normalization/standardization** = subtract mean from DPs	+ then	divide	by SD = **a	z-score**	(normal	= sometimes	called the	z-distribution)
* **QQ-Plot** = visually	determine	how	close	a	sample	is	to	Gaussian =	orders	z-scores	from	low	to	high + plots	each value’s	z-score	on the	y-axis
* x-axis	=	corresponding	quantile	of	a	Gaussian distritbution *for	that	value’s	rank* = **theoretical quantile**	
* Since	the	data	=	normalized,	units correspond	to	#	of	SDs	away	from	mean
* If poitns =	roughly	diagonal	line,	sample	distribution	can	be considered	close	to Gaussian

```{r}
s <- rnorm(100)
qqnorm(s)
abline(a=0,b=1,col='red')
```

***WARNING***: Converting	data	to	z-scores	(standardizing/normalizing data)	does	*NOT*	make data normally	distributed = just	puts	data	on	same	scale	as	standard	normal	distribution, often	for	comparison	purposes.

### Long-tailed distributions

Data	=	generally	not	normally distributed.

**Tail** =	long	narrow	portion	of	a	frequency	distribution,	where	**relatively	extreme	values	occur	at	low frequency**
**Skew** = 1 tail	of	distribution	=	longer	than	other.

Gaussian is	often	appropriate	+	useful	w/	respect	to distribution	of	errors	+	sample	stats,	typically	does	not	characterize distribution	of	raw	data =	sometimes highly	skewed/asymmetric (income	data)	or discrete (binomial	data)	
Both	symmetric	+	asymmetric	distributions	may	have	**long tails** =	the	extreme	values	(small	+ large).	
Long	tails +	guarding	against	them =	widely	recognized	in	practical work.	

* Nassim	Taleb =	**the	black	swan	theory** =	predicts anamolous	events,	such	as	stock	market	crash =	much	more	likely	to	occur than	would	be	predicted	by	the	Gaussian 

Good	example	=	stock	returns.

```{r,message=F,warning=F}
stocks <- read.csv('../../../../PracticalStatsForDataScientists_50/data/sp500_px.csv')
```
```{r}
nflx <- stocks %>%
    select(NFLX)
nflx <- diff(log(nflx[nflx>0]))
qqnorm(nflx)
abline(a=0,b=1,col='red',lwd=2)
```

QQ-Plot	for	daily	stock	returns	for	Netflix =	pts	=	far	below	line	for	low	values	+	far above for	high	values =	means	much	more	likely	to observe	extreme	values	than	would	be	expected	if	data	were Gaussian	Figure, also,	points	= close to	the	line	for	data	w/in 1 SD = Tukey	refers	to this	phenomenon	as	data	being	“normal	in	the	middle,”	but	having	much	longer tails

***NOTE***: Much	statistical	lit.	about	task	of	fitting	statistical	distributions	to	observed data, beware	an	excessively	data-centric	approach	to	this	job,	which	is	as	much	art	as	science. 

* Data	=	variable,	+	often	consistent,	on	its	face,	w/	more	> 1	shape	+	type	of distribution
* **typical	that	domain	+	statistical	knowledge	must	be	brought	to	bear to	determine	what	type	of	distribution	is	appropriate	to	model	a	given	situation**
  * might	have	data	on	level	of	internet	traffic	on	a	server	over	many	consecutive	5-sec periods =	useful	to	know	best	distribution	to	model	“events	per	time	period”	 = **Poisson**

***KEY	IDEAS***: Most	data	=	not	Gaussian + assuming	Gaussian	can	lead	to	underestimation	of	extreme	events	(“black	swans”)

### Student's/Gossett's t distribution

t distribution = used extensively in depicting distributions of sample statistics (typically sample means) = normally-shaped but w/ thicker + longer tails

* families = different t-distributions depending on how large sample is: larger = more normal
* widely used as reference basis for distribution of sample means, differerences between 2 sample means, regression parameters, + more

Gossett: “What is the sampling distribution of the mean of a sample drawn from a larger population?” = started w/ resampling experiment of n=4 from a data set of 3k measurements of criminals’ height + left-middle-finger lengths + plotted standardized results (z-scores) on x + frequency on y

Separately derived a function, = **Student’s t** + fit it over the sample results, plotting the comparison

\# of different statistics can be compared, after standardization, to the t-distribution, to estimate CIs in light of sampling variation.

Consider sample of size $n$ w/ calculated sample mean has been calculated. If $s$ = sample SD, a 90% CI around the sample mean is given by: $\bar{x} +/- t_{n-1}(.05) * \frac {s} {n}$ where $t_{n-1}(.05)$ = **value of t-statistic w/ $n-1$ dF** = chops off 5% of t-distribution @ each tailend

Lacking CPUs, statisticians turned to math + functions such as t-distribution to approximate sampling distributions (CPU power in 80s enabled practical resampling experiments, but by then, use of the t-distribution + similar distributions had become deeply embedded in stats)

t-distribution’s accuracy in depicting behavior of a sample stat **requires the distribution of that statistic for that sample be shaped like a normal distribution**: turns out sample stats = often normally distributed, even when underlying population data is NOT (fact led to widespread application of t-distribution) = CLT

***NOTE*** DS don't need to know whole lot about t-distribution + CLT = used in classical statistical inference, not as central to purposes of DS 

* **Understanding + quantifying uncertainty + variation = important to DS**, but **empirical bootstrap sampling** can answer most questions about sampling error. 
* However, DS routinely encounter t-statistics in output from software (A-B tests, regressions, etc.) = so familiarity is helpful

### Binomial distribution

* **Trial** = event w/ a *discrete* outcome
* **Success** = outcome of interest for a trial.
* **Binomial trial/Bernoulli trial** = trial w/ 2 outcomes
* **Binomial/Bernoulli distribution** = Distribution of # of successes in $x$ trials.

binomial outcomes lie @ heart of analytics since they're often a culmination of a decision or other process (buy/don’t buy, click/don’t click, survive/die, etc.) 

Central to understanding binomial distribution = idea of a set of trials, each w/ 2 possible outcomes w/ *definite probabilities* (need not be 50/50, just must sum to 1) where 1 = a "success" or a "rare" outcome

* success does not imply desirable or beneficial outcome, but indicates the outcome of interest (loan defaults or fraudulent transactions = relatively uncommon events we may be interested in predicting, but are not "good")

**Binomial distribution** = *frequency distribution* of # of successes ($x$) in given # of trials ($n$) w/ specified success probability ($p$) in each trial.

* families = different t-distributions depending on values of $x$, $n$, and $p$.

Binomial distribution answers question like: *"If $p$ of a click converting to a sale = 0.02, what is the $p$ of observing 0 sales in 200 clicks?"*"

```{r}
# calculate binomial probabilities for 2 successes in 5 trials w/ p = .1
dbinom(x=2, size=5, p=0.1)
```
`0.0729 = 7.4%` probability of observing *exactly* 2 successes in 5 trials, where probability of success for each trial = 10%

Often interested in determining $p$ of $x$ *or fewer* successes in $n$ trials

```{r}
# prob of seeing x OR LESS in n trials w/ p
pbinom(2, 5, 0.1)
```
`0.9914 = 99%` probability of observing 2 or fewer successes in 5 trials, where the probability of success for each trial = 10%

**Mean of a binomial distribution = $n*p$  = the expected # of successes in $n$ trials for success probability = $p$**, + variance = $n*p(1-p)$ 

W/ large enough # of trials (particularly when $p$ = close to .5), binomial = virtually indistinguishable from the normal 

Calculating binomial probabilities w/ large $n$ = computationally demanding, + most statistical procedures use the normal w/ mean + variance, as an approximation

***KEY IDEAS***: Binomial outcomes = important to model, since they represent, among other things, fundamental decisions (buy or 

* W/ large $n$, + provided $p$ = not too close to 0 or 1, the binomial can be approximated by the normal

### Poisson and Related Distributions

Many processes produce events randomly @ a given overall rate (visitors at a site, cars @ a toll plaza (*events spread over time*),
imperfections in a square meter of fabric, typos per 100 lines of code (*events spread over space*)

* **Lambda** = rate (per unit of time or space) at which events occur.
* **Poisson distribution** = frequency distribution of the *# of events* in sampled units of time or space.
* **Exponential distribution** = frequency distribution of *the time or distance* from 1 event to the next
* **Weibull distribution** = generalized version of the exponential, in which event rate is allowed to shift over time.

#### Poisson Distributions

From *prior* data we can estimate avg. # of events per unit of time/space, but might also want to know *how different this might be from 1 unit of time/space to another*

Poisson distribution tells us distribution of events per unit of time/space when sampling many such units = useful when addressing *queuing questions* like “How much capacity do we need to be 95% sure of fully processing the internet traffic that arrives on a server in any 5-second period?”

Key parameter in a Poisson distribution = **$\lambda$ = mean # of events that occurs in a specified interval of time/space*** + variance for a Poisson = *also* $\lambda$

Common technique = generate random #'s from a Poisson as part of a queuing simulation.

```{r}
# get 100 random #'s from a poission distribution w/ lambda = 2
rpois(100,lambda=2)
```

**This could represent, for example, if incoming customer service calls average 2/min., this simulate 100 minutes, returning the # of calls in each minute of those 100 minutes**

#### Exponential Distribution

Using same $\lambda$ parameter from Poisson distribution, can also model distribution of *time between events* (between visits to a
site or cars arriving at a toll, as well as model time to failure or time required per service call)

```{r}
# get 100 numbers from 2 events per time period
rexp(100,rate=.2)
```

See 100 random #'s from an exponential distribution where mean # of events per time period = 2: can use it to simulate 100 intervals, in minutes, between service calls, where avg. rate of incoming calls = 0.2/min

**Key assumption in any simulation study (for either the Poisson or exponential) = the rate, $\lambda$, remains *constant* over the period being considered**

* rarely reasonable in a global sense; traffic on roads/data networks varies by time of day + DOW
* But, time periods/areas of space *can usually be divided into segments sufficiently homogeneous so that analysis/simulation w/in those periods is valid*

#### Estimating the Failure Rate
Many applications = event rate $\lambda$ = known/can be estimated from prior data.

For *rare* events = not necessarily so

* Aircraft engine failure = sufficiently rare that, for given engine type, there may be little data on which to base an estimate of time between failures. 

W/ no/little data = little basis on which to estimate an event rate

Can make some guesses: if no events have been seen after 20 hours = can be pretty sure rate is *not* 1/hour. 

Via simulation or direct calculation of probabilities, can assess different hypothetical event rates + estimate threshold values below which the rate is very unlikely to fall.

If there is *some* data but not enough to provide a precise, reliable estimate of rate, a **goodness-of-fit test** can be applied to various rates to determine how well they fit the observed data.

#### Weibull Distribution

In many cases, event rate = not constant over time + if period over which it changes = much longer than typical interval between events, there is no problem; just subdivide the analysis into segments where rates = relatively constant,

If, however, *event rate changes over time of the interval*, exponential/Poisson distributions = no longer useful (likely to be case in mechanical failure where risk of failure increases as time goes by)

Weibull distribution = extension of the exponential = event rate is allowed to change, as specified by a **shape parameter, $\beta$** If $\beta$ > 1,$p$ of an event increases over time, if $\beta$ < 1, it decreases. 

B/c Weibull = used w/ time-to-failure analysis instead of event rate, the 2nd parameter, **scale parameter = eta** = $\eta$ = expressed in terms of **characteristic life**, rather than in terms of rate of events per interval

W/ Weibull, the estimation task now includes estimation of both parameters $\beta$ + $\eta$

```{r}
# generate 100 random #'s/lifetimes from a Weibull w/ shape = 1.5 + characteristic life of 5K
rweibull(100,shape=1.5,scale=5000)
```

***KEY IDEAS***

* Events that occur at a constant rate = # of events per unit of time/space can be modeled as a Poisson distribution.
  * can also model the time or distance *between* 1 event + the next as an exponential distribution.
* A changing event rate over time (e.g., increasing probability of device failure) can be modeled w/ the Weibull distribution.

###Summary

In era of big data, principles of **random sampling** remain important when accurate estimates are needed.

* Random selection of data can reduce bias + yield a higher quality data set than from just using conveniently available data. 
* Knowledge of various sampling + data generating distributions allows us to quantify potential errors in an estimate that might be due to random variation. 
* At the same time, *bootstrap** (sampling ww/ith replacement from observed data set) = attractive “1 size fits all” method to determine possible error in sample estimates.