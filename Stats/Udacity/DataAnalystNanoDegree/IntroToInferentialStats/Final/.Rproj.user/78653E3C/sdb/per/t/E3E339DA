{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Udacity Intro to Descriptive Statistics Final Projcet\"\noutput:\n  html_document: default\n  github_document: default\n  word_document: default\n---\n# *Questions for Investigation*\n\nFor our experiment, we have a standard deck of 52 playing cards divided into four suits (spades (???), hearts (???), diamonds (???), and clubs (???)), each suit containing thirteen cards (Ace, the numbers 2 through 10, and the three face cards: Jack, Queen, and King). \n\nFOr the first part of the experiment, I took 10,000 draws of a single card from the deck, and assigned each card a specified value\n  * Ace takes a value of 1\n  * Numbered take the value printed on the card\n  * Jack, Queen, and King each take a value of 10.\n\n```{r load data, echo = FALSE}\nvalues <- as.vector(read.csv('values.csv')[,2])\nsamples <- as.vector(read.csv('samples.csv')[,2])\n```\n\nThen, we plot a histogram depicting the relative frequencies of the card values, and I added lines for the median and mean.\n\n```{r random draws histogram, echo = FALSE}\n#plot histogram w/ mean and median\nlibrary(ggplot2)\nggplot() + \n  aes(x=values) + \n  geom_histogram(binwidth = 1, boundary = 2, color = 'white', aes(fill = ..count..)) +\n  geom_vline(aes(xintercept=mean(values)),\n             color=\"blue\", linetype=\"dashed\", size=1) + \n  geom_vline(aes(xintercept=median(values)),\n             color=\"red\", linetype=\"dashed\", size=1)\n```\n\nSo I can see I have an odd distribution, which I guess I would say that it is left/negatively-skewed, as I have a long tail to the left, compared to that very high bin of values up to 10. So the majority of the cards I have drawn were the number 10 from any of the 4 suites or a face card.\n\nNow, I'll get samples for a new distribution by drawing three cards from the deck (*without* replacement) and then summing up the three cards' values for a single value. I then would replace the drawn cards back into the deck and repeat this procedure 10,000 times, and then take a look at the distribution of the card sums.\n\n<center>![***Samples Sums Statistics***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/summary.png)\n\nSo I can see I have a mean and median that are quite close, and a standard deviation of just under 5. \n\nNow I must create a histogram of the sampled card sums.\n\n```{r samples sums histogram, echo = FALSE}\n#plot 2nd histogram w/ mean and median\nggplot() + \n  aes(x=samples) + \n  geom_histogram(binwidth = 1, boundary = 1, color = 'white', aes(fill = ..count..)) +\n  geom_vline(aes(xintercept=mean(samples)),\n             color=\"blue\", linetype=\"dashed\", size=1) + \n  geom_vline(aes(xintercept=median(samples)),\n             color=\"red\", linetype=\"dashed\", size=1)\n```\n\nI can see here that my distribution is now very normal-looking. Compared to our original distribution, this can be attribute to increasing the sample size from 1 to 3, as a larger sample size creates a larger z-score for the sample mean, which in turn decreases the proportion of sample means that vary from the sample mean.\n\nWe know that 90% of our data values in this distribution are those values with a chance of being selected that is between 5% and 95%, which on a Z-table come out to be Z-score values between -1.645 and 1.645.\n\n<center>![***Negative Z-score***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/negative_z.png)\n<center>![***Positive Z-score***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/positive_z.png)\n\nSo, if the Z-score is equal to the difference observation and the mean divided by the standard deviation, to get those observations in between which 90% of our data values fall, we need to multiply each of our Z-score values by the standard deviation and then add the mean.\n\nAnd if we do that, we'd have (-1.645 x 4.89) + 19.59 as our lower value, and (1.645 x 4.89) + 19.59 as our upper value. These formulas give us an lower boundary of\n```{r lower boundary, ,echo = FALSE}\n(-1.645*sd(samples)) + mean(samples)\n```\nand an upper boundary of\n```{r upper boundary, ,echo = FALSE}\n(1.645*sd(samples)) + mean(samples)\n```\n\nSo, 90% of sample sum values to fall between 11.3938 and 27.7860\n\nTo find the approximate probability I would a draw value of at least 20, I would need to find the Z-score for 20 in this distribution, find the proportion value in the Z-table, and then subtract this value from 1.\n\nSo, our Z-score would be (20 - 19.59) / 4.89, which gives a value of:\n```{r z}\n(20 - mean(samples)) / sd(samples)\n```\n\nThen we look at the Z-table for this score and retrieve that proportion.\n\n<center>![***Samples Sums Statistics***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/20_perc_z.png)\n\n#z = 0.07610279 --> .7764 = 77.64% at most 20 --> 1 - .7764 = 0.2236 = 22.36% at least 20",
    "created" : 1494359538280.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2773007418",
    "id" : "E3E339DA",
    "lastKnownWriteTime" : 1494359543,
    "last_content_update" : 1494359543324,
    "path" : "C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/finalMarkdown.Rmd",
    "project_path" : "finalMarkdown.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}