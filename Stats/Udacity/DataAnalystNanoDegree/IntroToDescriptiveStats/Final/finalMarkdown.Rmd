---
title: "Udacity Intro to Descriptive Statistics Final Projcet"
output:
  html_document: default
  github_document: default
  word_document: default
---
# *Questions for Investigation*

For our experiment, we have a standard deck of 52 playing cards divided into four suits (spades (???), hearts (???), diamonds (???), and clubs (???)), each suit containing thirteen cards (Ace, the numbers 2 through 10, and the three face cards: Jack, Queen, and King). 

FOr the first part of the experiment, I took 10,000 draws of a single card from the deck, and assigned each card a specified value
  * Ace takes a value of 1
  * Numbered take the value printed on the card
  * Jack, Queen, and King each take a value of 10.

```{r load data, echo = FALSE}
values <- as.vector(read.csv('values.csv')[,2])
samples <- as.vector(read.csv('samples.csv')[,2])
```

Then, we plot a histogram depicting the relative frequencies of the card values, and I added lines for the median and mean.

```{r random draws histogram, echo = FALSE}
#plot histogram w/ mean and median
library(ggplot2)
ggplot() + 
  aes(x=values) + 
  geom_histogram(binwidth = 1, boundary = 2, color = 'white', aes(fill = ..count..)) +
  geom_vline(aes(xintercept=mean(values)),
             color="blue", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=median(values)),
             color="red", linetype="dashed", size=1)
```

So I can see I have an odd distribution, which I guess I would say that it is left/negatively-skewed, as I have a long tail to the left, compared to that very high bin of values up to 10. So the majority of the cards I have drawn were the number 10 from any of the 4 suites or a face card.

Now, I'll get samples for a new distribution by drawing three cards from the deck (*without* replacement) and then summing up the three cards' values for a single value. I then would replace the drawn cards back into the deck and repeat this procedure 10,000 times, and then take a look at the distribution of the card sums.

<center>![***Samples Sums Statistics***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/summary.png)

So I can see I have a mean and median that are quite close, and a standard deviation of just under 5. 

Now I must create a histogram of the sampled card sums.

```{r samples sums histogram, echo = FALSE}
#plot 2nd histogram w/ mean and median
ggplot() + 
  aes(x=samples) + 
  geom_histogram(binwidth = 1, boundary = 1, color = 'white', aes(fill = ..count..)) +
  geom_vline(aes(xintercept=mean(samples)),
             color="blue", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=median(samples)),
             color="red", linetype="dashed", size=1)
```

I can see here that my distribution is now very normal-looking. Compared to our original distribution, this can be attribute to increasing the sample size from 1 to 3, as a larger sample size creates a larger z-score for the sample mean, which in turn decreases the proportion of sample means that vary from the sample mean.

We know that 90% of our data values in this distribution are those values with a chance of being selected that is between 5% and 95%, which on a Z-table come out to be Z-score values between -1.645 and 1.645.

<center>![***Negative Z-score***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/negative_z.png)
<center>![***Positive Z-score***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/positive_z.png)

So, if the Z-score is equal to the difference observation and the mean divided by the standard deviation, to get those observations in between which 90% of our data values fall, we need to multiply each of our Z-score values by the standard deviation and then add the mean.

And if we do that, we'd have (-1.645 x 4.89) + 19.59 as our lower value, and (1.645 x 4.89) + 19.59 as our upper value. These formulas give us an lower boundary of
```{r lower boundary, ,echo = FALSE}
(-1.645*sd(samples)) + mean(samples)
```
and an upper boundary of
```{r upper boundary, ,echo = FALSE}
(1.645*sd(samples)) + mean(samples)
```

So, 90% of sample sum values to fall between 11.3938 and 27.7860

To find the approximate probability I would a draw value of at least 20, I would need to find the Z-score for 20 in this distribution, find the proportion value in the Z-table, and then subtract this value from 1.

So, our Z-score would be (20 - 19.59) / 4.89, which gives a value of:
```{r z}
(20 - mean(samples)) / sd(samples)
```

Then we look at the Z-table for this score and retrieve that proportion.

<center>![***Samples Sums Statistics***](C:/Users/snewns/Dropbox/DataScienceMasters/Stats/Udacity/DataAnalystNanoDegree/IntroToDescriptiveStats/Final/20_perc_z.png)

#z = 0.07610279 --> .7764 = 77.64% at most 20 --> 1 - .7764 = 0.2236 = 22.36% at least 20