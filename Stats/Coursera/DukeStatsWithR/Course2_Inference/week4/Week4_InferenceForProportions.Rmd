---
title: "Wk4 - Inference"
author: "Steve Newns"
date: "October 19, 2017"
output: html_document
---
90% of all plant species are classified as angiosperms (flowering plants). If you were to randomly sample 200 plants from the list of all known plant species, what is the probability at least 95% of the plants in sample will be flowering plants?

200 is certainly < 10% of all plants, so we can assume plants in our sample are independent of another. 
And n = 200 sample size + p = 0.9, so
```{r}
p <- .9
n <- 200
(success.cond <- n*p)
(fail.cond <- n*(1-p))
```

Both of these are > 10, so success/failure condition holds 

These 2 facts tell us the distribution of the sample proportion is going to be nearly normal.

```{r}
p.hat = .9 # mean of our sampling distribution = population parameter (when normal)
n = 200
(se <- sqrt((p.hat*(1-p.hat))/n))
(z <- (.95 - p.hat)/se)
```

We are more than 2 SDs away from the mean at this point, so it's going to be a pretty small probability that at least 95% of plants our sample of 200 will be flowering plants. 
```{r}
# get proportion of values lower than this z-score on the curve
pnorm(p.hat,.95,se)
```
0.009211063 = 0.9% chance --> p(Z > 2.36) = ~.0091

### Using the binomial distribution as well. 

```{r}
n <- 200
p <- .9
desired.p <- .95
(min.success <- n*desired.p)
```
We're being asked for p(obtaining 95% successes) = 95% of 200 = at least 190 successes in 200 trials where proportion of success is 0.9.

```{r}
sum(dbinom(min.success:n,n,p)) # want prob of anything >= min of 195, up to 200
```

Would it be considered unusual to have 87.5% of plants in a sample end up being angiosperms (if 90% of all plant species are considered so)?

```{r}
(z <- (.875 - p.hat)/se)
pnorm(p.hat,.875,se)

```

No, as we are < 2 SDs away from the mean and have a high probability of ~88.1%

## CI for proportions

2 scientists want to know, if a certain drug is effective against high BP. The 1st scientist wants to give the drug to 1K people w/ high BP + see how many experience lower BP levels. The 2nd scientist wants to give the drug to 500 people w/ high BP + NOT give the drug to another 500 people w/ high BP + see how many in both groups experience lower BP levels. 

We know controlling is important when running experiments, the 2nd study where the group that doesn't get the drug acts = the control group, should be the better design. 

This question was posed to 670 Americans w/in the GSS in 2010, + 99 said all 1K should get the drug. So, we're going to be categorizing these as those w/ a "bad intuition" for experimental design. 571 said 500 should + shouldn't get drug ??? label "good intuition" about experimental design. 

**Our goal** = to *estimate what % of Americans have good intuition about experimental design.*
<ul> **Parameter of interest** = % of all Americans who have good intuition about experimental design, + denote this unknown population parameter, p, for population proportion. </ul>
<ul> **point estimate** = % of *SAMPLED* Americans who have good intuition about experimental design, denoted p^, our KNOWN sample proportion </ul>

```{r}
n <- 670
bad <- 99
good  <- 571
(p.hat <- good / n) # point estimate
(se.prop <- sqrt((p.hat*(1-p.hat))/n))
```

Before we calculate the CI, make sure that conditions for inference have been met
<ul> 1: Independence: Relies on a random sample/assignment + < 10% of the population being sampled. </ul>
  <ul> 670 Americans is definitely < 10% of all Americans + we know the GSS samples randomly. </ul>
  <ul> Therefore, we can assume that whether an American in the sample has good intuition about experimental design is independent of another. </ul>
  
<ul> 2: Sample Size/Skew: check this condition when dealing w/ categorical variables + proportions as the success/failure conditions. </ul>
  <ul> Need to make sure that we have at least 10 successes + 10 failures in our sample. </ul>
  
```{r}
successes <- good
failures <- bad
(successes >= 10 & failures >= 10)
```

Now calculate the CI via the point estimate +/- margin of error (which is z* multiplied by the SE)

```{r}
alpha = .95
(z.star = qnorm(1-((1-alpha)/2)))
(mOe <- z.star*se.prop)
(lower <- p.hat - mOe)
(upper <- p.hat + mOe)
```

The margin of error for this CI was 2.7%, + if we wanted to reduce margin of error to 1% while keeping confidence level the same, at least how many respondents should we sample? 

```{r}
desired.mOe <- .01
(min.n <- ((z.star/desired.mOe)^2)*(p.hat*(1-p.hat)))
```

So, for a minor reduction in our margin of error, we have to increase our sample size a lot, b/c the sample size appears under the square root sign in calculation of the margin of error. 

So, to have benefits from an increased sample size, increase your sample size by a lot before you can actually start reaping the benefits

Esimate min. sample size when we don't know p^ = use 0.5
```{r}
desired.mOe <- .03
(min.n <- ((z.star/desired.mOe)^2)*(0.5*(1-0.5)))
```

## Hypothesis Testing for Proportions

2013 Pew Research poll found 60% of 1,983 randomly sampled American adults believe in evolution. Does this provide convincing evidence that the majority of Americans believe in evolution, where majority is > 50%. 

So if the question is "Is the true proportion of Americans who believe in evolution > 50%", then our alternative H1 is "p > 0.5", and the null H0 is "p = .5"

In this sample, > 50% of respondents believe in evolution, but we're checking to see if this observed difference between the sample proportion + what we're hypothesizing is statistically significant

```{r}
h0 <- .5
p.hat <- .6
n <- 1983
exp.n.succ <- .5*n
exp.n.fail <- .5*n
```

#### Check conditions

n = 1,983 is definitely < 10% of all Americans + we have a random sample, therefore we can assume an American is independent of another. 

Check sample size/skew condition w/ success/failure condition of the EXPECTED (b/c we assume the null is true in hypothesis testing)

```{r}
(exp.n.succ >= 10 & exp.n.fail >= 10)
```

Since both conditions are met, we can assume a nearly normal sampling distribution for our sample proportion.

Our p.hat is distributed nearly normally according to our conditions + to this CLT for proportions, the center of that distribution should be @ the true population parameter, which we don't know, but, since we're doing a hypothesis test, we are assuming the null is true, so we plug in the value of the population parameter set forth in the null </ul>
<ul> Assume this is indeed the true population parameter for the purpose of this hypothesis test. 

```{r}
(se.prop <- sqrt((h0*(1-h0))/n))

(z <- (p.hat - h0)/se.prop) # observed - null / n = test statistic
```

That's a pretty high test statistic, if you think about it, b/c it's much farther than 3 SDs from the mean. So the p value (AUC area under the z curve beyond 8.92) is going to be almost zero

```{r}
pnorm(h0,p.hat,se.prop)
```

## CI for comparing 2 proportions

Using a 95% CI, estimate how Coursera students + the American public at large compare w/ respect to their views on laws banning possession of handguns

```{r}
n.c <- 83
n.a <- 1028
succ.c <- 59
succ.a <- 257
(p.hat.c <- succ.c/n.c)
(p.hat.a <- succ.a/n.a)
```

*Independence condition* =??? DO have a random sample for US population, but not for Coursera population = simply a voluntary poll posted on Coursera discussion forums, so can't say it was a random sample. 
<ul>
<li> 10% condition was met for both of the groups = both 1028 + 83 are < 10% of their respective populations </li>
<li> This means sampled Americans can be assumed to be independent of each other but sampled Coursera students may not be.  </li>
<li> So in this case, be wary of generalizing any of conclusion from these findings to the overall population at large b/c we don't really have a good sample from the Coursera population.  </li>
<li> Still move on w/ our analysis only for illustrative purposes </li>
</ul>

*Sample size + skew condition* =??? can we assume the sampling distribution of the difference between the 2 proportions is nearly normal
<ul>
<li> When dealing w/ proportions, check for this using success failure rule + when doing a CI, check for this w/ observed successes + observed failures </li>
<li> US = 257 observed successes + (1,028 - 257) = 771 observed failures </li>
<li> Coursera = 59 successes + 24 failures </li>
<li> Both of these met = success/failure condition is met for both of the groups + therefore we can assume the sampling distribution of the difference between the 2 proportions is nearly normal </li>
</ul>

```{r}
(point.estimate <- p.hat.c - p.hat.a)
alpha = .95
(z.star = qnorm(1-((1-alpha)/2)))
(se.diff <- sqrt((p.hat.c*(1-p.hat.c))/n.c + ((p.hat.a*(1-p.hat.a))/n.a)))
(mOe <- z.star*se.diff)
(lower <- point.estimate - mOe)
(upper <- point.estimate + mOe)
```

We are 95% confident the proportion of Coursera students who believe there should be a ban on possession of handguns is 36-56% higher than the proportion of Americans who do.

Now switch the order of the p.hat values

```{r}
(point.estimate <- p.hat.a - p.hat.c)
alpha = .95
(z.star = qnorm(1-((1-alpha)/2)))
(se.diff <- sqrt((p.hat.a*(1-p.hat.a))/n.a + ((p.hat.c*(1-p.hat.c))/n.c)))
(mOe <- z.star*se.diff)
(lower <- point.estimate - mOe)
(upper <- point.estimate + mOe)
```

We are 95% confident the proportion of Americans who believe there should be a ban on possession of handguns is 36%-56% lower than the proportions of Coursera students

Based on the CI calculated, should we expect to find a significant difference between the population proportions of Coursera students + the American public at large who believe there should be a law banning the possession of handguns at the equivalent significance level?

```{r}
pnorm(point.estimate,.95,se.diff)
```

In this case, we're asked to do a kind of mock hypothesis test, + in that case, our null = the difference between the 2 population proportions = 0 = these 2 populations are exactly the same

For our CI, anything between .36 + .56 is fair game for the difference between the 2 population proportions

The value= 0 does not appear, + based on that, we'd reject this null + say that "based on this CI, it doesn't appear that these 2 populations are the same w/ respect to proportion of those who believe there should be a law banning the possession of handguns"

### Hypothesis Test for Comparing Two Proportions
```{r}
n.male <- 90
n.female <- 122

suc.male <- 34
suc.female <- 61

fail.male <- n.male - suc.male
fail.female <- n.female - suc.female 

(p.hat.male <- suc.male / n.male)
(p.hat.female <- suc.female / n.female)

(pooled.prop <- (suc.female + suc.male) / (n.male + n.female))
```

### Check conditions
#### Within Groups
n = 90 + 122 is definitely < 10% of all Americans + we have a random sample, therefore we can assume sampled males + sampled females are independent of each other = within-group independence met

Not paired samples (different sample sizes indicated this) + had an overall random sample = between-groups independence met


Check sample size/skew condition w/ success/failure condition of the EXPECTED (b/c we assume null is true in hypothesis testing) + w/ pooled estimate of p^ b/c doing proportions + we don't know what to set p to so we estimate

```{r}
exp.succ.male <- pooled.prop*n.male
exp.fail.male <- (1-pooled.prop)*n.male

exp.succ.female <- pooled.prop*n.female
exp.fail.female <- (1-pooled.prop)*n.female

(exp.succ.male >= 10 & exp.fail.male >= 10)
(exp.succ.female >= 10 & exp.fail.female >= 10)
```

Can now assume that this sampling	distribution of the difference between the two sample proportions is nearly normal


Since both conditions are met, we can assume a nearly normal sampling distribution for our sampling distribution of the difference between the proportions.

Conduct a HT a 5% significance level, evaluating if males + females are equally likely to answer “yes”. 

```{r}
(se.prop.ht <- sqrt(((pooled.prop*(1-pooled.prop))/n.male) + ((pooled.prop*(1-pooled.prop))/n.female)))
(point.estimate <- p.hat.male - p.hat.female)

h0 <- 0
(z <- (point.estimate - h0)/se.prop.ht) # observed - null / n = test statistic

2*pnorm(point.estimate,h0,se.prop.ht)
```

### Small Samples Proportions

#### Paul the Octopus 8-game predictions simulation

```{r}
library(tidyverse)
source("http://bit.ly/dasi_inference")

# create set of the actual observed values
paul <- factor(c(rep("y",8),rep("n",0)), levels = c("y","n"))

# perform 10k simulations (default) to estimate a proportion using a hyp. test via a simulation
inference(paul, est = "proportion", type = "ht", method = "simulation", success = "y", 
          null = .5, alternative = "greater")
```

If Paul was indeed randomly guessing, the probability he’d get all 8 games correct = .0039
This data provides convincing evidence Paul did better than random guessing

#### Back of Hand
```{r}
source("http://bit.ly/dasi_inference")

# create set of the actual observed values
hand <- factor(c(rep("y",11),rep("n",1)), levels = c("y","n"))

# perform 100 simulations (default) to estimate a proportion using a hyp. test via a simulation
inference(hand, est = "proportion", type = "ht", method = "simulation", success = "y", 
          null = .1, alternative = "greater", nsim = 100)
```